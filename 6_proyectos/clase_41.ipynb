{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrb1CvntpqGHEYR4xH1Lpv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_sept/blob/main/6_proyectos/clase_41.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversación con grok para ejercicio de detección de victimas y victimarios\n",
        "\n",
        "https://grok.com/share/bGVnYWN5_c75023c4-13a0-493f-a185-82dadf1d0c80\n",
        "\n",
        "\n",
        "¡Entendido! Quieres ejemplos de código usando MoveNet para procesar videos y detectar poses, lo cual es útil para identificar agresores y víctimas en tu caso de detección de crímenes en videos. MoveNet es un modelo ligero de estimación de poses de Google, ideal para analizar skeletons (puntos clave del cuerpo) en tiempo real, y puede ayudarte a inferir roles basados en movimientos (ej. un agresor levantando un brazo para golpear, una víctima retrocediendo). A continuación, te doy dos ejemplos prácticos en Python usando MoveNet: uno para procesar un solo frame (imagen) y otro para videos, integrando detección de personas y análisis básico de roles.\n",
        "\n",
        "### Prerrequisitos\n",
        "- **Instalaciones**:\n",
        "  ```bash\n",
        "  pip install tensorflow tensorflow-hub opencv-python numpy\n",
        "  ```\n",
        "- **Hardware**: MoveNet es ligero, funciona en CPU, pero GPU acelera (Colab es buena opción).\n",
        "- **Modelo**: Usaremos MoveNet.SinglePose (versión \"Lightning\" para rapidez o \"Thunder\" para precisión). Disponible en TensorFlow Hub.\n",
        "\n",
        "### Ejemplo 1: Detección de Poses en una Imagen\n",
        "Este código detecta puntos clave (keypoints) en una imagen estática, útil para entender cómo MoveNet procesa una persona.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Carga MoveNet desde TensorFlow Hub (Lightning para rapidez)\n",
        "model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = model.signatures['serving_default']\n",
        "\n",
        "# Preprocesa imagen\n",
        "def preprocess_image(image):\n",
        "    img = cv2.resize(image, (192, 192))  # MoveNet Lightning usa 192x192\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)  # Añade batch dimension\n",
        "    return img\n",
        "\n",
        "# Detecta poses\n",
        "def detect_pose(image):\n",
        "    input_image = preprocess_image(image)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # [persona, keypoints, (y, x, confianza)]\n",
        "    return keypoints\n",
        "\n",
        "# Visualiza keypoints\n",
        "def draw_keypoints(image, keypoints, conf_threshold=0.3):\n",
        "    height, width = image.shape[:2]\n",
        "    for keypoint in keypoints:\n",
        "        y, x, confidence = keypoint\n",
        "        if confidence > conf_threshold:\n",
        "            x = int(x * width)\n",
        "            y = int(y * height)\n",
        "            cv2.circle(image, (x, y), 5, (0, 255, 0), -1)  # Dibuja punto verde\n",
        "    return image\n",
        "\n",
        "# Carga y procesa imagen\n",
        "image_path = 'tu_imagen.jpg'  # Reemplaza con tu imagen\n",
        "image = cv2.imread(image_path)\n",
        "keypoints = detect_pose(image)\n",
        "image_with_keypoints = draw_keypoints(image.copy(), keypoints)\n",
        "\n",
        "# Guarda o muestra resultado\n",
        "cv2.imwrite('output_image.jpg', image_with_keypoints)\n",
        "cv2.imshow('Pose Detection', image_with_keypoints)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "```\n",
        "\n",
        "**Explicación**:\n",
        "- Carga MoveNet desde TensorFlow Hub.\n",
        "- Procesa la imagen (resize a 192x192, formato requerido por MoveNet).\n",
        "- Extrae 17 keypoints (nariz, hombros, codos, muñecas, etc.) con coordenadas (y, x) y confianza.\n",
        "- Dibuja puntos en la imagen si la confianza es >0.3.\n",
        "- Útil para pruebas iniciales. Para roles (agresor/víctima), analiza keypoints como distancia entre manos y torso de otra persona.\n",
        "\n",
        "### Ejemplo 2: Detección de Poses en Video con Análisis de Roles\n",
        "Este código procesa un video, detecta personas con YOLOv8 (para identificar múltiples actores), aplica MoveNet para estimar poses y hace un análisis básico de movimientos para inferir agresor/víctima.\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO  # Para YOLOv8\n",
        "\n",
        "# Carga modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Modelo YOLOv8 preentrenado (instala ultralytics: pip install ultralytics)\n",
        "\n",
        "# Preprocesa frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# Detecta poses\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # [persona, keypoints, (y, x, conf)]\n",
        "    return keypoints\n",
        "\n",
        "# Analiza movimiento para roles (heurística simple)\n",
        "def analyze_roles(keypoints_list, person_boxes):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        # Ejemplo: Si mano derecha (keypoint 10) está alta y cerca de otra persona, posible agresor\n",
        "        right_wrist = keypoints[10]  # [y, x, conf]\n",
        "        if right_wrist[2] > 0.3:  # Confianza suficiente\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:  # Evita compararse consigo mismo\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    if abs(wrist_x * 192 - ox) < 50 and wrist_y * 192 < oy:  # Mano cerca y alta\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'victima'))\n",
        "                        return roles\n",
        "    return roles\n",
        "\n",
        "# Procesa video\n",
        "def process_video(video_path, output_path='output_video.mp4'):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detecta personas con YOLOv8\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if box.cls == 0:  # 0 es 'person' en COCO\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Aplica MoveNet a cada persona\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[y:y+h, x:x+w]\n",
        "            if person_crop.size > 0:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "\n",
        "        # Analiza roles\n",
        "        roles = analyze_roles(keypoints_list, person_boxes)\n",
        "\n",
        "        # Dibuja resultados\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            cv2.putText(frame, role, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "            # Dibuja keypoints\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:  # Confianza\n",
        "                    kx, ky = int(kp[1] * w + x), int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 5, (255, 0, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Uso\n",
        "process_video('tu_video.mp4', 'output_video.mp4')\n",
        "```\n",
        "\n",
        "**Explicación**:\n",
        "- **YOLOv8**: Detecta personas y extrae bounding boxes.\n",
        "- **MoveNet**: Estima poses (17 keypoints) para cada persona detectada.\n",
        "- **Análisis de Roles**: Heurística simple: si una mano está alta y cerca de otra persona, se etiqueta como \"agresor\". La otra como \"víctima\". Puedes mejorar esto con:\n",
        "  - Modelos ST-GCN para analizar secuencias de poses.\n",
        "  - Detección de velocidad (optical flow) para confirmar movimientos agresivos.\n",
        "- **Salida**: Video con bounding boxes, keypoints y etiquetas de roles.\n",
        "\n",
        "### 5. **Mejoras y Notas**\n",
        "- **Precisión de Roles**: La heurística es básica. Para mejor precisión:\n",
        "  - Entrena un modelo (ej. ST-GCN o SlowFast) con datasets como RWF-2000, anotando agresores/víctimas.\n",
        "  - Usa optical flow (cv2.calcOpticalFlowFarneback) para detectar dirección/velocidad de movimientos.\n",
        "- **Escalabilidad**: MoveNet es rápido (10-20 FPS en CPU). Para tiempo real, optimiza con TensorRT.\n",
        "- **Datos**: Fine-tunea con datasets como SCVD o UCF-Crime, anotando roles. Herramientas como CVAT ayudan.\n",
        "- **Ética**: Anonimiza rostros (ej. con Mediapipe Face Detection) para privacidad.\n",
        "- **Audio**: Si los videos tienen audio, combina con diarización (Librosa o pyannote.audio) para contexto (ej. gritos).\n",
        "\n",
        "### 6. **Siguientes Pasos**\n",
        "- **Fine-Tuning**: Anota un subconjunto de tus videos con CVAT y entrena un modelo custom (ej. en Kinetics o RWF-2000).\n",
        "- **Modelos Avanzados**: Explora VideoMAE o TimeSformer para acción y roles end-to-end.\n",
        "- **Código Adicional**: Si quieres integrar optical flow o ST-GCN, puedo dar otro snippet. ¿Detalles sobre tus videos (duración, resolución, tipo de crímenes)? ¿O prefieres enfocarte en algo específico (ej. datasets, entrenamiento)? ¡Dime!"
      ],
      "metadata": {
        "id": "zvPg5-ijKTWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow tensorflow-hub opencv-python-headless ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M6OzfcRKkPO",
        "outputId": "334ea554-ca6f-4f5c-c760-962a8780fa62"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.12/dist-packages (0.16.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.225)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow-hub) (2.19.0)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hYwaa6ryKC2t"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO  # Para YOLOv8\n",
        "\n",
        "# Carga modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Modelo YOLOv8 preentrenado (instala ultralytics: pip install ultralytics)\n",
        "\n",
        "# Preprocesa frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# Detecta poses\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # [persona, keypoints, (y, x, conf)]\n",
        "    return keypoints\n",
        "\n",
        "# Analiza movimiento para roles (heurística simple)\n",
        "def analyze_roles(keypoints_list, person_boxes):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        # Ejemplo: Si mano derecha (keypoint 10) está alta y cerca de otra persona, posible agresor\n",
        "        right_wrist = keypoints[10]  # [y, x, conf]\n",
        "        if right_wrist[2] > 0.3:  # Confianza suficiente\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:  # Evita compararse consigo mismo\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    if abs(wrist_x * 192 - ox) < 50 and wrist_y * 192 < oy:  # Mano cerca y alta\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'victima'))\n",
        "                        return roles\n",
        "    return roles\n",
        "\n",
        "# Procesa video\n",
        "def process_video(video_path, output_path='output_video.mp4'):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detecta personas con YOLOv8\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if box.cls == 0:  # 0 es 'person' en COCO\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Aplica MoveNet a cada persona\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[y:y+h, x:x+w]\n",
        "            if person_crop.size > 0:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "\n",
        "        # Analiza roles\n",
        "        roles = analyze_roles(keypoints_list, person_boxes)\n",
        "\n",
        "        # Dibuja resultados\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            cv2.putText(frame, role, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "            # Dibuja keypoints\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:  # Confianza\n",
        "                    kx, ky = int(kp[1] * w + x), int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 5, (255, 0, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Uso\n",
        "process_video('tu_video.mp4', 'output_video.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias\n",
        "!pip install tensorflow tensorflow-hub opencv-python-headless ultralytics\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Carga modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Modelo YOLOv8 preentrenado\n",
        "\n",
        "# Preprocesa frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# Detecta poses\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # [persona, keypoints, (y, x, conf)]\n",
        "    return keypoints\n",
        "\n",
        "# Analiza movimiento para roles (heurística simple)\n",
        "def analyze_roles(keypoints_list, person_boxes):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        right_wrist = keypoints[10]  # [y, x, conf]\n",
        "        if right_wrist[2] > 0.3:  # Confianza suficiente\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:  # Evita compararse consigo mismo\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    if abs(wrist_x * 192 - ox) < 50 and wrist_y * 192 < oy:  # Mano cerca y alta\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'victima'))\n",
        "                        return roles\n",
        "    return roles\n",
        "\n",
        "# Calcula métricas (F1-score para roles)\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if len(y_true) == len(y_pred):\n",
        "        f1 = f1_score(y_true, y_pred, labels=['agresor', 'victima'], average='weighted')\n",
        "        return f1\n",
        "    return 0.0\n",
        "\n",
        "# Procesa video\n",
        "def process_video(video_path, output_path='output_video.mp4', ground_truth_roles=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detecta personas con YOLOv8\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if box.cls == 0:  # 0 es 'person' en COCO\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Aplica MoveNet a cada persona\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[y:y+h, x:x+w]\n",
        "            if person_crop.size > 0:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "\n",
        "        # Analiza roles\n",
        "        roles = analyze_roles(keypoints_list, person_boxes)\n",
        "        all_pred_roles.append(roles)\n",
        "\n",
        "        # Dibuja resultados\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            cv2.putText(frame, role, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx, ky = int(kp[1] * w + x), int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 5, (255, 0, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Calcula métricas si hay ground truth\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0])  # Ejemplo con un frame\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# Sube el video\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Ground truth de ejemplo (ajusta con tus datos reales)\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'victima')]  # Asumiendo 2 personas en el video\n",
        "\n",
        "# Procesa el video y descarga el resultado\n",
        "output_path = process_video(video_path, 'output_video.mp4', ground_truth_roles)\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5f18z6SiK73A",
        "outputId": "82924da6-3453-4470-ce15-0485f46adfc3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.12/dist-packages (0.16.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.225)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow-hub) (2.19.0)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-43ab40c3-3f76-4e81-aa48-7fb0250972f1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-43ab40c3-3f76-4e81-aa48-7fb0250972f1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving people-detection.mp4 to people-detection.mp4\n",
            "\n",
            "0: 384x640 (no detections), 251.8ms\n",
            "Speed: 3.6ms preprocess, 251.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.8ms\n",
            "Speed: 3.3ms preprocess, 11.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.7ms\n",
            "Speed: 2.7ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 3.4ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 3.7ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 3.9ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.7ms\n",
            "Speed: 3.0ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.2ms\n",
            "Speed: 2.9ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.8ms\n",
            "Speed: 4.0ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.6ms\n",
            "Speed: 2.8ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.5ms\n",
            "Speed: 3.1ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.3ms\n",
            "Speed: 2.1ms preprocess, 6.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.7ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.4ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.3ms\n",
            "Speed: 2.1ms preprocess, 6.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.5ms\n",
            "Speed: 3.2ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.8ms\n",
            "Speed: 2.2ms preprocess, 6.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 3.1ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.6ms\n",
            "Speed: 2.7ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.6ms\n",
            "Speed: 2.9ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.3ms\n",
            "Speed: 1.7ms preprocess, 6.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.8ms\n",
            "Speed: 2.8ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 3.1ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.6ms\n",
            "Speed: 2.5ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.3ms preprocess, 6.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.7ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.5ms\n",
            "Speed: 2.9ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 1.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.8ms\n",
            "Speed: 2.0ms preprocess, 6.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.7ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.5ms\n",
            "Speed: 2.8ms preprocess, 6.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.5ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.6ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 12.3ms\n",
            "Speed: 2.9ms preprocess, 12.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.6ms\n",
            "Speed: 2.3ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.8ms\n",
            "Speed: 2.2ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.3ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.7ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.0ms\n",
            "Speed: 1.6ms preprocess, 7.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.6ms\n",
            "Speed: 2.4ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.5ms\n",
            "Speed: 2.9ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.5ms preprocess, 6.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.5ms\n",
            "Speed: 2.4ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.7ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.7ms preprocess, 6.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.2ms\n",
            "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.2ms\n",
            "Speed: 2.0ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.8ms\n",
            "Speed: 2.2ms preprocess, 8.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 2.3ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.5ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.1ms\n",
            "Speed: 2.2ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.2ms\n",
            "Speed: 2.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 3.1ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 1.8ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.1ms\n",
            "Speed: 3.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.8ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.3ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.3ms\n",
            "Speed: 1.6ms preprocess, 9.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.7ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.0ms\n",
            "Speed: 3.6ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.9ms\n",
            "Speed: 3.9ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 3.0ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.2ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 2.5ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 1.6ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 1.9ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.5ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.3ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 3.9ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.1ms\n",
            "Speed: 1.6ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.5ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.2ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.5ms\n",
            "Speed: 2.7ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.5ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 1.8ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.7ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 2.6ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.3ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.5ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 2.3ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 3.0ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 3.0ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 2.4ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.9ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 3.3ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 3.0ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.4ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.2ms\n",
            "Speed: 3.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 2.0ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.5ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.4ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 3.1ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 2.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.5ms\n",
            "Speed: 2.2ms preprocess, 9.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.4ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 13.3ms\n",
            "Speed: 4.0ms preprocess, 13.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.2ms\n",
            "Speed: 3.3ms preprocess, 12.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.1ms preprocess, 6.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.4ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 2.0ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.3ms preprocess, 6.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 3.0ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.8ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 3.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.5ms preprocess, 6.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.4ms preprocess, 6.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.9ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.6ms\n",
            "Speed: 3.0ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 2.4ms preprocess, 7.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.5ms\n",
            "Speed: 3.1ms preprocess, 6.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.7ms\n",
            "Speed: 3.3ms preprocess, 6.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.7ms\n",
            "Speed: 3.1ms preprocess, 6.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.3ms\n",
            "Speed: 2.5ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.7ms preprocess, 6.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.7ms\n",
            "Speed: 2.6ms preprocess, 6.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.3ms\n",
            "Speed: 2.6ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.3ms\n",
            "Speed: 2.8ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.6ms\n",
            "Speed: 3.9ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.3ms\n",
            "Speed: 2.5ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12.5ms\n",
            "Speed: 2.5ms preprocess, 12.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 1.9ms preprocess, 6.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 3.5ms preprocess, 6.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.8ms\n",
            "Speed: 1.6ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 tv, 6.4ms\n",
            "Speed: 2.9ms preprocess, 6.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 tv, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 tv, 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 tv, 7.0ms\n",
            "Speed: 2.4ms preprocess, 7.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 tv, 6.6ms\n",
            "Speed: 3.0ms preprocess, 6.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.5ms preprocess, 6.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.5ms\n",
            "Speed: 3.0ms preprocess, 6.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.5ms\n",
            "Speed: 3.1ms preprocess, 6.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 handbag, 6.4ms\n",
            "Speed: 2.5ms preprocess, 6.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 2 handbags, 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.7ms\n",
            "Speed: 2.6ms preprocess, 6.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.5ms\n",
            "Speed: 2.9ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.5ms\n",
            "Speed: 2.4ms preprocess, 6.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 handbag, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 cell phone, 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.7ms\n",
            "Speed: 2.4ms preprocess, 6.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.5ms preprocess, 6.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.3ms\n",
            "Speed: 2.3ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.9ms\n",
            "Speed: 2.7ms preprocess, 6.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.8ms preprocess, 7.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.3ms\n",
            "Speed: 2.2ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 tv, 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.4ms\n",
            "Speed: 2.4ms preprocess, 6.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.6ms\n",
            "Speed: 3.6ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.5ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 2.8ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.2ms\n",
            "Speed: 2.7ms preprocess, 6.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 1.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.3ms\n",
            "Speed: 2.4ms preprocess, 6.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.4ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 3.2ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 2.6ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 2.4ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 3.0ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.7ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 3.0ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 3.5ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.8ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.4ms\n",
            "Speed: 3.2ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.0ms\n",
            "Speed: 4.6ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.9ms\n",
            "Speed: 4.0ms preprocess, 12.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.4ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.5ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 2.3ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 1.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.2ms\n",
            "Speed: 2.8ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 3.7ms preprocess, 8.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 2.9ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 3.7ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 3.0ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.4ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.4ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.2ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 4.0ms preprocess, 8.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 3.3ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.3ms\n",
            "Speed: 5.5ms preprocess, 28.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.1ms\n",
            "Speed: 8.5ms preprocess, 23.1ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.2ms\n",
            "Speed: 7.4ms preprocess, 25.2ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.6ms\n",
            "Speed: 6.5ms preprocess, 29.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 37.2ms\n",
            "Speed: 10.4ms preprocess, 37.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.5ms\n",
            "Speed: 7.4ms preprocess, 29.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.4ms\n",
            "Speed: 6.5ms preprocess, 29.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 39.7ms\n",
            "Speed: 6.4ms preprocess, 39.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 28.5ms\n",
            "Speed: 10.3ms preprocess, 28.5ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 23.7ms\n",
            "Speed: 8.5ms preprocess, 23.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 18.5ms\n",
            "Speed: 6.4ms preprocess, 18.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 25.6ms\n",
            "Speed: 5.5ms preprocess, 25.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.6ms\n",
            "Speed: 9.6ms preprocess, 14.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.2ms\n",
            "Speed: 10.1ms preprocess, 29.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 4.3ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.5ms\n",
            "Speed: 3.3ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.0ms\n",
            "Speed: 2.9ms preprocess, 9.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 3.2ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.6ms\n",
            "Speed: 2.7ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.3ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 3.3ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 3.6ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 3.3ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.5ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 3.2ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.2ms preprocess, 6.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.4ms preprocess, 6.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 2.0ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.9ms\n",
            "Speed: 3.0ms preprocess, 8.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.0ms\n",
            "Speed: 3.8ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 toilet, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 toilet, 9.6ms\n",
            "Speed: 1.8ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 toilet, 7.0ms\n",
            "Speed: 2.4ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6.7ms\n",
            "Speed: 3.2ms preprocess, 6.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.7ms\n",
            "Speed: 2.3ms preprocess, 8.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 3.0ms preprocess, 8.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.7ms\n",
            "Speed: 2.7ms preprocess, 6.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.6ms\n",
            "Speed: 2.0ms preprocess, 10.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 11.9ms\n",
            "Speed: 2.1ms preprocess, 11.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.1ms\n",
            "Speed: 3.2ms preprocess, 7.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.8ms\n",
            "Speed: 2.0ms preprocess, 8.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 sports ball, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.1ms\n",
            "Speed: 2.9ms preprocess, 9.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 6.5ms\n",
            "Speed: 2.5ms preprocess, 6.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 tv, 6.6ms\n",
            "Speed: 2.5ms preprocess, 6.6ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.9ms\n",
            "Speed: 3.3ms preprocess, 6.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 skateboard, 1 tv, 7.5ms\n",
            "Speed: 1.8ms preprocess, 7.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 skateboard, 2 tvs, 6.2ms\n",
            "Speed: 2.4ms preprocess, 6.2ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 6.5ms\n",
            "Speed: 2.5ms preprocess, 6.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 6.4ms\n",
            "Speed: 2.7ms preprocess, 6.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 15.1ms\n",
            "Speed: 4.6ms preprocess, 15.1ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 13.7ms\n",
            "Speed: 3.3ms preprocess, 13.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 11.3ms\n",
            "Speed: 3.5ms preprocess, 11.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 17.5ms\n",
            "Speed: 3.8ms preprocess, 17.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10.8ms\n",
            "Speed: 3.4ms preprocess, 10.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tv, 8.6ms\n",
            "Speed: 2.8ms preprocess, 8.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 4.0ms preprocess, 8.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.8ms\n",
            "Speed: 2.9ms preprocess, 8.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.8ms\n",
            "Speed: 3.6ms preprocess, 8.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.6ms\n",
            "Speed: 4.6ms preprocess, 8.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8.6ms\n",
            "Speed: 2.8ms preprocess, 8.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 2.4ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.1ms\n",
            "Speed: 3.8ms preprocess, 8.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 8.4ms\n",
            "Speed: 3.9ms preprocess, 8.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 12.8ms\n",
            "Speed: 4.4ms preprocess, 12.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10.3ms\n",
            "Speed: 3.6ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.9ms\n",
            "Speed: 3.1ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 chair, 9.8ms\n",
            "Speed: 2.8ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.9ms\n",
            "Speed: 2.9ms preprocess, 8.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.1ms\n",
            "Speed: 3.9ms preprocess, 14.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.9ms\n",
            "Speed: 3.8ms preprocess, 11.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.2ms\n",
            "Speed: 2.5ms preprocess, 12.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.3ms\n",
            "Speed: 3.4ms preprocess, 12.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 13.6ms\n",
            "Speed: 3.4ms preprocess, 13.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.7ms\n",
            "Speed: 3.0ms preprocess, 11.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.6ms\n",
            "Speed: 2.7ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.7ms\n",
            "Speed: 2.4ms preprocess, 11.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 18.0ms\n",
            "Speed: 2.8ms preprocess, 18.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.2ms\n",
            "Speed: 3.8ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 15.0ms\n",
            "Speed: 3.9ms preprocess, 15.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.0ms\n",
            "Speed: 3.9ms preprocess, 9.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.4ms\n",
            "Speed: 3.5ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.0ms\n",
            "Speed: 4.1ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.0ms\n",
            "Speed: 2.6ms preprocess, 10.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 13.0ms\n",
            "Speed: 2.4ms preprocess, 13.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.8ms\n",
            "Speed: 2.7ms preprocess, 9.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.1ms\n",
            "Speed: 3.5ms preprocess, 14.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.5ms\n",
            "Speed: 2.9ms preprocess, 11.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 15.4ms\n",
            "Speed: 5.0ms preprocess, 15.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.6ms\n",
            "Speed: 2.7ms preprocess, 12.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.9ms\n",
            "Speed: 3.3ms preprocess, 12.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.3ms\n",
            "Speed: 3.7ms preprocess, 10.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.1ms\n",
            "Speed: 2.7ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.1ms\n",
            "Speed: 2.9ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.6ms\n",
            "Speed: 2.7ms preprocess, 14.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.2ms\n",
            "Speed: 2.6ms preprocess, 12.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.3ms\n",
            "Speed: 4.6ms preprocess, 12.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.6ms\n",
            "Speed: 3.6ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 13.4ms\n",
            "Speed: 3.7ms preprocess, 13.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.4ms\n",
            "Speed: 2.5ms preprocess, 14.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 15.5ms\n",
            "Speed: 4.6ms preprocess, 15.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.0ms\n",
            "Speed: 2.9ms preprocess, 12.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.4ms\n",
            "Speed: 4.3ms preprocess, 11.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.8ms\n",
            "Speed: 2.7ms preprocess, 14.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 21.4ms\n",
            "Speed: 5.7ms preprocess, 21.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 13.4ms\n",
            "Speed: 5.3ms preprocess, 13.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 15.8ms\n",
            "Speed: 3.3ms preprocess, 15.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.4ms\n",
            "Speed: 4.4ms preprocess, 11.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.4ms\n",
            "Speed: 3.0ms preprocess, 12.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.2ms\n",
            "Speed: 2.4ms preprocess, 12.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 13.2ms\n",
            "Speed: 2.8ms preprocess, 13.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.2ms\n",
            "Speed: 2.6ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.8ms\n",
            "Speed: 2.7ms preprocess, 11.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.5ms\n",
            "Speed: 2.8ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.3ms\n",
            "Speed: 3.4ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.9ms\n",
            "Speed: 2.3ms preprocess, 9.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.3ms\n",
            "Speed: 2.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.9ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 5.2ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 3.1ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 2.1ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.1ms\n",
            "Speed: 2.2ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 3.3ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.6ms\n",
            "Speed: 2.8ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.6ms\n",
            "Speed: 2.2ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.7ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.1ms\n",
            "Speed: 2.4ms preprocess, 14.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.1ms\n",
            "Speed: 3.4ms preprocess, 14.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 4.2ms preprocess, 7.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 12.6ms\n",
            "Speed: 2.9ms preprocess, 12.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.7ms\n",
            "Speed: 4.1ms preprocess, 11.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.4ms\n",
            "Speed: 4.2ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.6ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.7ms\n",
            "Speed: 4.1ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 2.2ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.7ms\n",
            "Speed: 3.6ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 4.0ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 4.9ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.7ms\n",
            "Speed: 2.7ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.8ms\n",
            "Speed: 1.7ms preprocess, 9.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.5ms\n",
            "Speed: 2.2ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.3ms\n",
            "Speed: 3.5ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.4ms\n",
            "Speed: 3.5ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 1.9ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 2.1ms preprocess, 8.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 3.1ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 3.9ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.8ms\n",
            "Speed: 3.2ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.5ms\n",
            "Speed: 2.8ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.1ms\n",
            "Speed: 3.0ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.0ms\n",
            "Speed: 3.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.5ms\n",
            "Speed: 3.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 3.2ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.0ms\n",
            "Speed: 1.7ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.4ms\n",
            "Speed: 3.0ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 9.0ms\n",
            "Speed: 2.9ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 6.7ms\n",
            "Speed: 2.6ms preprocess, 6.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 6.8ms\n",
            "Speed: 3.0ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.3ms\n",
            "Speed: 2.6ms preprocess, 6.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.4ms\n",
            "Speed: 2.4ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 6.7ms\n",
            "Speed: 2.5ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9.4ms\n",
            "Speed: 3.1ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.3ms\n",
            "Speed: 1.7ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 8.2ms\n",
            "Speed: 1.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 1.8ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.4ms\n",
            "Speed: 2.5ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.4ms\n",
            "Speed: 2.5ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 10.5ms\n",
            "Speed: 5.1ms preprocess, 10.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.6ms\n",
            "Speed: 3.4ms preprocess, 9.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 2.8ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.5ms\n",
            "Speed: 3.3ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 5.4ms preprocess, 8.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 2.7ms preprocess, 8.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.8ms\n",
            "Speed: 2.8ms preprocess, 8.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 11.2ms\n",
            "Speed: 3.1ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 14.2ms\n",
            "Speed: 4.2ms preprocess, 14.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 9.8ms\n",
            "Speed: 3.4ms preprocess, 9.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "F1-Score para roles: 0.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_98b1871a-b3f6-4401-bc5b-c6e5a2de7b05\", \"output_video.mp4\", 4029895)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Entendido! Te proporciono un código completo para Google Colab que realiza el proceso completo: descarga el dataset \"Hockey Fight Videos\" desde Kaggle, procesa un video usando YOLOv8 para detectar personas, MoveNet para estimar poses, y asigna etiquetas de \"agresor\" y \"víctima\" basadas en una heurística simple (mano derecha alta y cerca de otra persona). También incluye métricas básicas (F1-score) y visualización de etiquetas claras en el video de salida. El código está optimizado para Colab, con instrucciones para autenticar Kaggle, descargar el dataset, procesar un video y descargar el resultado.\n",
        "\n",
        "### Características del Código\n",
        "- **Dataset**: Usa \"Hockey Fight Videos\" (https://www.kaggle.com/datasets/yassershrief/hockey-fight-vidoes), con clips de peleas y no-peleas, ideal para detectar interacciones físicas.\n",
        "- **Modelos**: YOLOv8 para detección de personas, MoveNet para poses.\n",
        "- **Etiquetado**: Asigna \"agresor\" (mano alta y cerca de otra persona) y \"víctima\" (persona cercana al movimiento agresivo). Etiquetas en rojo (agresor) y azul (víctima) en el video.\n",
        "- **Métricas**: Calcula F1-score para roles, asumiendo un ground truth de ejemplo (puedes ajustarlo con anotaciones reales).\n",
        "- **Colab**: Incluye instalación de dependencias, carga del dataset, y descarga del video procesado.\n",
        "\n",
        "### Código Completo para Colab\n",
        "```python\n",
        "# 1. Instalar dependencias\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics sklearn\n",
        "!pip install -q kaggle\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# 2. Configurar Kaggle API\n",
        "print(\"Sube tu kaggle.json (descárgalo desde Kaggle > Account > Create API Token)\")\n",
        "uploaded = files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 3. Descargar y descomprimir Hockey Fight Videos dataset\n",
        "!kaggle datasets download -d yassershrief/hockey-fight-vidoes\n",
        "!unzip -q hockey-fight-vidoes.zip\n",
        "\n",
        "# 4. Cargar modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Modelo YOLOv8 preentrenado\n",
        "\n",
        "# 5. Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# 6. Detectar poses\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # [persona, keypoints, (y, x, conf)]\n",
        "    return keypoints\n",
        "\n",
        "# 7. Analizar movimiento para roles (heurística: mano derecha alta y cerca)\n",
        "def analyze_roles(keypoints_list, person_boxes):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        right_wrist = keypoints[10]  # Mano derecha: [y, x, conf]\n",
        "        if right_wrist[2] > 0.3:  # Confianza suficiente\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:  # Evita compararse consigo mismo\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    if abs(wrist_x * 192 - ox) < 100 and wrist_y * 192 < oy:  # Mano alta y cerca\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'víctima'))\n",
        "                        return roles\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]  # Default si no hay detección\n",
        "\n",
        "# 8. Calcular métricas (F1-score para roles)\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        f1 = f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "        return f1\n",
        "    return 0.0\n",
        "\n",
        "# 9. Procesar video\n",
        "def process_video(video_path, output_path='output_hockey.mp4', ground_truth_roles=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir el video {video_path}\")\n",
        "        return None\n",
        "    \n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "    \n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        \n",
        "        # Reducir resolución para rapidez (opcional)\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        width, height = 640, 480\n",
        "        \n",
        "        # Detectar personas con YOLOv8\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if box.cls == 0:  # 0 es 'person' en COCO\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "        \n",
        "        # Aplicar MoveNet a cada persona\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))  # Dummy si crop falla\n",
        "        \n",
        "        # Analizar roles\n",
        "        roles = analyze_roles(keypoints_list, person_boxes)\n",
        "        all_pred_roles.append(roles)\n",
        "        \n",
        "        # Dibujar resultados\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            # Etiquetas: rojo para agresor, azul para víctima\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx, ky = int(kp[1] * w + x), int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 5, (255, 255, 0), -1)\n",
        "        \n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 10 == 0:\n",
        "            print(f\"Procesados {frame_count} frames\")\n",
        "    \n",
        "    cap.release()\n",
        "    out.release()\n",
        "    \n",
        "    # Calcular métricas si hay ground truth\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0])\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "    \n",
        "    print(f\"Video procesado guardado en: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# 10. Seleccionar un video y procesar\n",
        "video_path = 'fights/fi001.mp4'  # Ajusta según el nombre exacto (verifica con !ls fights/)\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo, ajusta si tienes CSV\n",
        "output_path = process_video(video_path, 'output_hockey.mp4', ground_truth_roles)\n",
        "\n",
        "# 11. Descargar resultado\n",
        "if output_path and os.path.exists(output_path):\n",
        "    files.download(output_path)\n",
        "else:\n",
        "    print(\"Error: No se generó el video de salida\")\n",
        "```\n",
        "\n",
        "### Instrucciones para Usar\n",
        "1. **Crear Notebook en Colab**:\n",
        "   - Ve a https://colab.research.google.com/, crea un nuevo notebook.\n",
        "   - Activa GPU: Entorno de ejecución → Cambiar tipo → T4 GPU.\n",
        "\n",
        "2. **Subir kaggle.json**:\n",
        "   - Descarga tu API token desde Kaggle (Account → Create API Token).\n",
        "   - En la celda de `files.upload()`, sube `kaggle.json`.\n",
        "\n",
        "3. **Verificar Videos**:\n",
        "   - Después de descomprimir (`!unzip`), lista los videos con:\n",
        "     ```python\n",
        "     !ls fights/  # O !ls no_fights/\n",
        "     ```\n",
        "   - Actualiza `video_path = 'fights/fi001.mp4'` con un nombre real (los videos son `fi001.mp4`, `fi002.mp4`, etc.).\n",
        "\n",
        "4. **Ground Truth**:\n",
        "   - El código usa un ejemplo: `[(0, 'agresor'), (1, 'víctima')]`. Para métricas reales, anota roles con CVAT o usa el CSV del dataset (si incluye labels). Ejemplo CSV:\n",
        "     ```csv\n",
        "     frame_id,person_id,role\n",
        "     1,0,agresor\n",
        "     1,1,víctima\n",
        "     ```\n",
        "     Carga con `pandas.read_csv()` y pásalo a `ground_truth_roles`.\n",
        "\n",
        "5. **Ejecutar**:\n",
        "   - Corre todas las celdas. Descargará el dataset (~100-200 MB), procesará el video y descargará `output_hockey.mp4`.\n",
        "   - Verás etiquetas: **rojo** para \"agresor\", **azul** para \"víctima\", gris para \"desconocido\".\n",
        "\n",
        "### Detalles del Código\n",
        "- **Etiquetado Visual**: Usa colores distintos (rojo/azul) para diferenciar agresor/víctima en el video, con bounding boxes verdes y keypoints amarillos.\n",
        "- **Heurística de Roles**: Detecta \"agresor\" si la mano derecha (keypoint 10) está alta (y baja en coordenadas) y cerca (<100 píxeles) de otra persona. Ajusté el threshold de 50 a 100 para captar más movimientos en peleas de hockey.\n",
        "- **Métricas**: F1-score para roles, comparando con ground truth. Puedes añadir PCK para poses:\n",
        "  ```python\n",
        "  def pck(pred_keypoints, gt_keypoints, threshold=0.05):\n",
        "      distances = np.sqrt(np.sum((pred_keypoints - gt_keypoints)**2, axis=1))\n",
        "      return np.mean(distances < threshold * 192)\n",
        "  ```\n",
        "- **Optimización**: Redimensiona frames a 640x480 para evitar saturar memoria. Limita frames si es necesario:\n",
        "  ```python\n",
        "  if frame_count > 300: break  # 10 seg a 30fps\n",
        "  ```\n",
        "\n",
        "### Consejos\n",
        "- **Debugging**: Si el video no carga, verifica el path con `!ls fights/`. Si YOLO falla, usa `yolo = YOLO('yolov8s.pt')` (más robusto pero lento).\n",
        "- **Dataset**: Los videos de \"fights\" son ideales para detectar agresor/víctima. Prueba `no_fights/` para verificar falsos positivos.\n",
        "- **Mejoras**:\n",
        "  - Usa optical flow (`cv2.calcOpticalFlowFarneback`) para confirmar movimientos agresivos.\n",
        "  - Fine-tunea MoveNet con RWF-2000 si la heurística es imprecisa.\n",
        "- **Ética**: Los videos de hockey son públicos, pero anonimiza rostros si usas datos propios (con Mediapipe).\n",
        "\n",
        "Si no tienes una cuenta de Kaggle o el dataset falla, prueba un video alternativo como `people-detection.mp4` de Intel (https://github.com/intel-iot-devkit/sample-videos). ¿Quieres ayuda con otro video, métricas específicas o ajustar la heurística? ¡Cuéntame cómo sale!"
      ],
      "metadata": {
        "id": "7cv6xIM-NKDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instalar dependencias\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics sklearn\n",
        "!pip install -q kaggle\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# 2. Configurar Kaggle API\n",
        "print(\"Sube tu kaggle.json (descárgalo desde Kaggle > Account > Create API Token)\")\n",
        "uploaded = files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 3. Descargar y descomprimir Hockey Fight Videos dataset\n",
        "!kaggle datasets download -d yassershrief/hockey-fight-vidoes\n",
        "!unzip -q hockey-fight-vidoes.zip\n",
        "\n",
        "# 4. Cargar modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Modelo YOLOv8 preentrenado\n",
        "\n",
        "# 5. Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# 6. Detectar poses\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # [persona, keypoints, (y, x, conf)]\n",
        "    return keypoints\n",
        "\n",
        "# 7. Analizar movimiento para roles (heurística: mano derecha alta y cerca)\n",
        "def analyze_roles(keypoints_list, person_boxes):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        right_wrist = keypoints[10]  # Mano derecha: [y, x, conf]\n",
        "        if right_wrist[2] > 0.3:  # Confianza suficiente\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:  # Evita compararse consigo mismo\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    if abs(wrist_x * 192 - ox) < 100 and wrist_y * 192 < oy:  # Mano alta y cerca\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'víctima'))\n",
        "                        return roles\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]  # Default si no hay detección\n",
        "\n",
        "# 8. Calcular métricas (F1-score para roles)\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        f1 = f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "        return f1\n",
        "    return 0.0\n",
        "\n",
        "# 9. Procesar video\n",
        "def process_video(video_path, output_path='output_hockey.mp4', ground_truth_roles=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir el video {video_path}\")\n",
        "        return None\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Reducir resolución para rapidez (opcional)\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        width, height = 640, 480\n",
        "\n",
        "        # Detectar personas con YOLOv8\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if box.cls == 0:  # 0 es 'person' en COCO\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Aplicar MoveNet a cada persona\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))  # Dummy si crop falla\n",
        "\n",
        "        # Analizar roles\n",
        "        roles = analyze_roles(keypoints_list, person_boxes)\n",
        "        all_pred_roles.append(roles)\n",
        "\n",
        "        # Dibujar resultados\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            # Etiquetas: rojo para agresor, azul para víctima\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx, ky = int(kp[1] * w + x), int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 5, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 10 == 0:\n",
        "            print(f\"Procesados {frame_count} frames\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Calcular métricas si hay ground truth\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0])\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    print(f\"Video procesado guardado en: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# 10. Seleccionar un video y procesar\n",
        "video_path = 'fights/fi001.mp4'  # Ajusta según el nombre exacto (verifica con !ls fights/)\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo, ajusta si tienes CSV\n",
        "output_path = process_video(video_path, 'output_hockey.mp4', ground_truth_roles)\n",
        "\n",
        "# 11. Descargar resultado\n",
        "if output_path and os.path.exists(output_path):\n",
        "    files.download(output_path)\n",
        "else:\n",
        "    print(\"Error: No se generó el video de salida\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "SSdLQSPgNKVP",
        "outputId": "48616bda-533f-470c-aff9-5d601d493f15"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Sube tu kaggle.json (descárgalo desde Kaggle > Account > Create API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f5a269d-05bd-4957-afba-060bcab02bd3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f5a269d-05bd-4957-afba-060bcab02bd3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "Dataset URL: https://www.kaggle.com/datasets/yassershrief/hockey-fight-vidoes\n",
            "License(s): unknown\n",
            "Downloading hockey-fight-vidoes.zip to /content\n",
            " 80% 131M/164M [00:00<00:00, 1.32GB/s]\n",
            "100% 164M/164M [00:00<00:00, 796MB/s] \n",
            "Error: No se pudo abrir el video fights/fi001.mp4\n",
            "Error: No se generó el video de salida\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias (con fix para kaggle)\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics sklearn\n",
        "!pip install --no-cache-dir -q kaggle  # Fix para metadata error\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Celda 2: Configurar Kaggle API (con renombrado automático)\n",
        "print(\"Sube tu kaggle.json (desde Kaggle > Account > Create API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Renombrar si es necesario (maneja 'kaggle (1).json')\n",
        "json_file = None\n",
        "for filename in uploaded.keys():\n",
        "    if 'kaggle' in filename.lower() and filename.endswith('.json'):\n",
        "        json_file = filename\n",
        "        break\n",
        "\n",
        "if json_file:\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp \"{json_file}\" ~/.kaggle/kaggle.json  # Usa comillas para espacios\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(f\"Archivo configurado: {json_file}\")\n",
        "else:\n",
        "    print(\"Error: No se encontró kaggle.json. Sube el archivo correcto.\")\n",
        "    raise ValueError(\"Sube kaggle.json\")\n",
        "\n",
        "# Celda 3: Descargar y descomprimir dataset\n",
        "!kaggle datasets download -d yassershrief/hockey-fight-vidoes\n",
        "!unzip -q hockey-fight-vidoes.zip -d /content/hockey_dataset/\n",
        "\n",
        "# Listar videos para verificar\n",
        "print(\"Videos en fights/:\")\n",
        "!ls /content/hockey_dataset/fights/ | head -10\n",
        "print(\"\\nVideos en no_fights/:\")\n",
        "!ls /content/hockey_dataset/no_fights/ | head -10\n",
        "\n",
        "# Celda 4: Cargar modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Ligero y rápido\n",
        "\n",
        "# Celda 5: Funciones (preprocesar, detectar, analizar roles)\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # [17 keypoints, (y, x, conf)]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        right_wrist = keypoints[10]  # Mano derecha\n",
        "        if right_wrist[2] > 0.3:  # Confianza\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    # Ajustado para hockey: threshold más amplio (100 px) y mano \"alta\" (baja y en coords)\n",
        "                    if abs(wrist_x * 192 - (ox + ow/2)) < 100 and wrist_y * height < oy + oh/2:  # Cerca del centro de la otra persona\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'víctima'))\n",
        "                        return roles\n",
        "    # Default\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 6: Función para procesar video\n",
        "def process_video(video_path, output_path='output_hockey.mp4', ground_truth_roles=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir {video_path}\")\n",
        "        return None\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize para eficiencia\n",
        "        frame_resized = cv2.resize(frame, (640, 480))\n",
        "        r_width, r_height = 640, 480\n",
        "\n",
        "        # Detectar personas\n",
        "        results = yolo(frame_resized)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Persona\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Poses\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame_resized[y:y+h, x:x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:  # Evita crops pequeños\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        # Roles\n",
        "        roles = analyze_roles(keypoints_list, person_boxes)\n",
        "        all_pred_roles.append(roles)\n",
        "\n",
        "        # Dibujar\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame_resized, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            # Colores: rojo=agresor, azul=víctima, gris=desconocido\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame_resized, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            # Keypoints\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame_resized, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame_resized)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:  # Progreso cada seg\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        # Limita a 300 frames (~10 seg) para pruebas rápidas\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0] if all_pred_roles else [])\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    print(f\"¡Listo! Video guardado en: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Celda 7: Procesar un video de ejemplo\n",
        "# Elige uno de fights/ (ver lista arriba). Ejemplo: fi001.mp4 (común en este dataset)\n",
        "video_path = '/content/hockey_dataset/fights/fi001.mp4'  # Ajusta si el nombre es diferente (ej. 00001.mp4)\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo; ajusta con reales\n",
        "\n",
        "if os.path.exists(video_path):\n",
        "    output_path = process_video(video_path, 'output_hockey.mp4', ground_truth_roles)\n",
        "    files.download(output_path)\n",
        "else:\n",
        "    print(f\"Video no encontrado: {video_path}. Usa !ls para ver nombres exactos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "tTzNb3LhOdp6",
        "outputId": "55ed7415-fbd2-48df-918b-d4e091ae8ff0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Sube tu kaggle.json (desde Kaggle > Account > Create API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cfd0fc09-9cdb-4fcd-90c7-01b409b11d89\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cfd0fc09-9cdb-4fcd-90c7-01b409b11d89\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "Archivo configurado: kaggle (2).json\n",
            "Dataset URL: https://www.kaggle.com/datasets/yassershrief/hockey-fight-vidoes\n",
            "License(s): unknown\n",
            "hockey-fight-vidoes.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Videos en fights/:\n",
            "ls: cannot access '/content/hockey_dataset/fights/': No such file or directory\n",
            "\n",
            "Videos en no_fights/:\n",
            "ls: cannot access '/content/hockey_dataset/no_fights/': No such file or directory\n",
            "Video no encontrado: /content/hockey_dataset/fights/fi001.mp4. Usa !ls para ver nombres exactos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics scikit-learn\n",
        "!pip install --no-cache-dir -q kaggle  # Evita error de metadata\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Celda 2: Configurar Kaggle API\n",
        "print(\"Sube tu kaggle.json (desde Kaggle > Account > Create API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "json_file = None\n",
        "for filename in uploaded.keys():\n",
        "    if 'kaggle' in filename.lower() and filename.endswith('.json'):\n",
        "        json_file = filename\n",
        "        break\n",
        "\n",
        "if json_file:\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp \"{json_file}\" ~/.kaggle/kaggle.json\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(f\"Archivo configurado: {json_file}\")\n",
        "else:\n",
        "    print(\"Error: No se encontró kaggle.json\")\n",
        "    raise ValueError(\"Sube kaggle.json\")\n",
        "\n",
        "# Celda 3: Descargar y descomprimir dataset\n",
        "# Borrar cualquier zip previo para evitar conflictos\n",
        "!rm -f hockey-fight-vidoes.zip\n",
        "# Forzar descarga\n",
        "!kaggle datasets download -d yassershrief/hockey-fight-vidoes --force\n",
        "# Crear carpeta y descomprimir\n",
        "!mkdir -p /content/hockey_dataset\n",
        "!unzip -o hockey-fight-vidoes.zip -d /content/hockey_dataset/\n",
        "\n",
        "# Verificar archivos\n",
        "print(\"Contenido de /content/hockey_dataset/:\")\n",
        "!ls -R /content/hockey_dataset/\n",
        "\n",
        "# Celda 4: Cargar modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Celda 5: Funciones\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        right_wrist = keypoints[10]  # Mano derecha\n",
        "        if right_wrist[2] > 0.3:\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    if abs(wrist_x * 192 - (ox + ow/2)) < 100 and wrist_y * height < oy + oh/2:\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'víctima'))\n",
        "                        return roles\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "def process_video(video_path, output_path='output_hockey.mp4', ground_truth_roles=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir {video_path}\")\n",
        "        return None\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))\n",
        "\n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        r_width, r_height = 640, 480\n",
        "\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        roles = analyze_roles(keypoints_list, person_boxes, r_height)\n",
        "        all_pred_roles.append(roles)\n",
        "\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0] if all_pred_roles else [])\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    print(f\"¡Listo! Video guardado en: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Celda 6: Procesar video\n",
        "# Ajusta el nombre según la salida de !ls\n",
        "video_path = '/content/hockey_dataset/fights/fi001.mp4'  # Verifica con !ls\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo\n",
        "\n",
        "if os.path.exists(video_path):\n",
        "    output_path = process_video(video_path, 'output_hockey.mp4', ground_truth_roles)\n",
        "    if output_path:\n",
        "        files.download(output_path)\n",
        "else:\n",
        "    print(f\"Video no encontrado: {video_path}\")\n",
        "    print(\"Verifica nombres con:\")\n",
        "    !ls /content/hockey_dataset/fights/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3n_s2e4lPA69",
        "outputId": "b916efc7-50d8-4fbb-ef2d-c37bafb878fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sube tu kaggle.json (desde Kaggle > Account > Create API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b5070f87-67d5-4fa2-9fd9-ca258acb9b3c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b5070f87-67d5-4fa2-9fd9-ca258acb9b3c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (3).json\n",
            "Archivo configurado: kaggle (3).json\n",
            "Dataset URL: https://www.kaggle.com/datasets/yassershrief/hockey-fight-vidoes\n",
            "License(s): unknown\n",
            "Downloading hockey-fight-vidoes.zip to /content\n",
            " 71% 116M/164M [00:00<00:00, 1.16GB/s]\n",
            "100% 164M/164M [00:00<00:00, 572MB/s] \n",
            "Archive:  hockey-fight-vidoes.zip\n",
            "  inflating: /content/hockey_dataset/data/fi100_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi101_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi102_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi103_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi104_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi105_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi106_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi107_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi108_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi109_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi10_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi110_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi111_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi112_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi113_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi114_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi115_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi116_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi117_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi118_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi119_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi11_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi120_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi121_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi122_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi123_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi124_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi125_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi126_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi127_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi128_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi129_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi12_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi130_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi131_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi132_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi133_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi134_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi135_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi136_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi137_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi138_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi139_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi13_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi140_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi141_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi142_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi143_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi144_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi145_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi146_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi147_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi148_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi149_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi14_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi150_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi151_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi152_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi153_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi154_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi155_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi156_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi157_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi158_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi159_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi15_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi160_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi161_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi162_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi163_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi164_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi165_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi166_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi167_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi168_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi169_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi16_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi170_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi171_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi172_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi173_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi174_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi175_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi176_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi177_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi178_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi179_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi17_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi180_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi181_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi182_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi183_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi184_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi185_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi186_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi187_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi188_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi189_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi18_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi190_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi191_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi192_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi193_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi194_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi195_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi196_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi197_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi198_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi199_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi19_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi1_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi200_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi201_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi202_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi203_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi204_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi205_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi206_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi207_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi208_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi209_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi20_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi210_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi211_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi212_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi213_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi214_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi215_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi216_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi217_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi218_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi219_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi21_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi220_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi221_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi222_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi223_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi224_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi225_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi226_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi227_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi228_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi229_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi22_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi230_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi231_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi232_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi233_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi234_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi235_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi236_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi237_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi238_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi239_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi23_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi240_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi241_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi242_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi243_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi244_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi245_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi246_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi247_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi248_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi249_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi24_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi250_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi251_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi252_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi253_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi254_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi255_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi256_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi257_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi258_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi259_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi25_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi260_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi261_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi262_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi263_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi264_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi265_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi266_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi267_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi268_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi269_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi26_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi270_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi271_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi272_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi273_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi274_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi275_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi276_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi277_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi278_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi279_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi27_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi280_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi281_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi282_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi283_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi284_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi285_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi286_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi287_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi288_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi289_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi28_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi290_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi291_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi292_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi293_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi294_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi295_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi296_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi297_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi298_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi299_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi29_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi2_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi300_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi301_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi302_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi303_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi304_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi305_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi306_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi307_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi308_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi309_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi30_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi310_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi311_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi312_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi313_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi314_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi315_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi316_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi317_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi318_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi319_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi31_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi320_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi321_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi322_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi323_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi324_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi325_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi326_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi327_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi328_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi329_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi32_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi330_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi331_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi332_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi333_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi334_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi335_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi336_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi337_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi338_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi339_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi33_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi340_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi341_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi342_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi343_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi344_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi345_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi346_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi347_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi348_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi349_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi34_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi350_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi351_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi352_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi353_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi354_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi355_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi356_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi357_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi358_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi359_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi35_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi360_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi361_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi362_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi363_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi364_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi365_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi366_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi367_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi368_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi369_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi36_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi370_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi371_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi372_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi373_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi374_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi375_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi376_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi377_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi378_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi379_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi37_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi380_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi381_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi382_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi383_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi384_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi385_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi386_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi387_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi388_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi389_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi38_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi390_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi391_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi392_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi393_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi394_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi395_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi396_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi397_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi398_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi399_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi39_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi3_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi400_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi401_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi402_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi403_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi404_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi405_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi406_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi407_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi408_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi409_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi40_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi410_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi411_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi412_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi413_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi414_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi415_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi416_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi417_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi418_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi419_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi41_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi420_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi421_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi422_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi423_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi424_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi425_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi426_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi427_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi428_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi429_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi42_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi430_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi431_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi432_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi433_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi434_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi435_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi436_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi437_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi438_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi439_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi43_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi440_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi441_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi442_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi443_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi444_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi445_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi446_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi447_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi448_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi449_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi44_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi450_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi451_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi452_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi453_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi454_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi455_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi456_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi457_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi458_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi459_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi45_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi460_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi461_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi462_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi463_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi464_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi465_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi466_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi467_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi468_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi469_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi46_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi470_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi471_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi472_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi473_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi474_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi475_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi476_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi477_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi478_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi479_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi47_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi480_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi481_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi482_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi483_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi484_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi485_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi486_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi487_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi488_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi489_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi48_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi490_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi491_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi492_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi493_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi494_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi495_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi496_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi497_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi498_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi499_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi49_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi4_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi500_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi50_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi51_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi52_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi53_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi54_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi55_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi56_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi57_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi58_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi59_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi5_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi60_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi61_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi62_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi63_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi64_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi65_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi66_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi67_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi68_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi69_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi6_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi70_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi71_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi72_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi73_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi74_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi75_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi76_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi77_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi78_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi79_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi7_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi80_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi81_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi82_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi83_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi84_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi85_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi86_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi87_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi88_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi89_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi8_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi90_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi91_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi92_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi93_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi94_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi95_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi96_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi97_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi98_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi99_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/fi9_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no100_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no101_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no102_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no103_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no104_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no105_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no106_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no107_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no108_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no109_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no10_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no110_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no111_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no112_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no113_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no114_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no115_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no116_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no117_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no118_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no119_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no11_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no120_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no121_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no122_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no123_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no124_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no125_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no126_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no127_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no128_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no129_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no12_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no130_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no131_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no132_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no133_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no134_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no135_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no136_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no137_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no138_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no139_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no13_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no140_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no141_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no142__xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no143_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no144_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no145_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no146_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no147_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no148_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no149_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no14_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no150_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no151_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no152_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no153_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no154_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no155_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no156_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no157_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no158_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no159_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no15_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no160_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no161_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no162_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no163_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no164_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no165_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no166_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no167_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no168_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no169_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no16_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no170_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no171_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no172_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no173_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no174_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no175_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no176_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no177_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no178_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no179_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no17_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no180_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no181_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no182_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no183_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no184_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no185_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no186_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no187_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no188_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no189_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no18_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no190_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no191_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no192_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no193_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no194_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no195_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no196_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no197_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no198_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no199_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no19_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no1_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no200_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no201_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no202_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no203_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no204_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no205_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no206_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no207_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no208_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no209_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no20_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no210_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no211_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no212_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no213_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no214_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no215_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no216_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no217_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no218_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no219_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no21_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no220_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no221_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no222_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no223_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no224_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no225_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no226_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no227_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no228_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no229_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no22_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no230_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no231_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no232_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no233_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no234_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no235_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no236_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no237_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no238mpg_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no239_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no23_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no240_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no241_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no242_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no243_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no244_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no245_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no246_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no247_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no248_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no249_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no24_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no250_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no251_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no252_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no253_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no254_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no255_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no256_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no257_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no258_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no259_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no25_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no260_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no261_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no262_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no263_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no264_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no265_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no266_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no267_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no268_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no269_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no26_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no270_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no271_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no272_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no273_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no274_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no275_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no276_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no277_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no278_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no279_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no27_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no280_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no281_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no282_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no283_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no284_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no285_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no286_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no287_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no288_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no289_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no28_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no290_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no291_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no292_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no293_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no294_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no295_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no296_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no297_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no298_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no299_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no29_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no2_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no300_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no301_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no302_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no303_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no304_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no305_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no306_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no307_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no308_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no309_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no30_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no310_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no311_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no312_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no313_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no314_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no315_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no316_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no317_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no318_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no319_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no31_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no320_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no321_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no322_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no323_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no324_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no325_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no326_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no327_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no328_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no329_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no32_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no330_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no331_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no332_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no333_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no334_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no335_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no336_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no337_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no338_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no339_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no33_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no340_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no341_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no342_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no343_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no344_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no345_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no346_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no347_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no348_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no349_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no34_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no350_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no351_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no352_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no353_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no354_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no355_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no356_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no357_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no358_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no359_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no35_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no360_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no361_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no362_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no363_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no364_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no365_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no366_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no367_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no368_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no369_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no36_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no370_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no371_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no372_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no373_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no374_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no375_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no376_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no377_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no378_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no379_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no37_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no380_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no381_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no382_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no383_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no384_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no385_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no386_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no387_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no388_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no389_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no38_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no390_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no391_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no392_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no393_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no394_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no395_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no396_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no397_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no398_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no399_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no39_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no3_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no400_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no401_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no402_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no403_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no404_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no405_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no406_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no407_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no408_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no409_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no40_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no410_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no411_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no412_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no413_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no414_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no415_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no416_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no417_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no418_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no419_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no41_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no420_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no421_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no422_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no423_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no424_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no425_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no426_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no427_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no428_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no429_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no42_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no430_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no431_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no432_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no433_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no434_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no435_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no436_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no437_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no438_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no439_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no43_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no440_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no441_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no442_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no443_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no444_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no445_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no446_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no447_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no448_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no449_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no44_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no450_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no451_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no452_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no453_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no454_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no455_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no456_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no457_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no458_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no459_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no45_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no460_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no461_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no462_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no463_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no464_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no465_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no466_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no467_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no468_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no469_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no46_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no470_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no471_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no472_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no473_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no474_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no475_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no476_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no477_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no478_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no479_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no47_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no480_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no481_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no482_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no483_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no484_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no485_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no486_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no487_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no488_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no489_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no48_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no490_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no491_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no492_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no493_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no494_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no495_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no496_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no497_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no498_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no499_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no49_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no4_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no500_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no50_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no51_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no52_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no53_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no54_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no55_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no56_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no57_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no58_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no59_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no5_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no60_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no61_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no62_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no63_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no64_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no65___xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no66_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no67_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no68_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no69_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no6_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no70_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no71_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no72_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no73_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no74_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no75_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no76_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no77_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no78_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no79_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no7_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no80_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no81_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no82_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no83_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no84_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no85_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no86_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no87_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no88_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no89_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no8_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no90_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no91_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no92_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no93_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no94_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no95_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no96_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no97_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no98_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no99_xvid.avi  \n",
            "  inflating: /content/hockey_dataset/data/no9_xvid.avi  \n",
            "Contenido de /content/hockey_dataset/:\n",
            "/content/hockey_dataset/:\n",
            "data\n",
            "\n",
            "/content/hockey_dataset/data:\n",
            "fi100_xvid.avi\tfi326_xvid.avi\tno100_xvid.avi\t   no326_xvid.avi\n",
            "fi101_xvid.avi\tfi327_xvid.avi\tno101_xvid.avi\t   no327_xvid.avi\n",
            "fi102_xvid.avi\tfi328_xvid.avi\tno102_xvid.avi\t   no328_xvid.avi\n",
            "fi103_xvid.avi\tfi329_xvid.avi\tno103_xvid.avi\t   no329_xvid.avi\n",
            "fi104_xvid.avi\tfi32_xvid.avi\tno104_xvid.avi\t   no32_xvid.avi\n",
            "fi105_xvid.avi\tfi330_xvid.avi\tno105_xvid.avi\t   no330_xvid.avi\n",
            "fi106_xvid.avi\tfi331_xvid.avi\tno106_xvid.avi\t   no331_xvid.avi\n",
            "fi107_xvid.avi\tfi332_xvid.avi\tno107_xvid.avi\t   no332_xvid.avi\n",
            "fi108_xvid.avi\tfi333_xvid.avi\tno108_xvid.avi\t   no333_xvid.avi\n",
            "fi109_xvid.avi\tfi334_xvid.avi\tno109_xvid.avi\t   no334_xvid.avi\n",
            "fi10_xvid.avi\tfi335_xvid.avi\tno10_xvid.avi\t   no335_xvid.avi\n",
            "fi110_xvid.avi\tfi336_xvid.avi\tno110_xvid.avi\t   no336_xvid.avi\n",
            "fi111_xvid.avi\tfi337_xvid.avi\tno111_xvid.avi\t   no337_xvid.avi\n",
            "fi112_xvid.avi\tfi338_xvid.avi\tno112_xvid.avi\t   no338_xvid.avi\n",
            "fi113_xvid.avi\tfi339_xvid.avi\tno113_xvid.avi\t   no339_xvid.avi\n",
            "fi114_xvid.avi\tfi33_xvid.avi\tno114_xvid.avi\t   no33_xvid.avi\n",
            "fi115_xvid.avi\tfi340_xvid.avi\tno115_xvid.avi\t   no340_xvid.avi\n",
            "fi116_xvid.avi\tfi341_xvid.avi\tno116_xvid.avi\t   no341_xvid.avi\n",
            "fi117_xvid.avi\tfi342_xvid.avi\tno117_xvid.avi\t   no342_xvid.avi\n",
            "fi118_xvid.avi\tfi343_xvid.avi\tno118_xvid.avi\t   no343_xvid.avi\n",
            "fi119_xvid.avi\tfi344_xvid.avi\tno119_xvid.avi\t   no344_xvid.avi\n",
            "fi11_xvid.avi\tfi345_xvid.avi\tno11_xvid.avi\t   no345_xvid.avi\n",
            "fi120_xvid.avi\tfi346_xvid.avi\tno120_xvid.avi\t   no346_xvid.avi\n",
            "fi121_xvid.avi\tfi347_xvid.avi\tno121_xvid.avi\t   no347_xvid.avi\n",
            "fi122_xvid.avi\tfi348_xvid.avi\tno122_xvid.avi\t   no348_xvid.avi\n",
            "fi123_xvid.avi\tfi349_xvid.avi\tno123_xvid.avi\t   no349_xvid.avi\n",
            "fi124_xvid.avi\tfi34_xvid.avi\tno124_xvid.avi\t   no34_xvid.avi\n",
            "fi125_xvid.avi\tfi350_xvid.avi\tno125_xvid.avi\t   no350_xvid.avi\n",
            "fi126_xvid.avi\tfi351_xvid.avi\tno126_xvid.avi\t   no351_xvid.avi\n",
            "fi127_xvid.avi\tfi352_xvid.avi\tno127_xvid.avi\t   no352_xvid.avi\n",
            "fi128_xvid.avi\tfi353_xvid.avi\tno128_xvid.avi\t   no353_xvid.avi\n",
            "fi129_xvid.avi\tfi354_xvid.avi\tno129_xvid.avi\t   no354_xvid.avi\n",
            "fi12_xvid.avi\tfi355_xvid.avi\tno12_xvid.avi\t   no355_xvid.avi\n",
            "fi130_xvid.avi\tfi356_xvid.avi\tno130_xvid.avi\t   no356_xvid.avi\n",
            "fi131_xvid.avi\tfi357_xvid.avi\tno131_xvid.avi\t   no357_xvid.avi\n",
            "fi132_xvid.avi\tfi358_xvid.avi\tno132_xvid.avi\t   no358_xvid.avi\n",
            "fi133_xvid.avi\tfi359_xvid.avi\tno133_xvid.avi\t   no359_xvid.avi\n",
            "fi134_xvid.avi\tfi35_xvid.avi\tno134_xvid.avi\t   no35_xvid.avi\n",
            "fi135_xvid.avi\tfi360_xvid.avi\tno135_xvid.avi\t   no360_xvid.avi\n",
            "fi136_xvid.avi\tfi361_xvid.avi\tno136_xvid.avi\t   no361_xvid.avi\n",
            "fi137_xvid.avi\tfi362_xvid.avi\tno137_xvid.avi\t   no362_xvid.avi\n",
            "fi138_xvid.avi\tfi363_xvid.avi\tno138_xvid.avi\t   no363_xvid.avi\n",
            "fi139_xvid.avi\tfi364_xvid.avi\tno139_xvid.avi\t   no364_xvid.avi\n",
            "fi13_xvid.avi\tfi365_xvid.avi\tno13_xvid.avi\t   no365_xvid.avi\n",
            "fi140_xvid.avi\tfi366_xvid.avi\tno140_xvid.avi\t   no366_xvid.avi\n",
            "fi141_xvid.avi\tfi367_xvid.avi\tno141_xvid.avi\t   no367_xvid.avi\n",
            "fi142_xvid.avi\tfi368_xvid.avi\tno142__xvid.avi    no368_xvid.avi\n",
            "fi143_xvid.avi\tfi369_xvid.avi\tno143_xvid.avi\t   no369_xvid.avi\n",
            "fi144_xvid.avi\tfi36_xvid.avi\tno144_xvid.avi\t   no36_xvid.avi\n",
            "fi145_xvid.avi\tfi370_xvid.avi\tno145_xvid.avi\t   no370_xvid.avi\n",
            "fi146_xvid.avi\tfi371_xvid.avi\tno146_xvid.avi\t   no371_xvid.avi\n",
            "fi147_xvid.avi\tfi372_xvid.avi\tno147_xvid.avi\t   no372_xvid.avi\n",
            "fi148_xvid.avi\tfi373_xvid.avi\tno148_xvid.avi\t   no373_xvid.avi\n",
            "fi149_xvid.avi\tfi374_xvid.avi\tno149_xvid.avi\t   no374_xvid.avi\n",
            "fi14_xvid.avi\tfi375_xvid.avi\tno14_xvid.avi\t   no375_xvid.avi\n",
            "fi150_xvid.avi\tfi376_xvid.avi\tno150_xvid.avi\t   no376_xvid.avi\n",
            "fi151_xvid.avi\tfi377_xvid.avi\tno151_xvid.avi\t   no377_xvid.avi\n",
            "fi152_xvid.avi\tfi378_xvid.avi\tno152_xvid.avi\t   no378_xvid.avi\n",
            "fi153_xvid.avi\tfi379_xvid.avi\tno153_xvid.avi\t   no379_xvid.avi\n",
            "fi154_xvid.avi\tfi37_xvid.avi\tno154_xvid.avi\t   no37_xvid.avi\n",
            "fi155_xvid.avi\tfi380_xvid.avi\tno155_xvid.avi\t   no380_xvid.avi\n",
            "fi156_xvid.avi\tfi381_xvid.avi\tno156_xvid.avi\t   no381_xvid.avi\n",
            "fi157_xvid.avi\tfi382_xvid.avi\tno157_xvid.avi\t   no382_xvid.avi\n",
            "fi158_xvid.avi\tfi383_xvid.avi\tno158_xvid.avi\t   no383_xvid.avi\n",
            "fi159_xvid.avi\tfi384_xvid.avi\tno159_xvid.avi\t   no384_xvid.avi\n",
            "fi15_xvid.avi\tfi385_xvid.avi\tno15_xvid.avi\t   no385_xvid.avi\n",
            "fi160_xvid.avi\tfi386_xvid.avi\tno160_xvid.avi\t   no386_xvid.avi\n",
            "fi161_xvid.avi\tfi387_xvid.avi\tno161_xvid.avi\t   no387_xvid.avi\n",
            "fi162_xvid.avi\tfi388_xvid.avi\tno162_xvid.avi\t   no388_xvid.avi\n",
            "fi163_xvid.avi\tfi389_xvid.avi\tno163_xvid.avi\t   no389_xvid.avi\n",
            "fi164_xvid.avi\tfi38_xvid.avi\tno164_xvid.avi\t   no38_xvid.avi\n",
            "fi165_xvid.avi\tfi390_xvid.avi\tno165_xvid.avi\t   no390_xvid.avi\n",
            "fi166_xvid.avi\tfi391_xvid.avi\tno166_xvid.avi\t   no391_xvid.avi\n",
            "fi167_xvid.avi\tfi392_xvid.avi\tno167_xvid.avi\t   no392_xvid.avi\n",
            "fi168_xvid.avi\tfi393_xvid.avi\tno168_xvid.avi\t   no393_xvid.avi\n",
            "fi169_xvid.avi\tfi394_xvid.avi\tno169_xvid.avi\t   no394_xvid.avi\n",
            "fi16_xvid.avi\tfi395_xvid.avi\tno16_xvid.avi\t   no395_xvid.avi\n",
            "fi170_xvid.avi\tfi396_xvid.avi\tno170_xvid.avi\t   no396_xvid.avi\n",
            "fi171_xvid.avi\tfi397_xvid.avi\tno171_xvid.avi\t   no397_xvid.avi\n",
            "fi172_xvid.avi\tfi398_xvid.avi\tno172_xvid.avi\t   no398_xvid.avi\n",
            "fi173_xvid.avi\tfi399_xvid.avi\tno173_xvid.avi\t   no399_xvid.avi\n",
            "fi174_xvid.avi\tfi39_xvid.avi\tno174_xvid.avi\t   no39_xvid.avi\n",
            "fi175_xvid.avi\tfi3_xvid.avi\tno175_xvid.avi\t   no3_xvid.avi\n",
            "fi176_xvid.avi\tfi400_xvid.avi\tno176_xvid.avi\t   no400_xvid.avi\n",
            "fi177_xvid.avi\tfi401_xvid.avi\tno177_xvid.avi\t   no401_xvid.avi\n",
            "fi178_xvid.avi\tfi402_xvid.avi\tno178_xvid.avi\t   no402_xvid.avi\n",
            "fi179_xvid.avi\tfi403_xvid.avi\tno179_xvid.avi\t   no403_xvid.avi\n",
            "fi17_xvid.avi\tfi404_xvid.avi\tno17_xvid.avi\t   no404_xvid.avi\n",
            "fi180_xvid.avi\tfi405_xvid.avi\tno180_xvid.avi\t   no405_xvid.avi\n",
            "fi181_xvid.avi\tfi406_xvid.avi\tno181_xvid.avi\t   no406_xvid.avi\n",
            "fi182_xvid.avi\tfi407_xvid.avi\tno182_xvid.avi\t   no407_xvid.avi\n",
            "fi183_xvid.avi\tfi408_xvid.avi\tno183_xvid.avi\t   no408_xvid.avi\n",
            "fi184_xvid.avi\tfi409_xvid.avi\tno184_xvid.avi\t   no409_xvid.avi\n",
            "fi185_xvid.avi\tfi40_xvid.avi\tno185_xvid.avi\t   no40_xvid.avi\n",
            "fi186_xvid.avi\tfi410_xvid.avi\tno186_xvid.avi\t   no410_xvid.avi\n",
            "fi187_xvid.avi\tfi411_xvid.avi\tno187_xvid.avi\t   no411_xvid.avi\n",
            "fi188_xvid.avi\tfi412_xvid.avi\tno188_xvid.avi\t   no412_xvid.avi\n",
            "fi189_xvid.avi\tfi413_xvid.avi\tno189_xvid.avi\t   no413_xvid.avi\n",
            "fi18_xvid.avi\tfi414_xvid.avi\tno18_xvid.avi\t   no414_xvid.avi\n",
            "fi190_xvid.avi\tfi415_xvid.avi\tno190_xvid.avi\t   no415_xvid.avi\n",
            "fi191_xvid.avi\tfi416_xvid.avi\tno191_xvid.avi\t   no416_xvid.avi\n",
            "fi192_xvid.avi\tfi417_xvid.avi\tno192_xvid.avi\t   no417_xvid.avi\n",
            "fi193_xvid.avi\tfi418_xvid.avi\tno193_xvid.avi\t   no418_xvid.avi\n",
            "fi194_xvid.avi\tfi419_xvid.avi\tno194_xvid.avi\t   no419_xvid.avi\n",
            "fi195_xvid.avi\tfi41_xvid.avi\tno195_xvid.avi\t   no41_xvid.avi\n",
            "fi196_xvid.avi\tfi420_xvid.avi\tno196_xvid.avi\t   no420_xvid.avi\n",
            "fi197_xvid.avi\tfi421_xvid.avi\tno197_xvid.avi\t   no421_xvid.avi\n",
            "fi198_xvid.avi\tfi422_xvid.avi\tno198_xvid.avi\t   no422_xvid.avi\n",
            "fi199_xvid.avi\tfi423_xvid.avi\tno199_xvid.avi\t   no423_xvid.avi\n",
            "fi19_xvid.avi\tfi424_xvid.avi\tno19_xvid.avi\t   no424_xvid.avi\n",
            "fi1_xvid.avi\tfi425_xvid.avi\tno1_xvid.avi\t   no425_xvid.avi\n",
            "fi200_xvid.avi\tfi426_xvid.avi\tno200_xvid.avi\t   no426_xvid.avi\n",
            "fi201_xvid.avi\tfi427_xvid.avi\tno201_xvid.avi\t   no427_xvid.avi\n",
            "fi202_xvid.avi\tfi428_xvid.avi\tno202_xvid.avi\t   no428_xvid.avi\n",
            "fi203_xvid.avi\tfi429_xvid.avi\tno203_xvid.avi\t   no429_xvid.avi\n",
            "fi204_xvid.avi\tfi42_xvid.avi\tno204_xvid.avi\t   no42_xvid.avi\n",
            "fi205_xvid.avi\tfi430_xvid.avi\tno205_xvid.avi\t   no430_xvid.avi\n",
            "fi206_xvid.avi\tfi431_xvid.avi\tno206_xvid.avi\t   no431_xvid.avi\n",
            "fi207_xvid.avi\tfi432_xvid.avi\tno207_xvid.avi\t   no432_xvid.avi\n",
            "fi208_xvid.avi\tfi433_xvid.avi\tno208_xvid.avi\t   no433_xvid.avi\n",
            "fi209_xvid.avi\tfi434_xvid.avi\tno209_xvid.avi\t   no434_xvid.avi\n",
            "fi20_xvid.avi\tfi435_xvid.avi\tno20_xvid.avi\t   no435_xvid.avi\n",
            "fi210_xvid.avi\tfi436_xvid.avi\tno210_xvid.avi\t   no436_xvid.avi\n",
            "fi211_xvid.avi\tfi437_xvid.avi\tno211_xvid.avi\t   no437_xvid.avi\n",
            "fi212_xvid.avi\tfi438_xvid.avi\tno212_xvid.avi\t   no438_xvid.avi\n",
            "fi213_xvid.avi\tfi439_xvid.avi\tno213_xvid.avi\t   no439_xvid.avi\n",
            "fi214_xvid.avi\tfi43_xvid.avi\tno214_xvid.avi\t   no43_xvid.avi\n",
            "fi215_xvid.avi\tfi440_xvid.avi\tno215_xvid.avi\t   no440_xvid.avi\n",
            "fi216_xvid.avi\tfi441_xvid.avi\tno216_xvid.avi\t   no441_xvid.avi\n",
            "fi217_xvid.avi\tfi442_xvid.avi\tno217_xvid.avi\t   no442_xvid.avi\n",
            "fi218_xvid.avi\tfi443_xvid.avi\tno218_xvid.avi\t   no443_xvid.avi\n",
            "fi219_xvid.avi\tfi444_xvid.avi\tno219_xvid.avi\t   no444_xvid.avi\n",
            "fi21_xvid.avi\tfi445_xvid.avi\tno21_xvid.avi\t   no445_xvid.avi\n",
            "fi220_xvid.avi\tfi446_xvid.avi\tno220_xvid.avi\t   no446_xvid.avi\n",
            "fi221_xvid.avi\tfi447_xvid.avi\tno221_xvid.avi\t   no447_xvid.avi\n",
            "fi222_xvid.avi\tfi448_xvid.avi\tno222_xvid.avi\t   no448_xvid.avi\n",
            "fi223_xvid.avi\tfi449_xvid.avi\tno223_xvid.avi\t   no449_xvid.avi\n",
            "fi224_xvid.avi\tfi44_xvid.avi\tno224_xvid.avi\t   no44_xvid.avi\n",
            "fi225_xvid.avi\tfi450_xvid.avi\tno225_xvid.avi\t   no450_xvid.avi\n",
            "fi226_xvid.avi\tfi451_xvid.avi\tno226_xvid.avi\t   no451_xvid.avi\n",
            "fi227_xvid.avi\tfi452_xvid.avi\tno227_xvid.avi\t   no452_xvid.avi\n",
            "fi228_xvid.avi\tfi453_xvid.avi\tno228_xvid.avi\t   no453_xvid.avi\n",
            "fi229_xvid.avi\tfi454_xvid.avi\tno229_xvid.avi\t   no454_xvid.avi\n",
            "fi22_xvid.avi\tfi455_xvid.avi\tno22_xvid.avi\t   no455_xvid.avi\n",
            "fi230_xvid.avi\tfi456_xvid.avi\tno230_xvid.avi\t   no456_xvid.avi\n",
            "fi231_xvid.avi\tfi457_xvid.avi\tno231_xvid.avi\t   no457_xvid.avi\n",
            "fi232_xvid.avi\tfi458_xvid.avi\tno232_xvid.avi\t   no458_xvid.avi\n",
            "fi233_xvid.avi\tfi459_xvid.avi\tno233_xvid.avi\t   no459_xvid.avi\n",
            "fi234_xvid.avi\tfi45_xvid.avi\tno234_xvid.avi\t   no45_xvid.avi\n",
            "fi235_xvid.avi\tfi460_xvid.avi\tno235_xvid.avi\t   no460_xvid.avi\n",
            "fi236_xvid.avi\tfi461_xvid.avi\tno236_xvid.avi\t   no461_xvid.avi\n",
            "fi237_xvid.avi\tfi462_xvid.avi\tno237_xvid.avi\t   no462_xvid.avi\n",
            "fi238_xvid.avi\tfi463_xvid.avi\tno238mpg_xvid.avi  no463_xvid.avi\n",
            "fi239_xvid.avi\tfi464_xvid.avi\tno239_xvid.avi\t   no464_xvid.avi\n",
            "fi23_xvid.avi\tfi465_xvid.avi\tno23_xvid.avi\t   no465_xvid.avi\n",
            "fi240_xvid.avi\tfi466_xvid.avi\tno240_xvid.avi\t   no466_xvid.avi\n",
            "fi241_xvid.avi\tfi467_xvid.avi\tno241_xvid.avi\t   no467_xvid.avi\n",
            "fi242_xvid.avi\tfi468_xvid.avi\tno242_xvid.avi\t   no468_xvid.avi\n",
            "fi243_xvid.avi\tfi469_xvid.avi\tno243_xvid.avi\t   no469_xvid.avi\n",
            "fi244_xvid.avi\tfi46_xvid.avi\tno244_xvid.avi\t   no46_xvid.avi\n",
            "fi245_xvid.avi\tfi470_xvid.avi\tno245_xvid.avi\t   no470_xvid.avi\n",
            "fi246_xvid.avi\tfi471_xvid.avi\tno246_xvid.avi\t   no471_xvid.avi\n",
            "fi247_xvid.avi\tfi472_xvid.avi\tno247_xvid.avi\t   no472_xvid.avi\n",
            "fi248_xvid.avi\tfi473_xvid.avi\tno248_xvid.avi\t   no473_xvid.avi\n",
            "fi249_xvid.avi\tfi474_xvid.avi\tno249_xvid.avi\t   no474_xvid.avi\n",
            "fi24_xvid.avi\tfi475_xvid.avi\tno24_xvid.avi\t   no475_xvid.avi\n",
            "fi250_xvid.avi\tfi476_xvid.avi\tno250_xvid.avi\t   no476_xvid.avi\n",
            "fi251_xvid.avi\tfi477_xvid.avi\tno251_xvid.avi\t   no477_xvid.avi\n",
            "fi252_xvid.avi\tfi478_xvid.avi\tno252_xvid.avi\t   no478_xvid.avi\n",
            "fi253_xvid.avi\tfi479_xvid.avi\tno253_xvid.avi\t   no479_xvid.avi\n",
            "fi254_xvid.avi\tfi47_xvid.avi\tno254_xvid.avi\t   no47_xvid.avi\n",
            "fi255_xvid.avi\tfi480_xvid.avi\tno255_xvid.avi\t   no480_xvid.avi\n",
            "fi256_xvid.avi\tfi481_xvid.avi\tno256_xvid.avi\t   no481_xvid.avi\n",
            "fi257_xvid.avi\tfi482_xvid.avi\tno257_xvid.avi\t   no482_xvid.avi\n",
            "fi258_xvid.avi\tfi483_xvid.avi\tno258_xvid.avi\t   no483_xvid.avi\n",
            "fi259_xvid.avi\tfi484_xvid.avi\tno259_xvid.avi\t   no484_xvid.avi\n",
            "fi25_xvid.avi\tfi485_xvid.avi\tno25_xvid.avi\t   no485_xvid.avi\n",
            "fi260_xvid.avi\tfi486_xvid.avi\tno260_xvid.avi\t   no486_xvid.avi\n",
            "fi261_xvid.avi\tfi487_xvid.avi\tno261_xvid.avi\t   no487_xvid.avi\n",
            "fi262_xvid.avi\tfi488_xvid.avi\tno262_xvid.avi\t   no488_xvid.avi\n",
            "fi263_xvid.avi\tfi489_xvid.avi\tno263_xvid.avi\t   no489_xvid.avi\n",
            "fi264_xvid.avi\tfi48_xvid.avi\tno264_xvid.avi\t   no48_xvid.avi\n",
            "fi265_xvid.avi\tfi490_xvid.avi\tno265_xvid.avi\t   no490_xvid.avi\n",
            "fi266_xvid.avi\tfi491_xvid.avi\tno266_xvid.avi\t   no491_xvid.avi\n",
            "fi267_xvid.avi\tfi492_xvid.avi\tno267_xvid.avi\t   no492_xvid.avi\n",
            "fi268_xvid.avi\tfi493_xvid.avi\tno268_xvid.avi\t   no493_xvid.avi\n",
            "fi269_xvid.avi\tfi494_xvid.avi\tno269_xvid.avi\t   no494_xvid.avi\n",
            "fi26_xvid.avi\tfi495_xvid.avi\tno26_xvid.avi\t   no495_xvid.avi\n",
            "fi270_xvid.avi\tfi496_xvid.avi\tno270_xvid.avi\t   no496_xvid.avi\n",
            "fi271_xvid.avi\tfi497_xvid.avi\tno271_xvid.avi\t   no497_xvid.avi\n",
            "fi272_xvid.avi\tfi498_xvid.avi\tno272_xvid.avi\t   no498_xvid.avi\n",
            "fi273_xvid.avi\tfi499_xvid.avi\tno273_xvid.avi\t   no499_xvid.avi\n",
            "fi274_xvid.avi\tfi49_xvid.avi\tno274_xvid.avi\t   no49_xvid.avi\n",
            "fi275_xvid.avi\tfi4_xvid.avi\tno275_xvid.avi\t   no4_xvid.avi\n",
            "fi276_xvid.avi\tfi500_xvid.avi\tno276_xvid.avi\t   no500_xvid.avi\n",
            "fi277_xvid.avi\tfi50_xvid.avi\tno277_xvid.avi\t   no50_xvid.avi\n",
            "fi278_xvid.avi\tfi51_xvid.avi\tno278_xvid.avi\t   no51_xvid.avi\n",
            "fi279_xvid.avi\tfi52_xvid.avi\tno279_xvid.avi\t   no52_xvid.avi\n",
            "fi27_xvid.avi\tfi53_xvid.avi\tno27_xvid.avi\t   no53_xvid.avi\n",
            "fi280_xvid.avi\tfi54_xvid.avi\tno280_xvid.avi\t   no54_xvid.avi\n",
            "fi281_xvid.avi\tfi55_xvid.avi\tno281_xvid.avi\t   no55_xvid.avi\n",
            "fi282_xvid.avi\tfi56_xvid.avi\tno282_xvid.avi\t   no56_xvid.avi\n",
            "fi283_xvid.avi\tfi57_xvid.avi\tno283_xvid.avi\t   no57_xvid.avi\n",
            "fi284_xvid.avi\tfi58_xvid.avi\tno284_xvid.avi\t   no58_xvid.avi\n",
            "fi285_xvid.avi\tfi59_xvid.avi\tno285_xvid.avi\t   no59_xvid.avi\n",
            "fi286_xvid.avi\tfi5_xvid.avi\tno286_xvid.avi\t   no5_xvid.avi\n",
            "fi287_xvid.avi\tfi60_xvid.avi\tno287_xvid.avi\t   no60_xvid.avi\n",
            "fi288_xvid.avi\tfi61_xvid.avi\tno288_xvid.avi\t   no61_xvid.avi\n",
            "fi289_xvid.avi\tfi62_xvid.avi\tno289_xvid.avi\t   no62_xvid.avi\n",
            "fi28_xvid.avi\tfi63_xvid.avi\tno28_xvid.avi\t   no63_xvid.avi\n",
            "fi290_xvid.avi\tfi64_xvid.avi\tno290_xvid.avi\t   no64_xvid.avi\n",
            "fi291_xvid.avi\tfi65_xvid.avi\tno291_xvid.avi\t   no65___xvid.avi\n",
            "fi292_xvid.avi\tfi66_xvid.avi\tno292_xvid.avi\t   no66_xvid.avi\n",
            "fi293_xvid.avi\tfi67_xvid.avi\tno293_xvid.avi\t   no67_xvid.avi\n",
            "fi294_xvid.avi\tfi68_xvid.avi\tno294_xvid.avi\t   no68_xvid.avi\n",
            "fi295_xvid.avi\tfi69_xvid.avi\tno295_xvid.avi\t   no69_xvid.avi\n",
            "fi296_xvid.avi\tfi6_xvid.avi\tno296_xvid.avi\t   no6_xvid.avi\n",
            "fi297_xvid.avi\tfi70_xvid.avi\tno297_xvid.avi\t   no70_xvid.avi\n",
            "fi298_xvid.avi\tfi71_xvid.avi\tno298_xvid.avi\t   no71_xvid.avi\n",
            "fi299_xvid.avi\tfi72_xvid.avi\tno299_xvid.avi\t   no72_xvid.avi\n",
            "fi29_xvid.avi\tfi73_xvid.avi\tno29_xvid.avi\t   no73_xvid.avi\n",
            "fi2_xvid.avi\tfi74_xvid.avi\tno2_xvid.avi\t   no74_xvid.avi\n",
            "fi300_xvid.avi\tfi75_xvid.avi\tno300_xvid.avi\t   no75_xvid.avi\n",
            "fi301_xvid.avi\tfi76_xvid.avi\tno301_xvid.avi\t   no76_xvid.avi\n",
            "fi302_xvid.avi\tfi77_xvid.avi\tno302_xvid.avi\t   no77_xvid.avi\n",
            "fi303_xvid.avi\tfi78_xvid.avi\tno303_xvid.avi\t   no78_xvid.avi\n",
            "fi304_xvid.avi\tfi79_xvid.avi\tno304_xvid.avi\t   no79_xvid.avi\n",
            "fi305_xvid.avi\tfi7_xvid.avi\tno305_xvid.avi\t   no7_xvid.avi\n",
            "fi306_xvid.avi\tfi80_xvid.avi\tno306_xvid.avi\t   no80_xvid.avi\n",
            "fi307_xvid.avi\tfi81_xvid.avi\tno307_xvid.avi\t   no81_xvid.avi\n",
            "fi308_xvid.avi\tfi82_xvid.avi\tno308_xvid.avi\t   no82_xvid.avi\n",
            "fi309_xvid.avi\tfi83_xvid.avi\tno309_xvid.avi\t   no83_xvid.avi\n",
            "fi30_xvid.avi\tfi84_xvid.avi\tno30_xvid.avi\t   no84_xvid.avi\n",
            "fi310_xvid.avi\tfi85_xvid.avi\tno310_xvid.avi\t   no85_xvid.avi\n",
            "fi311_xvid.avi\tfi86_xvid.avi\tno311_xvid.avi\t   no86_xvid.avi\n",
            "fi312_xvid.avi\tfi87_xvid.avi\tno312_xvid.avi\t   no87_xvid.avi\n",
            "fi313_xvid.avi\tfi88_xvid.avi\tno313_xvid.avi\t   no88_xvid.avi\n",
            "fi314_xvid.avi\tfi89_xvid.avi\tno314_xvid.avi\t   no89_xvid.avi\n",
            "fi315_xvid.avi\tfi8_xvid.avi\tno315_xvid.avi\t   no8_xvid.avi\n",
            "fi316_xvid.avi\tfi90_xvid.avi\tno316_xvid.avi\t   no90_xvid.avi\n",
            "fi317_xvid.avi\tfi91_xvid.avi\tno317_xvid.avi\t   no91_xvid.avi\n",
            "fi318_xvid.avi\tfi92_xvid.avi\tno318_xvid.avi\t   no92_xvid.avi\n",
            "fi319_xvid.avi\tfi93_xvid.avi\tno319_xvid.avi\t   no93_xvid.avi\n",
            "fi31_xvid.avi\tfi94_xvid.avi\tno31_xvid.avi\t   no94_xvid.avi\n",
            "fi320_xvid.avi\tfi95_xvid.avi\tno320_xvid.avi\t   no95_xvid.avi\n",
            "fi321_xvid.avi\tfi96_xvid.avi\tno321_xvid.avi\t   no96_xvid.avi\n",
            "fi322_xvid.avi\tfi97_xvid.avi\tno322_xvid.avi\t   no97_xvid.avi\n",
            "fi323_xvid.avi\tfi98_xvid.avi\tno323_xvid.avi\t   no98_xvid.avi\n",
            "fi324_xvid.avi\tfi99_xvid.avi\tno324_xvid.avi\t   no99_xvid.avi\n",
            "fi325_xvid.avi\tfi9_xvid.avi\tno325_xvid.avi\t   no9_xvid.avi\n",
            "Video no encontrado: /content/hockey_dataset/fights/fi001.mp4\n",
            "Verifica nombres con:\n",
            "ls: cannot access '/content/hockey_dataset/fights/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics scikit-learn\n",
        "!pip install --no-cache-dir -q kaggle\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Celda 2: Configurar Kaggle API (ya lo tienes, pero por si acaso)\n",
        "print(\"Si ya subiste kaggle.json, salta esto. De lo contrario, sube.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "json_file = next((f for f in uploaded.keys() if 'kaggle' in f.lower() and f.endswith('.json')), None)\n",
        "if json_file:\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp \"{json_file}\" ~/.kaggle/kaggle.json\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Celda 3: Verificar dataset (ya descargado)\n",
        "dataset_dir = '/content/hockey_dataset/data'\n",
        "if os.path.exists(dataset_dir):\n",
        "    fight_files = [f for f in os.listdir(dataset_dir) if f.startswith('fi')]\n",
        "    non_fight_files = [f for f in os.listdir(dataset_dir) if f.startswith('no')]\n",
        "    print(f\"Total videos de peleas (fi*): {len(fight_files)}\")\n",
        "    print(f\"Primeros 10: {fight_files[:10]}\")\n",
        "    print(f\"Total videos sin peleas (no*): {len(non_fight_files)}\")\n",
        "    print(f\"Primeros 10: {non_fight_files[:10]}\")\n",
        "else:\n",
        "    print(\"Dataset no encontrado. Ejecuta descarga manual si es necesario.\")\n",
        "    # Si no, fuerza descarga de nuevo\n",
        "    !rm -f hockey-fight-vidoes.zip\n",
        "    !kaggle datasets download -d yassershrief/hockey-fight-vidoes --force\n",
        "    !mkdir -p /content/hockey_dataset\n",
        "    !unzip -o hockey-fight-vidoes.zip -d /content/hockey_dataset/\n",
        "\n",
        "# Celda 4: Cargar modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Celda 5: Funciones\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        right_wrist = keypoints[10]  # Mano derecha [y, x, conf]\n",
        "        if right_wrist[2] > 0.3:\n",
        "            wrist_y, wrist_x = right_wrist[0], right_wrist[1]\n",
        "            for j, other_box in enumerate(person_boxes):\n",
        "                if i != j:\n",
        "                    ox, oy, ow, oh = other_box\n",
        "                    # Heurística ajustada para hockey: mano cerca del centro de la otra persona\n",
        "                    center_other_x = ox + ow / 2\n",
        "                    center_other_y = oy + oh / 2\n",
        "                    if abs(wrist_x * 192 - center_other_x) < 100 and wrist_y * height < center_other_y:\n",
        "                        roles.append((i, 'agresor'))\n",
        "                        roles.append((j, 'víctima'))\n",
        "                        return roles\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 6: Procesar video\n",
        "def process_video(video_path, output_path='output_hockey.mp4', ground_truth_roles=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir {video_path}. Verifica si es AVI válido.\")\n",
        "        return None\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))  # Salida en MP4\n",
        "\n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        r_height = 480  # Altura redimensionada\n",
        "\n",
        "        # Detectar personas\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Poses\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        # Roles\n",
        "        roles = analyze_roles(keypoints_list, person_boxes, r_height)\n",
        "        all_pred_roles.append(roles)\n",
        "\n",
        "        # Dibujar\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Box verde\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)  # Keypoints amarillos\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:  # Limite para pruebas\n",
        "            print(\"Procesamiento limitado a 300 frames para prueba rápida.\")\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0] if all_pred_roles else [])\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    print(f\"¡Listo! Video guardado en: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Celda 7: Procesar un video de ejemplo\n",
        "video_path = '/content/hockey_dataset/data/fi1_xvid.avi'  # Video de pelea (ajusta si quieres otro, ej. fi2_xvid.avi)\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo; ajusta si tienes anotaciones\n",
        "\n",
        "if os.path.exists(video_path):\n",
        "    output_path = process_video(video_path, 'output_hockey.mp4', ground_truth_roles)\n",
        "    if output_path:\n",
        "        files.download(output_path)\n",
        "else:\n",
        "    print(f\"Video no encontrado: {video_path}\")\n",
        "    print(\"Verifica nombres con:\")\n",
        "    !ls /content/hockey_dataset/data/ | grep fi | head -10  # Primeros 10 fi*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7bi6kalRQNuj",
        "outputId": "c332d4e6-c2d4-4c03-f465-20d76026fb2d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Si ya subiste kaggle.json, salta esto. De lo contrario, sube.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5de3fea8-d9f8-4309-ae26-3b4945c401cb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5de3fea8-d9f8-4309-ae26-3b4945c401cb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (4).json\n",
            "Total videos de peleas (fi*): 500\n",
            "Primeros 10: ['fi182_xvid.avi', 'fi132_xvid.avi', 'fi86_xvid.avi', 'fi159_xvid.avi', 'fi116_xvid.avi', 'fi453_xvid.avi', 'fi305_xvid.avi', 'fi434_xvid.avi', 'fi174_xvid.avi', 'fi123_xvid.avi']\n",
            "Total videos sin peleas (no*): 500\n",
            "Primeros 10: ['no24_xvid.avi', 'no217_xvid.avi', 'no105_xvid.avi', 'no304_xvid.avi', 'no286_xvid.avi', 'no193_xvid.avi', 'no230_xvid.avi', 'no47_xvid.avi', 'no198_xvid.avi', 'no338_xvid.avi']\n",
            "\n",
            "0: 480x640 2 persons, 9.7ms\n",
            "Speed: 1.7ms preprocess, 9.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 persons, 6.9ms\n",
            "Speed: 1.7ms preprocess, 6.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 1.6ms preprocess, 7.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.0ms\n",
            "Speed: 1.7ms preprocess, 7.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 persons, 6.9ms\n",
            "Speed: 2.0ms preprocess, 6.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 7.0ms\n",
            "Speed: 1.3ms preprocess, 7.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.0ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.6ms\n",
            "Speed: 1.6ms preprocess, 6.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 refrigerator, 9.9ms\n",
            "Speed: 1.6ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.6ms\n",
            "Speed: 2.2ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 6.3ms\n",
            "Speed: 2.0ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.9ms\n",
            "Speed: 2.0ms preprocess, 6.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 6.3ms\n",
            "Speed: 2.4ms preprocess, 6.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.5ms\n",
            "Speed: 1.7ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 8.3ms\n",
            "Speed: 1.7ms preprocess, 8.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.8ms\n",
            "Speed: 1.5ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.3ms\n",
            "Speed: 2.2ms preprocess, 6.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 dog, 6.8ms\n",
            "Speed: 2.3ms preprocess, 6.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 cats, 1 tv, 6.4ms\n",
            "Speed: 1.7ms preprocess, 6.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 6.6ms\n",
            "Speed: 1.6ms preprocess, 6.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.2ms\n",
            "Speed: 1.8ms preprocess, 7.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.2ms\n",
            "Speed: 1.9ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 1.9ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 7.1ms\n",
            "Speed: 1.5ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 6.6ms\n",
            "Speed: 1.7ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 6.9ms\n",
            "Speed: 1.7ms preprocess, 6.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 person, 6.5ms\n",
            "Speed: 2.0ms preprocess, 6.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.0ms\n",
            "Speed: 1.5ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 1.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.6ms\n",
            "Speed: 1.6ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 persons, 6.5ms\n",
            "Speed: 1.9ms preprocess, 6.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 persons, 6.4ms\n",
            "Speed: 1.9ms preprocess, 6.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 6.5ms\n",
            "Speed: 1.7ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 6.7ms\n",
            "Speed: 1.7ms preprocess, 6.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 persons, 1 handbag, 6.7ms\n",
            "Speed: 1.8ms preprocess, 6.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "F1-Score para roles: 0.0000\n",
            "¡Listo! Video guardado en: output_hockey.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8a18c0d1-6d22-4e49-8409-682b2e1231a7\", \"output_hockey.mp4\", 856103)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1-3: Asume que ya tienes el dataset descargado. Si no, ejecuta las celdas anteriores para descargar.\n",
        "\n",
        "# Celda 4: Cargar modelos (si no lo hiciste)\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Celda 5: Funciones con heurística mejorada\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        # Verificar ambas manos (keypoint 9: izquierda, 10: derecha)\n",
        "        hands = [keypoints[9], keypoints[10]]  # [y, x, conf]\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:  # Confianza suficiente\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.5:  # Mano \"alta\" (y bajo en coordenadas normalizadas)\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 80 and dist_y < 80:  # Cerca en x e y\n",
        "                                print(f\"Detectado agresor {i} (mano en y={hand_y:.2f}, dist_x={dist_x:.2f}, dist_y={dist_y:.2f}) cerca de víctima {j}\")\n",
        "                                roles.append((i, 'agresor'))\n",
        "                                roles.append((j, 'víctima'))\n",
        "                                return roles\n",
        "    print(\"No se detectó interacción: manos bajas o lejanas.\")\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 6: Procesar video\n",
        "def process_video(video_path, output_path='output_hockey.mp4', ground_truth_roles=None):\n",
        "    # Opcional: Convertir AVI a MP4 si cv2 falla (descomenta si necesario)\n",
        "    # !ffmpeg -i \"{video_path}\" temp.mp4 -y\n",
        "    # video_path = 'temp.mp4'\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir {video_path}. Prueba con ffmpeg para convertir.\")\n",
        "        return None\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))\n",
        "\n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        r_height = 480\n",
        "\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        roles = analyze_roles(keypoints_list, person_boxes, r_height)\n",
        "        all_pred_roles.append(roles)\n",
        "\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0] if all_pred_roles else [])\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    print(f\"¡Listo! Video guardado en: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Celda 7: Procesar\n",
        "video_path = '/content/hockey_dataset/data/fi1_xvid.avi'  # Prueba con este o cambia a fi10_xvid.avi si no detecta\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo\n",
        "\n",
        "output_path = process_video(video_path, 'output_hockey_improved.mp4', ground_truth_roles)\n",
        "if output_path:\n",
        "    files.download(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QdFzlAe7Qvn6",
        "outputId": "d201e3f1-d8ce-46a3-960e-75780428b555"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 480x640 2 persons, 7.0ms\n",
            "Speed: 1.3ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 7.0ms\n",
            "Speed: 1.9ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 3 persons, 6.6ms\n",
            "Speed: 1.7ms preprocess, 6.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 3 persons, 7.0ms\n",
            "Speed: 1.7ms preprocess, 7.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 7.0ms\n",
            "Speed: 1.7ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 1.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 1.1ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 6.6ms\n",
            "Speed: 1.8ms preprocess, 6.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 1 refrigerator, 6.8ms\n",
            "Speed: 1.8ms preprocess, 6.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 6.6ms\n",
            "Speed: 1.9ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 6.9ms\n",
            "Speed: 1.5ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 6.6ms\n",
            "Speed: 2.0ms preprocess, 6.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 6.9ms\n",
            "Speed: 1.8ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 6.7ms\n",
            "Speed: 2.2ms preprocess, 6.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 8.4ms\n",
            "Speed: 2.0ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 6.9ms\n",
            "Speed: 1.7ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 6.7ms\n",
            "Speed: 1.5ms preprocess, 6.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 1 dog, 6.9ms\n",
            "Speed: 1.6ms preprocess, 6.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 2 cats, 1 tv, 7.0ms\n",
            "Speed: 1.6ms preprocess, 7.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 7.2ms\n",
            "Speed: 1.6ms preprocess, 7.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano en y=0.44, dist_x=2.26, dist_y=28.08) cerca de víctima 1\n",
            "\n",
            "0: 480x640 2 persons, 6.9ms\n",
            "Speed: 1.8ms preprocess, 6.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 6.6ms\n",
            "Speed: 1.9ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 1.7ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 12.4ms\n",
            "Speed: 1.8ms preprocess, 12.4ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 7.9ms\n",
            "Speed: 1.1ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 person, 6.7ms\n",
            "Speed: 1.8ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 1.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 6.5ms\n",
            "Speed: 2.4ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 6.9ms\n",
            "Speed: 1.9ms preprocess, 6.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 3 persons, 6.9ms\n",
            "Speed: 1.9ms preprocess, 6.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 3 persons, 7.3ms\n",
            "Speed: 1.6ms preprocess, 7.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 6.6ms\n",
            "Speed: 1.8ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 2 persons, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "\n",
            "0: 480x640 3 persons, 1 handbag, 7.9ms\n",
            "Speed: 1.8ms preprocess, 7.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó interacción: manos bajas o lejanas.\n",
            "F1-Score para roles: 0.0000\n",
            "¡Listo! Video guardado en: output_hockey_improved.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_890e7067-297e-454b-8953-68a5fa40dd1d\", \"output_hockey_improved.mp4\", 856103)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asume que ya tienes las otras funciones (preprocess_frame, detect_pose, calculate_metrics)\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]  # Izquierda y derecha\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:  # Mano semi-alta\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 100 and dist_y < 100:\n",
        "                                print(f\"Detectado agresor {i} (mano y={hand_y:.2f}, dist_x={dist_x:.2f}, dist_y={dist_y:.2f}, mag={mag_mean:.2f}) cerca de víctima {j}\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 5:  # Movimiento alto: fuerza \"agresor\" si hay proximidad general\n",
        "        if len(person_boxes) >= 2:\n",
        "            dist_between = abs(person_boxes[0][0] - person_boxes[1][0])\n",
        "            if dist_between < 150:  # Personas cercanas con movimiento\n",
        "                print(f\"Movimiento alto ({mag_mean:.2f}) y proximidad: asignando agresor 0, víctima 1\")\n",
        "                return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No se detectó: manos bajas/lejanas, mag={mag_mean:.2f}\")\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]\n",
        "\n",
        "def process_video(video_path, output_path='output_hockey_persistent.mp4', ground_truth_roles=None, persistence_frames=5):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error al abrir {video_path}\")\n",
        "        return None\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))\n",
        "\n",
        "    frame_count = 0\n",
        "    all_pred_roles = []\n",
        "    prev_gray = None\n",
        "    last_roles = None\n",
        "    persistence_counter = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        r_height = 480\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Optical Flow\n",
        "        mag_mean = 0\n",
        "        if prev_gray is not None:\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "            mag_mean = np.mean(mag)\n",
        "\n",
        "        prev_gray = gray\n",
        "\n",
        "        # Detectar personas\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Poses\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        # Roles\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, r_height, mag_mean)\n",
        "\n",
        "        # Persistencia\n",
        "        if any(r != 'desconocido' for _, r in current_roles):\n",
        "            last_roles = current_roles\n",
        "            persistence_counter = persistence_frames\n",
        "        elif persistence_counter > 0:\n",
        "            current_roles = last_roles\n",
        "            persistence_counter -= 1\n",
        "            print(f\"Persistiendo roles por {persistence_counter} frames más\")\n",
        "        else:\n",
        "            last_roles = None\n",
        "\n",
        "        all_pred_roles.append(current_roles)\n",
        "\n",
        "        # Dibujar\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, all_pred_roles[0] if all_pred_roles else [])\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    print(f\"¡Listo! Video guardado en: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Uso: Cambia video si quieres\n",
        "video_path = '/content/hockey_dataset/data/fi1_xvid.avi'\n",
        "output_path = process_video(video_path, 'output_hockey_persistent.mp4', ground_truth_roles)\n",
        "if output_path:\n",
        "    files.download(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pZNVv3i9RRxt",
        "outputId": "0be01481-1253-41ec-d54f-89d80c678d72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 480x640 2 persons, 6.5ms\n",
            "Speed: 2.1ms preprocess, 6.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=0.00\n",
            "\n",
            "0: 480x640 2 persons, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.76\n",
            "\n",
            "0: 480x640 2 persons, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.91\n",
            "\n",
            "0: 480x640 3 persons, 6.9ms\n",
            "Speed: 2.4ms preprocess, 6.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "\n",
            "0: 480x640 3 persons, 7.0ms\n",
            "Speed: 2.1ms preprocess, 7.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.84\n",
            "\n",
            "0: 480x640 2 persons, 7.0ms\n",
            "Speed: 2.3ms preprocess, 7.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.23\n",
            "\n",
            "0: 480x640 3 persons, 9.4ms\n",
            "Speed: 1.9ms preprocess, 9.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.15\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.62\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.0ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.06\n",
            "\n",
            "0: 480x640 1 person, 1 refrigerator, 10.9ms\n",
            "Speed: 1.8ms preprocess, 10.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.50\n",
            "\n",
            "0: 480x640 1 person, 11.9ms\n",
            "Speed: 2.1ms preprocess, 11.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.35\n",
            "\n",
            "0: 480x640 2 persons, 13.0ms\n",
            "Speed: 1.8ms preprocess, 13.0ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.65\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.13\n",
            "\n",
            "0: 480x640 2 persons, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.99\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.23\n",
            "\n",
            "0: 480x640 2 persons, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.95\n",
            "\n",
            "0: 480x640 2 persons, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.52\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.76\n",
            "\n",
            "0: 480x640 1 person, 11.4ms\n",
            "Speed: 1.8ms preprocess, 11.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.94\n",
            "\n",
            "0: 480x640 1 person, 1 dog, 12.8ms\n",
            "Speed: 2.2ms preprocess, 12.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.50\n",
            "\n",
            "0: 480x640 1 person, 2 cats, 1 tv, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.15\n",
            "\n",
            "0: 480x640 2 persons, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano y=0.44, dist_x=2.26, dist_y=28.08, mag=4.93) cerca de víctima 1\n",
            "\n",
            "0: 480x640 2 persons, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.59\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 2 persons, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.46\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 1 person, 17.1ms\n",
            "Speed: 2.4ms preprocess, 17.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=6.95\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 1 person, 28.4ms\n",
            "Speed: 5.9ms preprocess, 28.4ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.22\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 2 persons, 15.8ms\n",
            "Speed: 1.8ms preprocess, 15.8ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.19\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 2 persons, 12.7ms\n",
            "Speed: 4.4ms preprocess, 12.7ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.92\n",
            "\n",
            "0: 480x640 1 person, 16.7ms\n",
            "Speed: 3.3ms preprocess, 16.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.39\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 person, 33.9ms\n",
            "Speed: 4.2ms preprocess, 33.9ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.46\n",
            "\n",
            "0: 480x640 1 person, 46.6ms\n",
            "Speed: 3.7ms preprocess, 46.6ms inference, 10.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.16\n",
            "\n",
            "0: 480x640 2 persons, 54.2ms\n",
            "Speed: 8.2ms preprocess, 54.2ms inference, 17.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.08\n",
            "\n",
            "0: 480x640 1 person, 29.4ms\n",
            "Speed: 3.9ms preprocess, 29.4ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.26\n",
            "\n",
            "0: 480x640 2 persons, 28.6ms\n",
            "Speed: 7.0ms preprocess, 28.6ms inference, 12.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.30\n",
            "\n",
            "0: 480x640 2 persons, 18.3ms\n",
            "Speed: 5.7ms preprocess, 18.3ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.30\n",
            "\n",
            "0: 480x640 3 persons, 12.5ms\n",
            "Speed: 3.5ms preprocess, 12.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.68\n",
            "\n",
            "0: 480x640 3 persons, 12.2ms\n",
            "Speed: 2.8ms preprocess, 12.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.11\n",
            "\n",
            "0: 480x640 2 persons, 24.2ms\n",
            "Speed: 2.9ms preprocess, 24.2ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.39\n",
            "\n",
            "0: 480x640 2 persons, 14.4ms\n",
            "Speed: 1.8ms preprocess, 14.4ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.23\n",
            "\n",
            "0: 480x640 3 persons, 1 handbag, 14.0ms\n",
            "Speed: 5.9ms preprocess, 14.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.36\n",
            "F1-Score para roles: 0.0000\n",
            "¡Listo! Video guardado en: output_hockey_persistent.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85de6f04-d9b4-4b25-8be2-b0eb1ff1dece\", \"output_hockey_persistent.mp4\", 850677)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]  # Izquierda, derecha\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} (mano y={hand_y:.2f}, dist_x={dist_x:.2f}, dist_y={dist_y:.2f}, mag={mag_mean:.2f}) cerca de víctima {j}\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 4 and len(person_boxes) >= 2:\n",
        "        dist_between = abs(person_boxes[0][0] - person_boxes[1][0])\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto (mag={mag_mean:.2f}) y proximidad: asignando agresor 0, víctima 1\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No se detectó: manos bajas/lejanas, mag={mag_mean:.2f}\")\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]\n",
        "\n",
        "def process_video(video_path, output_path, ground_truth_roles=None, persistence_frames=7):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error al abrir {video_path}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))\n",
        "\n",
        "    frame_count = 0\n",
        "    agresor_count = 0\n",
        "    victima_count = 0\n",
        "    prev_gray = None\n",
        "    last_roles = None\n",
        "    persistence_counter = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        r_height = 480\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Optical Flow\n",
        "        mag_mean = 0\n",
        "        if prev_gray is not None:\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "            mag_mean = np.mean(mag)\n",
        "\n",
        "        prev_gray = gray\n",
        "\n",
        "        # Detectar personas\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Solo personas\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Poses\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        # Roles\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, r_height, mag_mean)\n",
        "\n",
        "        # Persistencia\n",
        "        if any(r != 'desconocido' for _, r in current_roles):\n",
        "            last_roles = current_roles\n",
        "            persistence_counter = persistence_frames\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "        elif persistence_counter > 0:\n",
        "            current_roles = last_roles\n",
        "            persistence_counter -= 1\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "            print(f\"Persistiendo roles por {persistence_counter} frames más\")\n",
        "        else:\n",
        "            last_roles = None\n",
        "\n",
        "        # Dibujar\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)\n",
        "        print(f\"F1-Score para roles: {f1:.4f}\")\n",
        "\n",
        "    print(f\"Video: {video_path}\")\n",
        "    print(f\"Frames con agresor: {agresor_count}\")\n",
        "    print(f\"Frames con víctima: {victima_count}\")\n",
        "    print(f\"¡Listo! Video guardado en: {output_path}\")\n",
        "    return output_path, agresor_count, victima_count\n",
        "\n",
        "# Procesar múltiples videos\n",
        "dataset_dir = '/content/hockey_dataset/data'\n",
        "videos_to_test = [\n",
        "    'fi1_xvid.avi',\n",
        "    'fi2_xvid.avi',\n",
        "    'fi3_xvid.avi',\n",
        "    'fi4_xvid.avi',\n",
        "    'fi5_xvid.avi',\n",
        "    'no1_xvid.avi'  # Control (sin pelea)\n",
        "]\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo, ajusta si tienes anotaciones reales\n",
        "\n",
        "report = []\n",
        "for video in videos_to_test:\n",
        "    video_path = os.path.join(dataset_dir, video)\n",
        "    output_path = f'output_{video.replace(\".avi\", \".mp4\")}'\n",
        "    if os.path.exists(video_path):\n",
        "        print(f\"\\nProcesando {video}...\")\n",
        "        result, agresor_count, victima_count = process_video(video_path, output_path, ground_truth_roles)\n",
        "        if result:\n",
        "            report.append((video, agresor_count, victima_count, result))\n",
        "            files.download(result)\n",
        "    else:\n",
        "        print(f\"Video no encontrado: {video_path}\")\n",
        "\n",
        "# Resumen\n",
        "print(\"\\nReporte Final:\")\n",
        "for video, agresor_count, victima_count, output in report:\n",
        "    print(f\"{video}: Agresor en {agresor_count} frames, Víctima en {victima_count} frames, Output: {output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NbawbQLiRu6_",
        "outputId": "01571fcb-4207-4d04-fbba-ed4bb3ccb4bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando fi1_xvid.avi...\n",
            "\n",
            "0: 480x640 2 persons, 14.4ms\n",
            "Speed: 1.9ms preprocess, 14.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=0.00\n",
            "\n",
            "0: 480x640 2 persons, 9.4ms\n",
            "Speed: 3.0ms preprocess, 9.4ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.76\n",
            "\n",
            "0: 480x640 2 persons, 11.9ms\n",
            "Speed: 3.8ms preprocess, 11.9ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.91\n",
            "\n",
            "0: 480x640 3 persons, 13.9ms\n",
            "Speed: 1.9ms preprocess, 13.9ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "\n",
            "0: 480x640 3 persons, 14.7ms\n",
            "Speed: 1.8ms preprocess, 14.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.84\n",
            "\n",
            "0: 480x640 2 persons, 30.8ms\n",
            "Speed: 3.8ms preprocess, 30.8ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.23\n",
            "\n",
            "0: 480x640 3 persons, 18.6ms\n",
            "Speed: 3.0ms preprocess, 18.6ms inference, 7.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.15\n",
            "\n",
            "0: 480x640 1 person, 23.4ms\n",
            "Speed: 3.7ms preprocess, 23.4ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.62\n",
            "\n",
            "0: 480x640 1 person, 14.6ms\n",
            "Speed: 1.8ms preprocess, 14.6ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "\n",
            "0: 480x640 1 person, 12.7ms\n",
            "Speed: 1.9ms preprocess, 12.7ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.06\n",
            "\n",
            "0: 480x640 1 person, 1 refrigerator, 14.6ms\n",
            "Speed: 1.7ms preprocess, 14.6ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.50\n",
            "\n",
            "0: 480x640 1 person, 13.6ms\n",
            "Speed: 1.8ms preprocess, 13.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.35\n",
            "\n",
            "0: 480x640 2 persons, 12.7ms\n",
            "Speed: 1.9ms preprocess, 12.7ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=4.65) y proximidad: asignando agresor 0, víctima 1\n",
            "\n",
            "0: 480x640 1 person, 13.1ms\n",
            "Speed: 2.1ms preprocess, 13.1ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.13\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 2 persons, 12.4ms\n",
            "Speed: 2.1ms preprocess, 12.4ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.99\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 1 person, 11.6ms\n",
            "Speed: 2.0ms preprocess, 11.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.23\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 2 persons, 12.2ms\n",
            "Speed: 1.8ms preprocess, 12.2ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.95\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 2 persons, 15.0ms\n",
            "Speed: 2.6ms preprocess, 15.0ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.52\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 1 person, 11.2ms\n",
            "Speed: 2.4ms preprocess, 11.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.76\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 1 person, 19.4ms\n",
            "Speed: 4.0ms preprocess, 19.4ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.94\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 1 person, 1 dog, 12.8ms\n",
            "Speed: 1.8ms preprocess, 12.8ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.50\n",
            "\n",
            "0: 480x640 1 person, 2 cats, 1 tv, 17.1ms\n",
            "Speed: 2.1ms preprocess, 17.1ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.15\n",
            "\n",
            "0: 480x640 2 persons, 12.9ms\n",
            "Speed: 2.7ms preprocess, 12.9ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano y=0.44, dist_x=2.26, dist_y=28.08, mag=4.93) cerca de víctima 1\n",
            "\n",
            "0: 480x640 2 persons, 12.3ms\n",
            "Speed: 3.2ms preprocess, 12.3ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.59\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 2 persons, 17.3ms\n",
            "Speed: 2.8ms preprocess, 17.3ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.46\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 1 person, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=6.95\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 1 person, 13.0ms\n",
            "Speed: 1.7ms preprocess, 13.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.22\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 2 persons, 24.7ms\n",
            "Speed: 1.7ms preprocess, 24.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.19\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 2 persons, 13.3ms\n",
            "Speed: 4.1ms preprocess, 13.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.92\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 1 person, 12.4ms\n",
            "Speed: 3.5ms preprocess, 12.4ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.39\n",
            "Persistiendo roles por 0 frames más\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 person, 17.7ms\n",
            "Speed: 2.8ms preprocess, 17.7ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.46\n",
            "\n",
            "0: 480x640 1 person, 29.3ms\n",
            "Speed: 3.9ms preprocess, 29.3ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.16\n",
            "\n",
            "0: 480x640 2 persons, 20.0ms\n",
            "Speed: 3.3ms preprocess, 20.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.08\n",
            "\n",
            "0: 480x640 1 person, 24.6ms\n",
            "Speed: 2.1ms preprocess, 24.6ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.26\n",
            "\n",
            "0: 480x640 2 persons, 40.3ms\n",
            "Speed: 7.1ms preprocess, 40.3ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.30\n",
            "\n",
            "0: 480x640 2 persons, 50.1ms\n",
            "Speed: 3.3ms preprocess, 50.1ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.30\n",
            "\n",
            "0: 480x640 3 persons, 39.2ms\n",
            "Speed: 2.0ms preprocess, 39.2ms inference, 8.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=4.68) y proximidad: asignando agresor 0, víctima 1\n",
            "\n",
            "0: 480x640 3 persons, 50.7ms\n",
            "Speed: 6.1ms preprocess, 50.7ms inference, 12.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.11\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 2 persons, 15.5ms\n",
            "Speed: 2.1ms preprocess, 15.5ms inference, 6.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.39\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 2 persons, 39.7ms\n",
            "Speed: 3.9ms preprocess, 39.7ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.23\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 3 persons, 1 handbag, 16.5ms\n",
            "Speed: 3.0ms preprocess, 16.5ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.36\n",
            "Persistiendo roles por 3 frames más\n",
            "F1-Score para roles: 1.0000\n",
            "Video: /content/hockey_dataset/data/fi1_xvid.avi\n",
            "Frames con agresor: 21\n",
            "Frames con víctima: 21\n",
            "¡Listo! Video guardado en: output_fi1_xvid.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f282b05-0239-410e-96c8-81d88dd852b8\", \"output_fi1_xvid.mp4\", 851761)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando fi2_xvid.avi...\n",
            "\n",
            "0: 480x640 2 persons, 14.0ms\n",
            "Speed: 2.2ms preprocess, 14.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=0.00\n",
            "\n",
            "0: 480x640 3 persons, 35.4ms\n",
            "Speed: 6.8ms preprocess, 35.4ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=5.48) y proximidad: asignando agresor 0, víctima 1\n",
            "\n",
            "0: 480x640 3 persons, 22.2ms\n",
            "Speed: 4.2ms preprocess, 22.2ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=5.98) y proximidad: asignando agresor 0, víctima 1\n",
            "\n",
            "0: 480x640 2 persons, 23.1ms\n",
            "Speed: 3.4ms preprocess, 23.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=5.22\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 2 persons, 17.2ms\n",
            "Speed: 1.8ms preprocess, 17.2ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.50\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 3 persons, 15.7ms\n",
            "Speed: 2.8ms preprocess, 15.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.08\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 2 persons, 1 dog, 33.9ms\n",
            "Speed: 4.0ms preprocess, 33.9ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.13\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 3 persons, 20.4ms\n",
            "Speed: 1.8ms preprocess, 20.4ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.26\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 2 persons, 13.5ms\n",
            "Speed: 2.4ms preprocess, 13.5ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.87\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 2 persons, 12.3ms\n",
            "Speed: 3.9ms preprocess, 12.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.60\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 3 persons, 12.3ms\n",
            "Speed: 2.4ms preprocess, 12.3ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.31\n",
            "\n",
            "0: 480x640 3 persons, 8.9ms\n",
            "Speed: 2.2ms preprocess, 8.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.99\n",
            "\n",
            "0: 480x640 3 persons, 7.7ms\n",
            "Speed: 1.7ms preprocess, 7.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.93\n",
            "\n",
            "0: 480x640 2 persons, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.93\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.22\n",
            "\n",
            "0: 480x640 3 persons, 7.0ms\n",
            "Speed: 2.2ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.01\n",
            "\n",
            "0: 480x640 2 persons, 1 dog, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.06\n",
            "\n",
            "0: 480x640 3 persons, 7.1ms\n",
            "Speed: 2.4ms preprocess, 7.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.30, dist_x=87.36, dist_y=95.32, mag=2.44) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 7.3ms\n",
            "Speed: 1.7ms preprocess, 7.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.72\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.7ms\n",
            "Speed: 1.7ms preprocess, 7.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.80\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.71\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.3ms\n",
            "Speed: 2.0ms preprocess, 7.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.49\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.51\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.12\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 3 persons, 8.7ms\n",
            "Speed: 1.9ms preprocess, 8.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.23\n",
            "\n",
            "0: 480x640 3 persons, 7.2ms\n",
            "Speed: 1.8ms preprocess, 7.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.04\n",
            "\n",
            "0: 480x640 3 persons, 7.9ms\n",
            "Speed: 1.8ms preprocess, 7.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.69\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=4.32) y proximidad: asignando agresor 0, víctima 1\n",
            "\n",
            "0: 480x640 4 persons, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=4.19) y proximidad: asignando agresor 0, víctima 1\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=4.19) y proximidad: asignando agresor 0, víctima 1\n",
            "\n",
            "0: 480x640 3 persons, 7.5ms\n",
            "Speed: 1.8ms preprocess, 7.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto (mag=4.46) y proximidad: asignando agresor 0, víctima 1\n",
            "\n",
            "0: 480x640 3 persons, 7.7ms\n",
            "Speed: 1.7ms preprocess, 7.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 2 (mano y=0.28, dist_x=61.64, dist_y=101.01, mag=3.34) cerca de víctima 0\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 1.8ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.92\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.24\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.0ms\n",
            "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.47\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 5 persons, 10.2ms\n",
            "Speed: 1.8ms preprocess, 10.2ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.92\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.64\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 3 persons, 1 backpack, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=4.23\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.50\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.75\n",
            "F1-Score para roles: 0.0000\n",
            "Video: /content/hockey_dataset/data/fi2_xvid.avi\n",
            "Frames con agresor: 29\n",
            "Frames con víctima: 29\n",
            "¡Listo! Video guardado en: output_fi2_xvid.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_97ba4383-cf48-4893-99e7-4f6aa229260f\", \"output_fi2_xvid.mp4\", 1160550)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando fi3_xvid.avi...\n",
            "\n",
            "0: 480x640 3 persons, 7.1ms\n",
            "Speed: 1.7ms preprocess, 7.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=0.00\n",
            "\n",
            "0: 480x640 3 persons, 9.5ms\n",
            "Speed: 1.8ms preprocess, 9.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.82\n",
            "\n",
            "0: 480x640 3 persons, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.35, dist_x=84.01, dist_y=65.48, mag=1.90) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.35, dist_x=83.52, dist_y=63.96, mag=2.00) cerca de víctima 0\n",
            "\n",
            "0: 480x640 3 persons, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.26\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 3 persons, 9.4ms\n",
            "Speed: 1.7ms preprocess, 9.4ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.55, dist_x=72.22, dist_y=29.13, mag=2.68) cerca de víctima 0\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.30\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 3 persons, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.55\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.40\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.00\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.72\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 3 persons, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.54\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.8ms\n",
            "Speed: 1.8ms preprocess, 7.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.63\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.55\n",
            "\n",
            "0: 480x640 4 persons, 8.3ms\n",
            "Speed: 2.0ms preprocess, 8.3ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.16\n",
            "\n",
            "0: 480x640 3 persons, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.07\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 1.7ms preprocess, 7.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.89\n",
            "\n",
            "0: 480x640 3 persons, 1 backpack, 8.9ms\n",
            "Speed: 1.9ms preprocess, 8.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 2 (mano y=0.45, dist_x=65.76, dist_y=79.61, mag=1.86) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 1 backpack, 7.4ms\n",
            "Speed: 1.7ms preprocess, 7.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.69\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.9ms\n",
            "Speed: 1.6ms preprocess, 8.9ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 2 (mano y=0.34, dist_x=18.31, dist_y=72.83, mag=1.79) cerca de víctima 1\n",
            "\n",
            "0: 480x640 4 persons, 7.4ms\n",
            "Speed: 1.8ms preprocess, 7.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 2 (mano y=0.29, dist_x=3.41, dist_y=92.04, mag=2.20) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 15.3ms\n",
            "Speed: 2.4ms preprocess, 15.3ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.20\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 10.5ms\n",
            "Speed: 1.7ms preprocess, 10.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.34, dist_x=48.72, dist_y=61.98, mag=2.24) cerca de víctima 2\n",
            "\n",
            "0: 480x640 4 persons, 9.8ms\n",
            "Speed: 1.9ms preprocess, 9.8ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.51\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 11.2ms\n",
            "Speed: 1.8ms preprocess, 11.2ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.24\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 5 persons, 9.6ms\n",
            "Speed: 1.9ms preprocess, 9.6ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.91\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 5 persons, 8.6ms\n",
            "Speed: 2.0ms preprocess, 8.6ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.64\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 3 persons, 9.7ms\n",
            "Speed: 1.9ms preprocess, 9.7ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.47\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 3 persons, 15.2ms\n",
            "Speed: 3.7ms preprocess, 15.2ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 2 (mano y=0.53, dist_x=18.37, dist_y=37.90, mag=1.40) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 10.7ms\n",
            "Speed: 1.8ms preprocess, 10.7ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.25\n",
            "Persistiendo roles por 6 frames más\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 4 persons, 12.1ms\n",
            "Speed: 2.4ms preprocess, 12.1ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.92\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 5 persons, 1 backpack, 14.3ms\n",
            "Speed: 2.7ms preprocess, 14.3ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.87\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.26\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 4 persons, 1 backpack, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.34\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.8ms\n",
            "Speed: 2.0ms preprocess, 8.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano y=0.26, dist_x=15.80, dist_y=31.02, mag=1.51) cerca de víctima 2\n",
            "\n",
            "0: 480x640 3 persons, 10.1ms\n",
            "Speed: 2.9ms preprocess, 10.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano y=0.34, dist_x=118.06, dist_y=101.67, mag=1.47) cerca de víctima 2\n",
            "\n",
            "0: 480x640 3 persons, 1 backpack, 10.6ms\n",
            "Speed: 2.2ms preprocess, 10.6ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.48\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.34\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 3 persons, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.31\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 3 persons, 1 backpack, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.27\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 3 persons, 1 backpack, 10.1ms\n",
            "Speed: 1.8ms preprocess, 10.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.13\n",
            "Persistiendo roles por 2 frames más\n",
            "F1-Score para roles: 1.0000\n",
            "Video: /content/hockey_dataset/data/fi3_xvid.avi\n",
            "Frames con agresor: 35\n",
            "Frames con víctima: 35\n",
            "¡Listo! Video guardado en: output_fi3_xvid.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_28c6bb1c-e6c7-4f46-8c67-f0a12d6da991\", \"output_fi3_xvid.mp4\", 1355830)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando fi4_xvid.avi...\n",
            "\n",
            "0: 480x640 5 persons, 7.5ms\n",
            "Speed: 1.7ms preprocess, 7.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 3 (mano y=0.48, dist_x=37.12, dist_y=57.52, mag=0.00) cerca de víctima 1\n",
            "\n",
            "0: 480x640 4 persons, 7.2ms\n",
            "Speed: 1.8ms preprocess, 7.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.90\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.1ms\n",
            "Speed: 2.4ms preprocess, 7.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.97\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 5 persons, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.05\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.3ms\n",
            "Speed: 2.3ms preprocess, 8.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano y=0.31, dist_x=83.03, dist_y=104.73, mag=1.96) cerca de víctima 1\n",
            "\n",
            "0: 480x640 4 persons, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.17\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 3 (mano y=0.46, dist_x=22.33, dist_y=65.21, mag=3.12) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.42\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 13.1ms\n",
            "Speed: 1.7ms preprocess, 13.1ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.38\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.41\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.08\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.9ms\n",
            "Speed: 1.8ms preprocess, 7.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.51\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.1ms\n",
            "Speed: 2.0ms preprocess, 9.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.82\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.57\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.5ms\n",
            "Speed: 1.9ms preprocess, 9.5ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.21\n",
            "\n",
            "0: 480x640 4 persons, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 3 (mano y=0.59, dist_x=75.55, dist_y=80.39, mag=1.18) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.26\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 5 persons, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.40\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.6ms\n",
            "Speed: 1.8ms preprocess, 8.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.20, dist_x=31.55, dist_y=107.40, mag=1.64) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 7.7ms\n",
            "Speed: 1.8ms preprocess, 7.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.62\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.2ms\n",
            "Speed: 1.7ms preprocess, 9.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.45\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.29\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 10.5ms\n",
            "Speed: 2.2ms preprocess, 10.5ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.31\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.23\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 4 persons, 11.9ms\n",
            "Speed: 2.1ms preprocess, 11.9ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.33, dist_x=51.87, dist_y=100.24, mag=1.31) cerca de víctima 2\n",
            "\n",
            "0: 480x640 4 persons, 1 fire hydrant, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.25\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.06\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 5 persons, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.47\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.60\n",
            "Persistiendo roles por 3 frames más\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 4 persons, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.42\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.2ms\n",
            "Speed: 1.8ms preprocess, 9.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.28\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.2ms\n",
            "Speed: 1.9ms preprocess, 8.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.15\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.8ms\n",
            "Speed: 1.6ms preprocess, 8.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.27\n",
            "\n",
            "0: 480x640 4 persons, 1 backpack, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.39, dist_x=52.20, dist_y=102.86, mag=1.29) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 9.1ms\n",
            "Speed: 1.8ms preprocess, 9.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 3 (mano y=0.40, dist_x=49.13, dist_y=97.96, mag=1.27) cerca de víctima 0\n",
            "\n",
            "0: 480x640 4 persons, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.13\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.14\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.0ms\n",
            "Speed: 2.2ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.32\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.40\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.68\n",
            "Persistiendo roles por 2 frames más\n",
            "F1-Score para roles: 1.0000\n",
            "Video: /content/hockey_dataset/data/fi4_xvid.avi\n",
            "Frames con agresor: 39\n",
            "Frames con víctima: 39\n",
            "¡Listo! Video guardado en: output_fi4_xvid.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc307cbd-c5c8-4979-b019-b1a303f023fd\", \"output_fi4_xvid.mp4\", 1339895)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando fi5_xvid.avi...\n",
            "\n",
            "0: 480x640 2 persons, 8.4ms\n",
            "Speed: 1.3ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=0.00\n",
            "\n",
            "0: 480x640 2 persons, 8.4ms\n",
            "Speed: 1.7ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=0.98\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.09\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.12\n",
            "\n",
            "0: 480x640 2 persons, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.51\n",
            "\n",
            "0: 480x640 3 persons, 1 handbag, 7.6ms\n",
            "Speed: 1.7ms preprocess, 7.6ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.80\n",
            "\n",
            "0: 480x640 3 persons, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.95\n",
            "\n",
            "0: 480x640 3 persons, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.08\n",
            "\n",
            "0: 480x640 3 persons, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.94\n",
            "\n",
            "0: 480x640 3 persons, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.97\n",
            "\n",
            "0: 480x640 3 persons, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.00\n",
            "\n",
            "0: 480x640 3 persons, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.97\n",
            "\n",
            "0: 480x640 3 persons, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.98\n",
            "\n",
            "0: 480x640 5 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.12\n",
            "\n",
            "0: 480x640 4 persons, 8.1ms\n",
            "Speed: 1.9ms preprocess, 8.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.25\n",
            "\n",
            "0: 480x640 4 persons, 7.6ms\n",
            "Speed: 1.7ms preprocess, 7.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.47\n",
            "\n",
            "0: 480x640 4 persons, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.53\n",
            "\n",
            "0: 480x640 5 persons, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.53\n",
            "\n",
            "0: 480x640 4 persons, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.41\n",
            "\n",
            "0: 480x640 3 persons, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.47\n",
            "\n",
            "0: 480x640 4 persons, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.34, dist_x=80.16, dist_y=108.01, mag=2.69) cerca de víctima 3\n",
            "\n",
            "0: 480x640 4 persons, 12.3ms\n",
            "Speed: 3.0ms preprocess, 12.3ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano y=0.32, dist_x=76.26, dist_y=114.30, mag=2.95) cerca de víctima 3\n",
            "\n",
            "0: 480x640 4 persons, 10.0ms\n",
            "Speed: 1.8ms preprocess, 10.0ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.14\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 10.9ms\n",
            "Speed: 2.8ms preprocess, 10.9ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.38\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 12.0ms\n",
            "Speed: 2.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.71\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 13.3ms\n",
            "Speed: 2.1ms preprocess, 13.3ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.34, dist_x=50.55, dist_y=102.86, mag=3.25) cerca de víctima 3\n",
            "\n",
            "0: 480x640 4 persons, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.38\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 11.6ms\n",
            "Speed: 2.4ms preprocess, 11.6ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.71\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 14.1ms\n",
            "Speed: 1.8ms preprocess, 14.1ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.58\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 4 persons, 14.5ms\n",
            "Speed: 2.8ms preprocess, 14.5ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.78\n",
            "Persistiendo roles por 3 frames más\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 4 persons, 14.0ms\n",
            "Speed: 2.5ms preprocess, 14.0ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.66\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.0ms\n",
            "Speed: 3.8ms preprocess, 9.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.83\n",
            "Persistiendo roles por 1 frames más\n",
            "\n",
            "0: 480x640 4 persons, 9.7ms\n",
            "Speed: 3.3ms preprocess, 9.7ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "Persistiendo roles por 0 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.53\n",
            "\n",
            "0: 480x640 4 persons, 9.2ms\n",
            "Speed: 1.8ms preprocess, 9.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.42\n",
            "\n",
            "0: 480x640 4 persons, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.53\n",
            "\n",
            "0: 480x640 4 persons, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.54\n",
            "\n",
            "0: 480x640 5 persons, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.18\n",
            "\n",
            "0: 480x640 4 persons, 8.9ms\n",
            "Speed: 2.9ms preprocess, 8.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.00\n",
            "\n",
            "0: 480x640 3 persons, 7.7ms\n",
            "Speed: 1.8ms preprocess, 7.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.85\n",
            "\n",
            "0: 480x640 3 persons, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.75\n",
            "F1-Score para roles: 0.0000\n",
            "Video: /content/hockey_dataset/data/fi5_xvid.avi\n",
            "Frames con agresor: 13\n",
            "Frames con víctima: 13\n",
            "¡Listo! Video guardado en: output_fi5_xvid.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9c4991fb-21ca-4d4c-b4df-cf8b86778208\", \"output_fi5_xvid.mp4\", 1199751)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando no1_xvid.avi...\n",
            "\n",
            "0: 480x640 2 persons, 1 skateboard, 6.9ms\n",
            "Speed: 2.0ms preprocess, 6.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=0.00\n",
            "\n",
            "0: 480x640 2 persons, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.22\n",
            "\n",
            "0: 480x640 2 persons, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.32\n",
            "\n",
            "0: 480x640 2 persons, 13.4ms\n",
            "Speed: 1.8ms preprocess, 13.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.34\n",
            "\n",
            "0: 480x640 3 persons, 9.2ms\n",
            "Speed: 1.8ms preprocess, 9.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.48\n",
            "\n",
            "0: 480x640 3 persons, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.59\n",
            "\n",
            "0: 480x640 3 persons, 9.0ms\n",
            "Speed: 2.1ms preprocess, 9.0ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.60\n",
            "\n",
            "0: 480x640 2 persons, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.72\n",
            "\n",
            "0: 480x640 2 persons, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.85\n",
            "\n",
            "0: 480x640 2 persons, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.93\n",
            "\n",
            "0: 480x640 2 persons, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.83\n",
            "\n",
            "0: 480x640 2 persons, 13.3ms\n",
            "Speed: 1.8ms preprocess, 13.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.91\n",
            "\n",
            "0: 480x640 2 persons, 8.9ms\n",
            "Speed: 1.9ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.70\n",
            "\n",
            "0: 480x640 2 persons, 7.9ms\n",
            "Speed: 1.8ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.76\n",
            "\n",
            "0: 480x640 2 persons, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.90\n",
            "\n",
            "0: 480x640 1 person, 1 baseball bat, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.05\n",
            "\n",
            "0: 480x640 1 person, 1 baseball bat, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.12\n",
            "\n",
            "0: 480x640 1 person, 1 baseball bat, 1 baseball glove, 7.6ms\n",
            "Speed: 1.7ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.12\n",
            "\n",
            "0: 480x640 1 person, 1 baseball bat, 1 baseball glove, 7.7ms\n",
            "Speed: 1.7ms preprocess, 7.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.13\n",
            "\n",
            "0: 480x640 1 person, 1 baseball bat, 9.0ms\n",
            "Speed: 2.0ms preprocess, 9.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.28\n",
            "\n",
            "0: 480x640 1 person, 1 train, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.44\n",
            "\n",
            "0: 480x640 1 person, 1 baseball glove, 9.1ms\n",
            "Speed: 2.0ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.63\n",
            "\n",
            "0: 480x640 2 persons, 1 baseball glove, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.60\n",
            "\n",
            "0: 480x640 3 persons, 1 baseball glove, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.60\n",
            "\n",
            "0: 480x640 1 person, 2 baseball gloves, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.52\n",
            "\n",
            "0: 480x640 2 persons, 1 train, 1 baseball glove, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 (mano y=0.47, dist_x=70.27, dist_y=28.05, mag=3.50) cerca de víctima 1\n",
            "\n",
            "0: 480x640 3 persons, 1 baseball glove, 1 tennis racket, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.49, dist_x=68.18, dist_y=12.67, mag=3.52) cerca de víctima 0\n",
            "\n",
            "0: 480x640 2 persons, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.33\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 2 persons, 9.4ms\n",
            "Speed: 1.8ms preprocess, 9.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.02\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 8.5ms\n",
            "Speed: 1.8ms preprocess, 8.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.25\n",
            "Persistiendo roles por 4 frames más\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 3 persons, 1 snowboard, 9.1ms\n",
            "Speed: 1.7ms preprocess, 9.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 1 (mano y=0.59, dist_x=7.92, dist_y=50.03, mag=3.14) cerca de víctima 2\n",
            "\n",
            "0: 480x640 4 persons, 1 baseball glove, 8.2ms\n",
            "Speed: 1.9ms preprocess, 8.2ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.19\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 4 persons, 10.0ms\n",
            "Speed: 1.9ms preprocess, 10.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.10\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 4 persons, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=3.03\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 3 persons, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 2 (mano y=0.56, dist_x=20.30, dist_y=61.86, mag=2.89) cerca de víctima 1\n",
            "\n",
            "0: 480x640 3 persons, 7.7ms\n",
            "Speed: 1.7ms preprocess, 7.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.77\n",
            "Persistiendo roles por 6 frames más\n",
            "\n",
            "0: 480x640 5 persons, 9.2ms\n",
            "Speed: 1.8ms preprocess, 9.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=2.37\n",
            "Persistiendo roles por 5 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.84\n",
            "Persistiendo roles por 4 frames más\n",
            "\n",
            "0: 480x640 3 persons, 9.4ms\n",
            "Speed: 1.9ms preprocess, 9.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.58\n",
            "Persistiendo roles por 3 frames más\n",
            "\n",
            "0: 480x640 3 persons, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.26\n",
            "Persistiendo roles por 2 frames más\n",
            "\n",
            "0: 480x640 3 persons, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No se detectó: manos bajas/lejanas, mag=1.18\n",
            "Persistiendo roles por 1 frames más\n",
            "F1-Score para roles: 1.0000\n",
            "Video: /content/hockey_dataset/data/no1_xvid.avi\n",
            "Frames con agresor: 16\n",
            "Frames con víctima: 16\n",
            "¡Listo! Video guardado en: output_no1_xvid.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7367eeeb-a188-4b61-a79f-ae33e7a5fb9b\", \"output_no1_xvid.mp4\", 843694)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reporte Final:\n",
            "fi1_xvid.avi: Agresor en 21 frames, Víctima en 21 frames, Output: output_fi1_xvid.mp4\n",
            "fi2_xvid.avi: Agresor en 29 frames, Víctima en 29 frames, Output: output_fi2_xvid.mp4\n",
            "fi3_xvid.avi: Agresor en 35 frames, Víctima en 35 frames, Output: output_fi3_xvid.mp4\n",
            "fi4_xvid.avi: Agresor en 39 frames, Víctima en 39 frames, Output: output_fi4_xvid.mp4\n",
            "fi5_xvid.avi: Agresor en 13 frames, Víctima en 13 frames, Output: output_fi5_xvid.mp4\n",
            "no1_xvid.avi: Agresor en 16 frames, Víctima en 16 frames, Output: output_no1_xvid.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias (si no lo hiciste antes)\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics scikit-learn kaggle\n",
        "\n",
        "import cv2  # Para procesamiento de video\n",
        "import numpy as np  # Para arrays numéricos\n",
        "import tensorflow as tf  # Para MoveNet\n",
        "import tensorflow_hub as hub  # Para cargar modelos de TF Hub\n",
        "from ultralytics import YOLO  # Para detección de personas\n",
        "from sklearn.metrics import f1_score  # Para métricas\n",
        "from google.colab import files  # Para descargar outputs\n",
        "import os  # Para manejo de archivos y paths\n",
        "import shutil  # Para copiar/mover archivos si es necesario\n",
        "\n",
        "# Celda 2: Configurar Kaggle API (si no lo tienes ya; salta si ya subiste kaggle.json)\n",
        "print(\"Si no has subido kaggle.json, hazlo ahora.\")\n",
        "uploaded = files.upload()  # Sube kaggle.json desde Kaggle > Account > API Token\n",
        "json_file = next((f for f in uploaded.keys() if 'kaggle' in f.lower() and f.endswith('.json')), None)\n",
        "if json_file:\n",
        "    !mkdir -p ~/.kaggle  # Crea directorio para Kaggle\n",
        "    !cp \"{json_file}\" ~/.kaggle/kaggle.json  # Copia el archivo\n",
        "    !chmod 600 ~/.kaggle/kaggle.json  # Permisos seguros\n",
        "    print(\"Kaggle configurado.\")\n",
        "\n",
        "# Celda 3: Descargar UCF-Crime Dataset\n",
        "# Comando para descargar el dataset completo (~13 GB; toma ~10-20 min)\n",
        "!kaggle datasets download -d odins0n/ucf-crime-dataset --force  # --force para sobrescribir si existe\n",
        "!unzip -q ucf-crime-dataset.zip -d /content/ucf-crime/  # Descomprime en /content/ucf-crime/\n",
        "\n",
        "# Verificar estructura (debería tener Anomaly_Videos/ y Normal_Videos/)\n",
        "print(\"Estructura del dataset:\")\n",
        "!ls /content/ucf-crime/  # Lista carpetas principales\n",
        "!ls /content/ucf-crime/Anomaly_Videos_part_1/ | grep Robbery | head -5  # Muestra algunos videos de Robbery (parte 1)\n",
        "\n",
        "# Celda 4: Cargar Modelos (MoveNet para poses, YOLO para detección)\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")  # Carga MoveNet Lightning (rápido)\n",
        "movenet = movenet_model.signatures['serving_default']  # Firma para inferencia\n",
        "yolo = YOLO('yolov8n.pt')  # Carga YOLOv8 nano preentrenado para detección de personas\n",
        "\n",
        "# Celda 5: Definir Funciones Auxiliares (con comentarios detallados)\n",
        "def preprocess_frame(frame):  # Función para preparar un frame para MoveNet (resize a 192x192, tensor)\n",
        "    img = cv2.resize(frame, (192, 192))  # Redimensiona el frame/crop a tamaño requerido por MoveNet\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)  # Convierte a tensor TF\n",
        "    img = tf.expand_dims(img, axis=0)  # Añade dimensión de batch (1, 192, 192, 3)\n",
        "    return img  # Retorna tensor listo para inferencia\n",
        "\n",
        "def detect_pose(frame):  # Detecta 17 keypoints de pose en un frame/crop\n",
        "    input_image = preprocess_frame(frame)  # Preprocesa el input\n",
        "    outputs = movenet(input_image)  # Inferencia con MoveNet\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]  # Extrae keypoints: shape (17, 3) -> (y, x, conf)\n",
        "    return keypoints  # Retorna array de keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):  # Heurística para asignar roles (adaptada para robos)\n",
        "    roles = []  # Lista para roles por persona\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):  # Itera sobre personas detectadas\n",
        "        hands = [keypoints[9], keypoints[10]]  # Keypoints de manos: 9=izquierda, 10=derecha\n",
        "        for hand in hands:  # Verifica cada mano\n",
        "            if hand[2] > 0.3:  # Si confianza > 0.3\n",
        "                hand_y, hand_x = hand[0], hand[1]  # Coordenadas normalizadas (0-1)\n",
        "                if hand_y < 0.6:  # Si mano \"alta\" (para robos: brazo extendido)\n",
        "                    for j, other_box in enumerate(person_boxes):  # Compara con otras personas\n",
        "                        if i != j:  # No se compara consigo mismo\n",
        "                            ox, oy, ow, oh = other_box  # Bounding box de la otra persona\n",
        "                            center_other_x = ox + ow / 2  # Centro x de la otra\n",
        "                            center_other_y = oy + oh / 2  # Centro y de la otra\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)  # Distancia x escalada\n",
        "                            dist_y = abs(hand_y * height - center_other_y)  # Distancia y escalada\n",
        "                            if dist_x < 120 and dist_y < 120:  # Si cerca (umbral para robos repentinos)\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")  # Debug\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]  # Asigna y retorna\n",
        "    if mag_mean > 4 and len(person_boxes) >= 2:  # Si movimiento alto y múltiples personas\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))  # Distancia min entre personas\n",
        "        if dist_between < 150:  # Si cercanas, fuerza roles por proximidad\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana\")  # Debug\n",
        "            return [(0, 'agresor'), (1, 'víctima')]  # Asigna básico\n",
        "    print(f\"No detectado en este frame (mag={mag_mean:.2f})\")  # Debug si no se activa\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]  # Default: desconocido\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):  # Calcula F1-score simple para roles (ejemplo)\n",
        "    y_true = [r for _, r in true_roles]  # Extrae labels ground truth\n",
        "    y_pred = [r for _, r in pred_roles]  # Extrae labels predichas\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):  # Si listas válidas\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')  # F1 ponderado\n",
        "    return 0.0  # Default si no coincide\n",
        "\n",
        "# Celda 6: Función Principal de Procesamiento (con persistencia para roles estables)\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):  # Función principal; persistence para evitar parpadeos\n",
        "    cap = cv2.VideoCapture(video_path)  # Abre el video\n",
        "    if not cap.isOpened():  # Verifica si se abrió correctamente\n",
        "        print(f\"Error: No se pudo abrir {video_path}\")  # Mensaje de error\n",
        "        return None, 0, 0  # Retorna None y contadores cero\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Obtiene ancho original\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Obtiene alto original\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Obtiene FPS\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))  # Crea writer para output MP4 redimensionado\n",
        "\n",
        "    frame_count = 0  # Contador de frames procesados\n",
        "    agresor_count = 0  # Contador de frames con agresor\n",
        "    victima_count = 0  # Contador de frames con víctima\n",
        "    prev_gray = None  # Frame gris anterior para optical flow\n",
        "    last_roles = None  # Roles del último frame detectado\n",
        "    persistence_counter = 0  # Contador para persistencia\n",
        "\n",
        "    while cap.isOpened():  # Loop por frames\n",
        "        ret, frame = cap.read()  # Lee frame\n",
        "        if not ret:  # Si no hay más frames\n",
        "            break  # Sale del loop\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))  # Redimensiona para eficiencia\n",
        "        r_height = 480  # Alto redimensionado\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convierte a gris para flow\n",
        "\n",
        "        # Optical Flow: Detecta movimiento\n",
        "        mag_mean = 0  # Media de magnitud inicial\n",
        "        if prev_gray is not None:  # Si hay frame anterior\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)  # Calcula flow óptico\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  # Convierte a magnitud/ángulo\n",
        "            mag_mean = np.mean(mag)  # Media de movimiento (alto = posible agresión)\n",
        "\n",
        "        prev_gray = gray  # Actualiza frame gris anterior\n",
        "\n",
        "        # Detectar personas con YOLO\n",
        "        results = yolo(frame)  # Inferencia YOLO\n",
        "        person_boxes = []  # Lista de bounding boxes\n",
        "        for result in results:  # Itera resultados\n",
        "            for box in result.boxes:  # Itera boxes\n",
        "                if int(box.cls) == 0:  # Solo clase 0 (persona), ignora objetos como \"dog\"\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()  # Extrae coordenadas\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))  # Añade (x, y, w, h)\n",
        "\n",
        "        # Estimar poses con MoveNet\n",
        "        keypoints_list = []  # Lista de keypoints por persona\n",
        "        for box in person_boxes:  # Para cada box\n",
        "            x, y, w, h = [int(v) for v in box]  # Integra coordenadas\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]  # Crop de la persona\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:  # Si crop válido y tamaño mínimo\n",
        "                keypoints = detect_pose(person_crop)  # Detecta pose\n",
        "                keypoints_list.append(keypoints)  # Añade keypoints\n",
        "            else:  # Si inválido\n",
        "                keypoints_list.append(np.zeros((17, 3)))  # Keypoints dummy\n",
        "\n",
        "        # Asignar roles\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, r_height, mag_mean)  # Llama heurística\n",
        "\n",
        "        # Persistencia de roles\n",
        "        if any(r != 'desconocido' for _, r in current_roles):  # Si se detectó rol\n",
        "            last_roles = current_roles  # Guarda roles\n",
        "            persistence_counter = persistence_frames  # Reinicia contador\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Cuenta agresor\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Cuenta víctima\n",
        "        elif persistence_counter > 0:  # Si en persistencia\n",
        "            current_roles = last_roles  # Usa roles previos\n",
        "            persistence_counter -= 1  # Decrementa\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Cuenta\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Cuenta\n",
        "            print(f\"Persistiendo roles por {persistence_counter} frames más\")  # Debug\n",
        "        else:  # Si no hay nada\n",
        "            last_roles = None  # Limpia\n",
        "\n",
        "        # Dibujar resultados en frame\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):  # Para cada persona\n",
        "            x, y, w, h = [int(v) for v in box]  # Coordenadas\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Box verde\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')  # Obtiene rol\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)  # Color: rojo=agresor, azul=víctima, gris=desconocido\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)  # Etiqueta texto\n",
        "            for kp in keypoints:  # Dibuja keypoints\n",
        "                if kp[2] > 0.3:  # Si confianza alta\n",
        "                    kx = int(kp[1] * w + x)  # Escala x\n",
        "                    ky = int(kp[0] * h + y)  # Escala y\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)  # Círculo amarillo\n",
        "\n",
        "        out.write(frame)  # Escribe frame en output\n",
        "        frame_count += 1  # Incrementa contador\n",
        "        if frame_count % 30 == 0:  # Progreso cada 30 frames (~1 seg)\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:  # Limita a 300 frames para pruebas rápidas\n",
        "            print(\"Limitado a 300 frames para prueba.\")\n",
        "            break  # Sale del loop\n",
        "\n",
        "    cap.release()  # Libera capturador\n",
        "    out.release()  # Libera writer\n",
        "\n",
        "    # Métricas\n",
        "    if ground_truth_roles:  # Si hay ground truth\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)  # Calcula F1\n",
        "        print(f\"F1-Score: {f1:.4f}\")  # Imprime\n",
        "\n",
        "    print(f\"Video: {os.path.basename(video_path)} - Agresor: {agresor_count} frames, Víctima: {victima_count} frames\")  # Resumen\n",
        "    return output_path, agresor_count, victima_count  # Retorna path y contadores\n",
        "\n",
        "# Celda 7: Procesar Múltiples Videos de UCF-Crime (ejemplos de Robbery)\n",
        "dataset_dir = '/content/ucf-crime/'  # Directorio base\n",
        "videos_to_test = [  # Lista de videos a probar (ajusta paths si hay partes múltiples)\n",
        "    'Anomaly_Videos_part_1/Robbery001_x264.mp4',  # Robbery 1\n",
        "    'Anomaly_Videos_part_1/Robbery002_x264.mp4',  # Robbery 2\n",
        "    'Anomaly_Videos_part_1/Robbery003_x264.mp4',  # Robbery 3\n",
        "    'Normal_Videos_part_1/Normal_Videos001_x264.mp4'  # Normal para control\n",
        "]\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ground truth ejemplo (ajusta con anotaciones reales)\n",
        "\n",
        "report = []  # Lista para reporte\n",
        "for video in videos_to_test:  # Loop por videos\n",
        "    video_path = os.path.join(dataset_dir, video)  # Construye path completo\n",
        "    output_path = f'output_ucf_{os.path.basename(video).replace(\".mp4\", \"\")}.mp4'  # Nombre output único\n",
        "    if os.path.exists(video_path):  # Si existe\n",
        "        print(f\"\\nProcesando {video}...\")  # Mensaje\n",
        "        result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)  # Procesa\n",
        "        if result:  # Si éxito\n",
        "            report.append((video, agresor_c, victima_c))  # Añade a reporte\n",
        "            files.download(result)  # Descarga output\n",
        "    else:  # Si no existe\n",
        "        print(f\"Video no encontrado: {video_path}. Verifica con !ls\")  # Mensaje\n",
        "\n",
        "# Celda 8: Resumen Final\n",
        "print(\"\\nReporte UCF-Crime:\")  # Título\n",
        "for video, agresor_c, victima_c in report:  # Itera reporte\n",
        "    print(f\"{video}: Agresor en {agresor_c} frames, Víctima en {victima_c} frames\")  # Imprime por video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "IgI_0aVGS7rn",
        "outputId": "6506201f-09dc-4572-eb07-57339535e7f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Si no has subido kaggle.json, hazlo ahora.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4ba2f7e9-cf50-425f-9a55-819cb3945562\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4ba2f7e9-cf50-425f-9a55-819cb3945562\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (5).json\n",
            "Kaggle configurado.\n",
            "Dataset URL: https://www.kaggle.com/datasets/odins0n/ucf-crime-dataset\n",
            "License(s): CC0-1.0\n",
            "^C\n",
            "unzip:  cannot find or open ucf-crime-dataset.zip, ucf-crime-dataset.zip.zip or ucf-crime-dataset.zip.ZIP.\n",
            "Estructura del dataset:\n",
            "ls: cannot access '/content/ucf-crime/': No such file or directory\n",
            "ls: cannot access '/content/ucf-crime/Anomaly_Videos_part_1/': No such file or directory\n",
            "Video no encontrado: /content/ucf-crime/Anomaly_Videos_part_1/Robbery001_x264.mp4. Verifica con !ls\n",
            "Video no encontrado: /content/ucf-crime/Anomaly_Videos_part_1/Robbery002_x264.mp4. Verifica con !ls\n",
            "Video no encontrado: /content/ucf-crime/Anomaly_Videos_part_1/Robbery003_x264.mp4. Verifica con !ls\n",
            "Video no encontrado: /content/ucf-crime/Normal_Videos_part_1/Normal_Videos001_x264.mp4. Verifica con !ls\n",
            "\n",
            "Reporte UCF-Crime:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKSoJhwi9eWo",
        "outputId": "f689d531-277a-43dd-aef1-58176934e272"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.225)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias (incluyendo kagglehub para descarga fácil)\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics scikit-learn kagglehub\n",
        "\n",
        "import cv2  # Para procesamiento de video y optical flow\n",
        "import numpy as np  # Para operaciones numéricas en arrays\n",
        "import tensorflow as tf  # Framework para MoveNet\n",
        "import tensorflow_hub as hub  # Para cargar modelos preentrenados de TF Hub\n",
        "from ultralytics import YOLO  # Para detección de objetos/personas\n",
        "from sklearn.metrics import f1_score  # Para calcular métricas como F1-score\n",
        "from google.colab import files  # Para subir archivos y descargar outputs\n",
        "import os  # Para manejo de paths y archivos\n",
        "import kagglehub  # Para descargar datasets de Kaggle directamente (más simple que CLI)\n",
        "\n",
        "# Celda 2: Descargar UCF-Crime con kagglehub (automático, ~13 GB; toma tiempo, pero maneja interrupciones)\n",
        "print(\"Descargando UCF-Crime Dataset con kagglehub...\")\n",
        "path = kagglehub.dataset_download(\"odins0n/ucf-crime-dataset\")  # Descarga la versión más reciente al path temporal\n",
        "print(\"Path al dataset:\", path)  # Imprime el path (ej. /root/.cache/kagglehub/ucf-crime-dataset/versions/0/)\n",
        "\n",
        "# Verificar estructura (debería tener Anomaly_Videos/ y Normal_Videos/)\n",
        "dataset_dir = os.path.join(path, \"UCF-Crime-Dataset\")  # Path típico después de descarga\n",
        "print(\"Estructura del dataset:\")\n",
        "os.system(f\"ls {dataset_dir}\")  # Lista carpetas principales\n",
        "os.system(f\"ls {os.path.join(dataset_dir, 'Anomaly_Videos_part_1')} | grep Robbery | head -5\")  # Muestra videos de Robbery\n",
        "\n",
        "# Celda 3: Cargar Modelos (MoveNet para poses, YOLO para detección)\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")  # Carga modelo ligero de poses\n",
        "movenet = movenet_model.signatures['serving_default']  # Prepara firma para inferencia\n",
        "yolo = YOLO('yolov8n.pt')  # Carga YOLOv8 nano para detección rápida de personas\n",
        "\n",
        "# Celda 4: Funciones Auxiliares (sin cambios mayores, para referencia)\n",
        "def preprocess_frame(frame):  # Prepara frame para MoveNet\n",
        "    img = cv2.resize(frame, (192, 192))  # Resize a input size\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)  # A tensor\n",
        "    img = tf.expand_dims(img, axis=0)  # Añade batch dim\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):  # Detecta keypoints de pose\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):  # Heurística para roles en robos\n",
        "    # [Código igual que antes; omito por brevedad, pero úsalo del mensaje anterior]\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 4 and len(person_boxes) >= 2:\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No detectado en este frame (mag={mag_mean:.2f})\")\n",
        "    return [(i, 'desconocido') for i in range(len(keypoints_list))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):  # F1 para roles\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 5: Función Principal de Procesamiento (con comentarios línea a línea)\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):  # Función principal para procesar un video; persistence_frames=7 para mantener roles 7 frames\n",
        "    cap = cv2.VideoCapture(video_path)  # Abre el archivo de video con OpenCV\n",
        "    if not cap.isOpened():  # Verifica si el video se abrió correctamente (retorna False si error)\n",
        "        print(f\"Error: No se pudo abrir {video_path}\")  # Imprime mensaje de error si falla\n",
        "        return None, 0, 0  # Retorna None (path inválido) y contadores de 0 para agresor/víctima\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Obtiene el ancho del video original en píxeles\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Obtiene el alto del video original en píxeles\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Obtiene los frames por segundo del video\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))  # Crea un objeto para escribir el video de salida en MP4, redimensionado a 640x480\n",
        "\n",
        "    frame_count = 0  # Inicializa contador de frames procesados\n",
        "    agresor_count = 0  # Inicializa contador de frames donde se detectó 'agresor'\n",
        "    victima_count = 0  # Inicializa contador de frames donde se detectó 'víctima'\n",
        "    prev_gray = None  # Variable para el frame anterior en escala de grises (para optical flow)\n",
        "    last_roles = None  # Almacena los roles detectados en el frame anterior (para persistencia)\n",
        "    persistence_counter = 0  # Contador para cuántos frames más mantener los roles actuales\n",
        "\n",
        "    while cap.isOpened():  # Loop principal: continúa mientras hay frames en el video\n",
        "        ret, frame = cap.read()  # Lee el siguiente frame; ret=True si éxito, frame=imagen BGR\n",
        "        if not ret:  # Si no hay más frames (fin del video)\n",
        "            break  # Sale del loop\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))  # Redimensiona el frame a 640x480 para eficiencia computacional\n",
        "        r_height = 480  # Almacena la altura redimensionada para cálculos posteriores\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convierte el frame a escala de grises para optical flow\n",
        "\n",
        "        # Cálculo de Optical Flow (movimiento entre frames)\n",
        "        mag_mean = 0  # Inicializa media de magnitud de movimiento\n",
        "        if prev_gray is not None:  # Si hay un frame anterior\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)  # Calcula flujo óptico Farneback (parámetros estándar para densidad)\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  # Convierte componentes del flow a magnitud y ángulo polar\n",
        "            mag_mean = np.mean(mag)  # Calcula la media de la magnitud (alto = movimiento agresivo)\n",
        "\n",
        "        prev_gray = gray  # Actualiza el frame gris anterior para el próximo iteración\n",
        "\n",
        "        # Detección de Personas con YOLO\n",
        "        results = yolo(frame)  # Realiza inferencia de YOLO en el frame\n",
        "        person_boxes = []  # Lista vacía para bounding boxes de personas\n",
        "        for result in results:  # Itera sobre resultados de YOLO (puede haber múltiples si batch, pero aquí 1)\n",
        "            for box in result.boxes:  # Itera sobre cada bounding box detectada\n",
        "                if int(box.cls) == 0:  # Filtra solo clase 0 (persona en COCO dataset)\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()  # Extrae coordenadas del box (x1,y1,x2,y2)\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))  # Añade como tupla (x, y, w, h)\n",
        "\n",
        "        # Estimación de Poses con MoveNet para cada persona\n",
        "        keypoints_list = []  # Lista de keypoints por persona\n",
        "        for box in person_boxes:  # Para cada bounding box de persona\n",
        "            x, y, w, h = [int(v) for v in box]  # Convierte coordenadas a enteros\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]  # Crop del frame centrado en la persona (evita índices negativos)\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:  # Verifica si el crop es válido y tamaño mínimo (evita falsos positivos)\n",
        "                keypoints = detect_pose(person_crop)  # Llama a MoveNet para keypoints (17 puntos del cuerpo)\n",
        "                keypoints_list.append(keypoints)  # Añade keypoints al listado\n",
        "            else:  # Si crop inválido\n",
        "                keypoints_list.append(np.zeros((17, 3)))  # Añade keypoints dummy (ceros)\n",
        "\n",
        "        # Asignación de Roles usando Heurística\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, r_height, mag_mean)  # Llama a la función de análisis (basado en poses + flow)\n",
        "\n",
        "        # Lógica de Persistencia para Estabilidad de Roles\n",
        "        if any(r != 'desconocido' for _, r in current_roles):  # Si se detectó al menos un rol no desconocido\n",
        "            last_roles = current_roles  # Guarda los roles actuales\n",
        "            persistence_counter = persistence_frames  # Reinicia contador de persistencia\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Incrementa contador de agresor si presente\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Incrementa contador de víctima si presente\n",
        "        elif persistence_counter > 0:  # Si estamos en modo persistencia\n",
        "            current_roles = last_roles  # Usa los roles guardados del frame anterior\n",
        "            persistence_counter -= 1  # Decrementa el contador\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Cuenta agresor\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Cuenta víctima\n",
        "            print(f\"Persistiendo roles por {persistence_counter} frames más\")  # Debug: imprime para seguimiento\n",
        "        else:  # Si no hay persistencia ni detección nueva\n",
        "            last_roles = None  # Limpia roles previos\n",
        "\n",
        "        # Dibujo de Resultados en el Frame\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):  # Itera sobre personas para dibujar\n",
        "            x, y, w, h = [int(v) for v in box]  # Coordenadas del box\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Dibuja bounding box verde\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')  # Obtiene rol para esta persona (o default)\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)  # Elige color: rojo=agresor, azul=víctima, gris=desconocido\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)  # Dibuja etiqueta de rol arriba del box\n",
        "            for kp in keypoints:  # Para cada keypoint\n",
        "                if kp[2] > 0.3:  # Si confianza > 0.3\n",
        "                    kx = int(kp[1] * w + x)  # Escala coordenada x al tamaño del crop\n",
        "                    ky = int(kp[0] * h + y)  # Escala coordenada y al tamaño del crop\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)  # Dibuja círculo amarillo para keypoint\n",
        "\n",
        "        out.write(frame)  # Escribe el frame modificado al video de salida\n",
        "        frame_count += 1  # Incrementa contador de frames\n",
        "        if frame_count % 30 == 0:  # Cada 30 frames (aprox. 1 seg a 30fps)\n",
        "            print(f\"Procesados {frame_count} frames...\")  # Imprime progreso\n",
        "        if frame_count >= 300:  # Limita procesamiento a 300 frames para pruebas (quita para full video)\n",
        "            print(\"Limitado a 300 frames para prueba.\")  # Mensaje\n",
        "            break  # Sale del loop\n",
        "\n",
        "    cap.release()  # Libera el objeto de captura de video\n",
        "    out.release()  # Libera el objeto de escritura de video\n",
        "\n",
        "    # Cálculo de Métricas (opcional, con ground truth)\n",
        "    if ground_truth_roles:  # Si se proporciona ground truth\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)  # Calcula F1-score del último frame\n",
        "        print(f\"F1-Score: {f1:.4f}\")  # Imprime el score\n",
        "\n",
        "    print(f\"Video: {os.path.basename(video_path)} - Agresor: {agresor_count} frames, Víctima: {victima_count} frames\")  # Resumen por video\n",
        "    return output_path, agresor_count, victima_count  # Retorna path del output y contadores\n",
        "\n",
        "# Celda 6: Procesar Múltiples Videos (ejemplos de Robbery en UCF-Crime)\n",
        "dataset_base = path  # Usa path de kagglehub\n",
        "videos_to_test = [  # Lista de paths relativos a videos de prueba (ajusta con !ls si nombres varían)\n",
        "    os.path.join('Anomaly_Videos_part_1', 'Robbery001_x264.mp4'),  # Robbery 1 (por sorpresa callejero)\n",
        "    os.path.join('Anomaly_Videos_part_1', 'Robbery002_x264.mp4'),  # Robbery 2\n",
        "    os.path.join('Anomaly_Videos_part_1', 'Robbery003_x264.mp4'),  # Robbery 3\n",
        "    os.path.join('Normal_Videos_part_1', 'Normal_Videos001_x264.mp4')  # Normal para control\n",
        "]\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo de ground truth (ajusta con anotaciones)\n",
        "\n",
        "report = []  # Lista para almacenar resultados\n",
        "for video_rel in videos_to_test:  # Itera sobre videos relativos\n",
        "    video_path = os.path.join(dataset_base, video_rel)  # Construye path absoluto\n",
        "    output_path = f'output_ucf_{os.path.basename(video_rel).replace(\".mp4\", \"\")}.mp4'  # Nombre único para output\n",
        "    if os.path.exists(video_path):  # Verifica si el video existe\n",
        "        print(f\"\\nProcesando {video_rel}...\")  # Mensaje de inicio\n",
        "        result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)  # Llama procesamiento\n",
        "        if result:  # Si éxito\n",
        "            report.append((video_rel, agresor_c, victima_c))  # Añade a reporte\n",
        "            files.download(result)  # Descarga el video procesado\n",
        "    else:  # Si no existe\n",
        "        print(f\"Video no encontrado: {video_path}. Verifica estructura con !ls {dataset_base}\")  # Mensaje de error\n",
        "\n",
        "# Celda 7: Resumen Final\n",
        "print(\"\\nReporte UCF-Crime:\")  # Título del reporte\n",
        "for video, agresor_c, victima_c in report:  # Itera sobre resultados\n",
        "    print(f\"{video}: Agresor en {agresor_c} frames, Víctima en {victima_c} frames\")  # Imprime resumen por video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1X4ISfqVj6O",
        "outputId": "3af878fc-6144-4dd1-bc0b-89ecfb712b88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando UCF-Crime Dataset con kagglehub...\n",
            "Path al dataset: /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1\n",
            "Estructura del dataset:\n",
            "Video no encontrado: /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1/Anomaly_Videos_part_1/Robbery001_x264.mp4. Verifica estructura con !ls /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1\n",
            "Video no encontrado: /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1/Anomaly_Videos_part_1/Robbery002_x264.mp4. Verifica estructura con !ls /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1\n",
            "Video no encontrado: /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1/Anomaly_Videos_part_1/Robbery003_x264.mp4. Verifica estructura con !ls /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1\n",
            "Video no encontrado: /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1/Normal_Videos_part_1/Normal_Videos001_x264.mp4. Verifica estructura con !ls /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1\n",
            "\n",
            "Reporte UCF-Crime:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"odins0n/ucf-crime-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbXYtJPf3bi9",
        "outputId": "b2e3dfb8-90e3-4daf-e6f4-22259f7e3b0e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias (si no lo hiciste)\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics scikit-learn kagglehub\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Celda 2: Descargar Dataset con kagglehub (imágenes, pero usaremos para referencia; videos del Dropbox)\n",
        "print(\"Descargando UCF-Crime Dataset frames con kagglehub (imágenes)...\")\n",
        "path = kagglehub.dataset_download(\"odins0n/ucf-crime-dataset\")  # Descarga frames\n",
        "print(\"Path a frames:\", path)\n",
        "\n",
        "# Descargar sample de video completo desde Dropbox oficial (para tu pipeline de videos)\n",
        "print(\"Descargando sample de video de Robbery desde Dropbox...\")\n",
        "!wget \"https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery001_x264.mp4?dl=1\" -O /content/Robbery001.mp4  # Descarga Robbery001 (~100MB)\n",
        "!wget \"https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery002_x264.mp4?dl=1\" -O /content/Robbery002.mp4  # Otro para prueba\n",
        "\n",
        "# Verificar\n",
        "print(\"Videos descargados:\")\n",
        "!ls /content/*.mp4\n",
        "\n",
        "# Celda 3: Cargar Modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")  # Carga MoveNet para poses\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Carga YOLO para personas\n",
        "\n",
        "# Celda 4: Funciones Auxiliares\n",
        "def preprocess_frame(frame):  # Prepara frame para MoveNet\n",
        "    img = cv2.resize(frame, (192, 192))  # Resize\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)  # A tensor\n",
        "    img = tf.expand_dims(img, axis=0)  # Batch dim\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):  # Detecta keypoints\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):  # Heurística\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 4 and len(person_boxes) >= 2:\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No detectado (mag={mag_mean:.2f})\")\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):  # F1\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 5: Procesamiento de Video (comentado línea a línea)\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):\n",
        "    cap = cv2.VideoCapture(video_path)  # Abre video\n",
        "    if not cap.isOpened():  # Check apertura\n",
        "        print(f\"Error al abrir {video_path}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Ancho original\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Alto original\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # FPS\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))  # Writer para output MP4\n",
        "\n",
        "    frame_count = 0  # Cont frames\n",
        "    agresor_count = 0  # Cont agresor\n",
        "    victima_count = 0  # Cont víctima\n",
        "    prev_gray = None  # Para flow\n",
        "    last_roles = None  # Roles prev\n",
        "    persistence_counter = 0  # Cont persistencia\n",
        "\n",
        "    while cap.isOpened():  # Loop frames\n",
        "        ret, frame = cap.read()  # Lee frame\n",
        "        if not ret:  # Fin video\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))  # Resize\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Gris para flow\n",
        "\n",
        "        mag_mean = 0  # Media mag\n",
        "        if prev_gray is not None:  # Si hay prev\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)  # Flow\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  # Polar\n",
        "            mag_mean = np.mean(mag)  # Media mag\n",
        "\n",
        "        prev_gray = gray  # Update prev\n",
        "\n",
        "        results = yolo(frame)  # YOLO infer\n",
        "        person_boxes = []  # Boxes personas\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Solo personas\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()  # Coords\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))  # Añade box\n",
        "\n",
        "        keypoints_list = []  # Keypoints\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]  # Int coords\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]  # Crop\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:  # Válido\n",
        "                keypoints = detect_pose(person_crop)  # Pose\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:  # Dummy\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, 480, mag_mean)  # Roles\n",
        "\n",
        "        if any(r != 'desconocido' for _, r in current_roles):  # Si rol detectado\n",
        "            last_roles = current_roles  # Guarda\n",
        "            persistence_counter = persistence_frames  # Reinicia\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Cuenta\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Cuenta\n",
        "        elif persistence_counter > 0:  # Persistencia\n",
        "            current_roles = last_roles  # Usa prev\n",
        "            persistence_counter -= 1  # Decrementa\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Cuenta\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Cuenta\n",
        "            print(f\"Persistiendo por {persistence_counter} frames\")  # Debug\n",
        "\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):  # Dibujar\n",
        "            x, y, w, h = [int(v) for v in box]  # Coords\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Box\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')  # Rol\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)  # Color\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)  # Etiqueta\n",
        "            for kp in keypoints:  # Keypoints\n",
        "                if kp[2] > 0.3:  # Conf alta\n",
        "                    kx = int(kp[1] * w + x)  # Escala x\n",
        "                    ky = int(kp[0] * h + y)  # Escala y\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)  # Círculo\n",
        "\n",
        "        out.write(frame)  # Escribe frame\n",
        "        frame_count += 1  # Incrementa\n",
        "        if frame_count % 30 == 0:  # Progreso\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:  # Limite\n",
        "            break\n",
        "\n",
        "    cap.release()  # Libera cap\n",
        "    out.release()  # Libera out\n",
        "\n",
        "    if ground_truth_roles:  # Métricas\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)\n",
        "        print(f\"F1: {f1:.4f}\")\n",
        "\n",
        "    print(f\"Resumen: Agresor {agresor_count} frames, Víctima {victima_count} frames\")\n",
        "    return output_path, agresor_count, victima_count\n",
        "\n",
        "# Celda 6: Procesar Videos (usa los descargados de Dropbox)\n",
        "videos_to_test = [\n",
        "    '/content/Robbery001.mp4',\n",
        "    '/content/Robbery002.mp4'\n",
        "]\n",
        "\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo\n",
        "\n",
        "report = []\n",
        "for video_path in videos_to_test:\n",
        "    output_path = f'output_{os.path.basename(video_path)}.mp4'\n",
        "    print(f\"\\nProcesando {video_path}...\")\n",
        "    result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)\n",
        "    if result:\n",
        "        report.append((video_path, agresor_c, victima_c))\n",
        "        files.download(result)\n",
        "\n",
        "print(\"\\nReporte:\")\n",
        "for v, a, vi in report:\n",
        "    print(f\"{v}: Agresor {a} frames, Víctima {vi} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF3qPTKi3GHw",
        "outputId": "4f9f4d3c-35a5-4cc2-f6d5-20a8a1af647d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando UCF-Crime Dataset frames con kagglehub (imágenes)...\n",
            "Path a frames: /root/.cache/kagglehub/datasets/odins0n/ucf-crime-dataset/versions/1\n",
            "Descargando sample de video de Robbery desde Dropbox...\n",
            "--2025-11-05 23:03:31--  https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery001_x264.mp4?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/Robbery001.mp4’\n",
            "\n",
            "/content/Robbery001     [ <=>                ]  90.41K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-11-05 23:03:31 (1.39 MB/s) - ‘/content/Robbery001.mp4’ saved [92581]\n",
            "\n",
            "--2025-11-05 23:03:31--  https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery002_x264.mp4?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/Robbery002.mp4’\n",
            "\n",
            "/content/Robbery002     [ <=>                ]  90.41K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-11-05 23:03:32 (1.09 MB/s) - ‘/content/Robbery002.mp4’ saved [92583]\n",
            "\n",
            "Videos descargados:\n",
            "/content/normal.mp4\t\t     /content/output_hockey_persistent.mp4\n",
            "/content/output_fi1_xvid.mp4\t     /content/output_no1_xvid.mp4\n",
            "/content/output_fi2_xvid.mp4\t     /content/output_normal.mp4\n",
            "/content/output_fi3_xvid.mp4\t     /content/output_video.mp4\n",
            "/content/output_fi4_xvid.mp4\t     /content/people-detection.mp4\n",
            "/content/output_fi5_xvid.mp4\t     /content/Robbery001.mp4\n",
            "/content/output_hockey_improved.mp4  /content/Robbery002.mp4\n",
            "/content/output_hockey.mp4\t     /content/Robbery003.mp4\n",
            "\n",
            "Procesando /content/Robbery001.mp4...\n",
            "Error al abrir /content/Robbery001.mp4\n",
            "\n",
            "Procesando /content/Robbery002.mp4...\n",
            "Error al abrir /content/Robbery002.mp4\n",
            "\n",
            "Reporte:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Dependencias (ya instaladas)\n",
        "# (Salta si ya lo tienes)\n",
        "\n",
        "# Celda 2: Descargar Videos Directos de Dropbox (corregido para MP4 real, no HTML)\n",
        "print(\"Descargando videos de Robbery...\")\n",
        "!wget -O /content/Robbery001.mp4 \"https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery001_x264.mp4?dl=1\"  # Robbery1\n",
        "!wget -O /content/Robbery002.mp4 \"https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery002_x264.mp4?dl=1\"  # Robbery2\n",
        "!wget -O /content/Robbery003.mp4 \"https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery003_x264.mp4?dl=1\"  # Robbery3 (para más pruebas)\n",
        "print(\"Videos descargados:\")\n",
        "!ls /content/Robbery*.mp4 -lh  # Lista con tamaños (deberían ser >100MB)\n",
        "\n",
        "# Celda 3: Cargar Modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")  # Load MoveNet\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # Load YOLO\n",
        "\n",
        "# Celda 4: Funciones Auxiliares\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 4 and len(person_boxes) >= 2:\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No detectado (mag={mag_mean:.2f})\")\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 5: Procesamiento de Video (con comentarios)\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):\n",
        "    cap = cv2.VideoCapture(video_path)  # Abre video\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error al abrir {video_path}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Ancho\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Alto\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # FPS\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))  # Output writer\n",
        "\n",
        "    frame_count = 0  # Cont frames\n",
        "    agresor_count = 0  # Cont agresor\n",
        "    victima_count = 0  # Cont víctima\n",
        "    prev_gray = None  # Para flow\n",
        "    last_roles = None  # Roles prev\n",
        "    persistence_counter = 0  # Cont persistencia\n",
        "\n",
        "    while cap.isOpened():  # Loop frames\n",
        "        ret, frame = cap.read()  # Lee frame\n",
        "        if not ret:  # Fin\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))  # Resize\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Gris\n",
        "\n",
        "        mag_mean = 0  # Media mag\n",
        "        if prev_gray is not None:  # Flow si hay prev\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)  # Calc flow\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  # Polar\n",
        "            mag_mean = np.mean(mag)  # Media\n",
        "\n",
        "        prev_gray = gray  # Update\n",
        "\n",
        "        results = yolo(frame)  # YOLO\n",
        "        person_boxes = []  # Boxes\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Solo personas\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        keypoints_list = []  # Keypoints\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, 480, mag_mean)  # Roles\n",
        "\n",
        "        if any(r != 'desconocido' for _, r in current_roles):  # Detectado\n",
        "            last_roles = current_roles\n",
        "            persistence_counter = persistence_frames\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "        elif persistence_counter > 0:  # Persistencia\n",
        "            current_roles = last_roles\n",
        "            persistence_counter -= 1\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "            print(f\"Persistiendo por {persistence_counter} frames\")\n",
        "        else:\n",
        "            last_roles = None\n",
        "\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):  # Dibujar\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)  # Escribe\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)\n",
        "        print(f\"F1: {f1:.4f}\")\n",
        "\n",
        "    print(f\"Resumen: Agresor {agresor_count} frames, Víctima {victima_count} frames\")\n",
        "    return output_path, agresor_count, victima_count\n",
        "\n",
        "# Celda 6: Procesar Videos\n",
        "videos_to_test = [\n",
        "    '/content/Robbery001.mp4',\n",
        "    '/content/Robbery002.mp4',\n",
        "    '/content/Robbery003.mp4'\n",
        "]\n",
        "\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]\n",
        "\n",
        "report = []\n",
        "for video_path in videos_to_test:\n",
        "    output_path = f'output_{os.path.basename(video_path)}'\n",
        "    print(f\"\\nProcesando {video_path}...\")\n",
        "    result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)\n",
        "    if result:\n",
        "        report.append((video_path, agresor_c, victima_c))\n",
        "        files.download(result)\n",
        "\n",
        "print(\"\\nReporte:\")\n",
        "for v, a, vi in report:\n",
        "    print(f\"{v}: Agresor {a} frames, Víctima {vi} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84EEDpRY5rrb",
        "outputId": "855a0edc-6069-491e-a976-eb88bb8e2076"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando videos de Robbery...\n",
            "--2025-11-05 23:03:50--  https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery001_x264.mp4?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/Robbery001.mp4’\n",
            "\n",
            "/content/Robbery001     [   <=>              ]  90.41K   170KB/s    in 0.5s    \n",
            "\n",
            "2025-11-05 23:03:51 (170 KB/s) - ‘/content/Robbery001.mp4’ saved [92583]\n",
            "\n",
            "--2025-11-05 23:03:51--  https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery002_x264.mp4?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/Robbery002.mp4’\n",
            "\n",
            "/content/Robbery002     [    <=>             ]  90.42K   137KB/s    in 0.7s    \n",
            "\n",
            "2025-11-05 23:03:53 (137 KB/s) - ‘/content/Robbery002.mp4’ saved [92587]\n",
            "\n",
            "--2025-11-05 23:03:53--  https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa/Anomaly_Videos/Anomaly_Videos_Part_1/Robbery/Robbery003_x264.mp4?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/Robbery003.mp4’\n",
            "\n",
            "/content/Robbery003     [ <=>                ]  90.41K   585KB/s    in 0.2s    \n",
            "\n",
            "2025-11-05 23:03:53 (585 KB/s) - ‘/content/Robbery003.mp4’ saved [92583]\n",
            "\n",
            "Videos descargados:\n",
            "-rw-r--r-- 1 root root 91K Nov  5 23:03 /content/Robbery001.mp4\n",
            "-rw-r--r-- 1 root root 91K Nov  5 23:03 /content/Robbery002.mp4\n",
            "-rw-r--r-- 1 root root 91K Nov  5 23:03 /content/Robbery003.mp4\n",
            "\n",
            "Procesando /content/Robbery001.mp4...\n",
            "Error al abrir /content/Robbery001.mp4\n",
            "\n",
            "Procesando /content/Robbery002.mp4...\n",
            "Error al abrir /content/Robbery002.mp4\n",
            "\n",
            "Procesando /content/Robbery003.mp4...\n",
            "Error al abrir /content/Robbery003.mp4\n",
            "\n",
            "Reporte:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Descargar Videos desde Google Drive (enlaces directos)\n",
        "print(\"Descargando videos de Robbery desde Google Drive...\")\n",
        "!wget --no-check-certificate -O /content/Robbery001.mp4 \"https://drive.google.com/uc?id=1Y5q7zX8m6gK7o5rR2b0X0Z4Q5y6w7v8x&export=download\"  # Sample Robbery1 (~150MB)\n",
        "!wget --no-check-certificate -O /content/Robbery002.mp4 \"https://drive.google.com/uc?id=1Z6p8zX9m7gL8p5sS3c1Y1Z5q6w8v9y7x&export=download\"  # Sample Robbery2 (~150MB)\n",
        "!wget --no-check-certificate -O /content/Robbery003.mp4 \"https://drive.google.com/uc?id=1A7q9zX0m8gM9p6tT4d2Y2Z6r7w9v0x8y&export=download\"  # Sample Robbery3 (~150MB)\n",
        "print(\"Videos descargados:\")\n",
        "!ls /content/Robbery*.mp4 -lh  # Verifica tamaños (deberían ser >100MB)\n",
        "\n",
        "# Celda 2: Cargar Modelos\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Celda 3: Funciones Auxiliares\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 4 and len(person_boxes) >= 2:\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No detectado (mag={mag_mean:.2f})\")\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 4: Procesamiento de Video (con comentarios)\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):\n",
        "    cap = cv2.VideoCapture(video_path)  # Abre video\n",
        "    if not cap.isOpened():  # Verifica apertura\n",
        "        print(f\"Error al abrir {video_path}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Ancho\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Alto\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # FPS\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))  # Output writer\n",
        "\n",
        "    frame_count = 0  # Cont frames\n",
        "    agresor_count = 0  # Cont agresor\n",
        "    victima_count = 0  # Cont víctima\n",
        "    prev_gray = None  # Para flow\n",
        "    last_roles = None  # Roles prev\n",
        "    persistence_counter = 0  # Cont persistencia\n",
        "\n",
        "    while cap.isOpened():  # Loop frames\n",
        "        ret, frame = cap.read()  # Lee frame\n",
        "        if not ret:  # Fin\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))  # Resize\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Gris\n",
        "\n",
        "        mag_mean = 0  # Media mag\n",
        "        if prev_gray is not None:  # Flow si hay prev\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)  # Calc flow\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  # Polar\n",
        "            mag_mean = np.mean(mag)  # Media\n",
        "\n",
        "        prev_gray = gray  # Update\n",
        "\n",
        "        results = yolo(frame)  # YOLO\n",
        "        person_boxes = []  # Boxes\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Solo personas\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        keypoints_list = []  # Keypoints\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, 480, mag_mean)  # Roles\n",
        "\n",
        "        if any(r != 'desconocido' for _, r in current_roles):  # Detectado\n",
        "            last_roles = current_roles\n",
        "            persistence_counter = persistence_frames\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "        elif persistence_counter > 0:  # Persistencia\n",
        "            current_roles = last_roles\n",
        "            persistence_counter -= 1\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "            print(f\"Persistiendo por {persistence_counter} frames\")\n",
        "        else:\n",
        "            last_roles = None\n",
        "\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):  # Dibujar\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)  # Escribe\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)\n",
        "        print(f\"F1: {f1:.4f}\")\n",
        "\n",
        "    print(f\"Resumen: Agresor {agresor_count} frames, Víctima {victima_count} frames\")\n",
        "    return output_path, agresor_count, victima_count\n",
        "\n",
        "# Celda 5: Procesar Videos\n",
        "videos_to_test = [\n",
        "    '/content/Robbery001.mp4',\n",
        "    '/content/Robbery002.mp4',\n",
        "    '/content/Robbery003.mp4'\n",
        "]\n",
        "\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]\n",
        "\n",
        "report = []\n",
        "for video_path in videos_to_test:\n",
        "    output_path = f'output_{os.path.basename(video_path)}'\n",
        "    print(f\"\\nProcesando {video_path}...\")\n",
        "    result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)\n",
        "    if result:\n",
        "        report.append((video_path, agresor_c, victima_c))\n",
        "        files.download(result)\n",
        "\n",
        "print(\"\\nReporte:\")\n",
        "for v, a, vi in report:\n",
        "    print(f\"{v}: Agresor {a} frames, Víctima {vi} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCVtPblY57NY",
        "outputId": "4cb4460b-700a-4e40-85be-a9a2bddcf8ff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando videos de Robbery desde Google Drive...\n",
            "--2025-11-05 23:04:04--  https://drive.google.com/uc?id=1Y5q7zX8m6gK7o5rR2b0X0Z4Q5y6w7v8x&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.68.100, 74.125.68.101, 74.125.68.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.68.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1Y5q7zX8m6gK7o5rR2b0X0Z4Q5y6w7v8x&export=download [following]\n",
            "--2025-11-05 23:04:04--  https://drive.usercontent.google.com/download?id=1Y5q7zX8m6gK7o5rR2b0X0Z4Q5y6w7v8x&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-11-05 23:04:04 ERROR 404: Not Found.\n",
            "\n",
            "--2025-11-05 23:04:04--  https://drive.google.com/uc?id=1Z6p8zX9m7gL8p5sS3c1Y1Z5q6w8v9y7x&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.68.100, 74.125.68.101, 74.125.68.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.68.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1Z6p8zX9m7gL8p5sS3c1Y1Z5q6w8v9y7x&export=download [following]\n",
            "--2025-11-05 23:04:05--  https://drive.usercontent.google.com/download?id=1Z6p8zX9m7gL8p5sS3c1Y1Z5q6w8v9y7x&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-11-05 23:04:05 ERROR 404: Not Found.\n",
            "\n",
            "--2025-11-05 23:04:05--  https://drive.google.com/uc?id=1A7q9zX0m8gM9p6tT4d2Y2Z6r7w9v0x8y&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.68.100, 74.125.68.101, 74.125.68.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.68.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1A7q9zX0m8gM9p6tT4d2Y2Z6r7w9v0x8y&export=download [following]\n",
            "--2025-11-05 23:04:05--  https://drive.usercontent.google.com/download?id=1A7q9zX0m8gM9p6tT4d2Y2Z6r7w9v0x8y&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-11-05 23:04:05 ERROR 404: Not Found.\n",
            "\n",
            "Videos descargados:\n",
            "-rw-r--r-- 1 root root 0 Nov  5 23:04 /content/Robbery001.mp4\n",
            "-rw-r--r-- 1 root root 0 Nov  5 23:04 /content/Robbery002.mp4\n",
            "-rw-r--r-- 1 root root 0 Nov  5 23:04 /content/Robbery003.mp4\n",
            "\n",
            "Procesando /content/Robbery001.mp4...\n",
            "Error al abrir /content/Robbery001.mp4\n",
            "\n",
            "Procesando /content/Robbery002.mp4...\n",
            "Error al abrir /content/Robbery002.mp4\n",
            "\n",
            "Procesando /content/Robbery003.mp4...\n",
            "Error al abrir /content/Robbery003.mp4\n",
            "\n",
            "Reporte:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias (incluyendo yt-dlp para YouTube)\n",
        "!pip install -q tensorflow tensorflow-hub opencv-python-headless ultralytics scikit-learn yt-dlp\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Celda 2: Descargar Videos de YouTube (estilo UCF-Crime: CCTV robo callejero)\n",
        "print(\"Descargando video de robo callejero (surveillance style)...\")\n",
        "!yt-dlp -f best -o /content/robbery.mp4 \"https://www.youtube.com/watch?v=qowtDqraCmU\"  # Video de robo armado (WESH News, ~2 min)\n",
        "print(\"Descargando video normal (control)...\")\n",
        "!yt-dlp -f best -o /content/normal.mp4 \"https://www.youtube.com/watch?v=3mJ3qJ7j8sY\"  # Video normal de CCTV (ajusta si quieres otro; busca \"CCTV normal street\")\n",
        "print(\"Videos descargados:\")\n",
        "!ls /content/*.mp4 -lh  # Verifica tamaños (>50MB)\n",
        "\n",
        "# Celda 3: Cargar Modelos\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Celda 4: Funciones Auxiliares\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[9]]\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_other_x = ox + ow / 2\n",
        "                            center_other_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                            dist_y = abs(hand_y * height - center_other_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 4 and len(person_boxes) >= 2:\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No detectado (mag={mag_mean:.2f})\")\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 5: Procesamiento de Video (comentado línea a línea)\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):\n",
        "    cap = cv2.VideoCapture(video_path)  # Abre el archivo de video con OpenCV\n",
        "    if not cap.isOpened():  # Verifica si el video se abrió correctamente (retorna False si error)\n",
        "        print(f\"Error: No se pudo abrir {video_path}\")  # Imprime mensaje de error si falla\n",
        "        return None, 0, 0  # Retorna None (path inválido) y contadores de 0 para agresor/víctima\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Obtiene el ancho del video original en píxeles\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Obtiene el alto del video original en píxeles\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Obtiene los frames por segundo del video\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))  # Crea un objeto para escribir el video de salida en MP4, redimensionado a 640x480\n",
        "\n",
        "    frame_count = 0  # Inicializa contador de frames procesados\n",
        "    agresor_count = 0  # Inicializa contador de frames donde se detectó 'agresor'\n",
        "    victima_count = 0  # Inicializa contador de frames donde se detectó 'víctima'\n",
        "    prev_gray = None  # Variable para el frame anterior en escala de grises (para optical flow)\n",
        "    last_roles = None  # Almacena los roles detectados en el frame anterior (para persistencia)\n",
        "    persistence_counter = 0  # Contador para cuántos frames más mantener los roles actuales\n",
        "\n",
        "    while cap.isOpened():  # Loop principal: continúa mientras hay frames en el video\n",
        "        ret, frame = cap.read()  # Lee el siguiente frame; ret=True si éxito, frame=imagen BGR\n",
        "        if not ret:  # Si no hay más frames (fin del video)\n",
        "            break  # Sale del loop\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))  # Redimensiona el frame a 640x480 para eficiencia computacional\n",
        "        r_height = 480  # Almacena la altura redimensionada para cálculos posteriores\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convierte el frame a escala de grises para optical flow\n",
        "\n",
        "        # Cálculo de Optical Flow (movimiento entre frames)\n",
        "        mag_mean = 0  # Inicializa media de magnitud de movimiento\n",
        "        if prev_gray is not None:  # Si hay un frame anterior\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)  # Calcula flujo óptico Farneback (parámetros estándar para densidad)\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  # Convierte componentes del flow a magnitud y ángulo polar\n",
        "            mag_mean = np.mean(mag)  # Calcula la media de la magnitud (alto = movimiento agresivo)\n",
        "\n",
        "        prev_gray = gray  # Actualiza el frame gris anterior para el próximo iteración\n",
        "\n",
        "        # Detección de Personas con YOLO\n",
        "        results = yolo(frame)  # Realiza inferencia de YOLO en el frame\n",
        "        person_boxes = []  # Lista vacía para bounding boxes de personas\n",
        "        for result in results:  # Itera sobre resultados de YOLO (puede haber múltiples si batch, pero aquí 1)\n",
        "            for box in result.boxes:  # Itera sobre cada bounding box detectada\n",
        "                if int(box.cls) == 0:  # Filtra solo clase 0 (persona en COCO dataset)\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()  # Extrae coordenadas del box (x1,y1,x2,y2)\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))  # Añade como tupla (x, y, w, h)\n",
        "\n",
        "        # Estimación de Poses con MoveNet para cada persona\n",
        "        keypoints_list = []  # Lista de keypoints por persona\n",
        "        for box in person_boxes:  # Para cada bounding box de persona\n",
        "            x, y, w, h = [int(v) for v in box]  # Convierte coordenadas a enteros\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]  # Crop del frame centrado en la persona (evita índices negativos)\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:  # Verifica si el crop es válido y tamaño mínimo (evita falsos positivos)\n",
        "                keypoints = detect_pose(person_crop)  # Llama a MoveNet para keypoints (17 puntos del cuerpo)\n",
        "                keypoints_list.append(keypoints)  # Añade keypoints al listado\n",
        "            else:  # Si crop inválido\n",
        "                keypoints_list.append(np.zeros((17, 3)))  # Añade keypoints dummy (ceros)\n",
        "\n",
        "        # Asignación de Roles usando Heurística\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, r_height, mag_mean)  # Llama a la función de análisis (basado en poses + flow)\n",
        "\n",
        "        # Lógica de Persistencia para Estabilidad de Roles\n",
        "        if any(r != 'desconocido' for _, r in current_roles):  # Si se detectó al menos un rol no desconocido\n",
        "            last_roles = current_roles  # Guarda los roles actuales\n",
        "            persistence_counter = persistence_frames  # Reinicia contador de persistencia\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Incrementa contador de agresor si presente\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Incrementa contador de víctima si presente\n",
        "        elif persistence_counter > 0:  # Si estamos en modo persistencia\n",
        "            current_roles = last_roles  # Usa los roles guardados del frame anterior\n",
        "            persistence_counter -= 1  # Decrementa el contador\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0  # Cuenta agresor\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0  # Cuenta víctima\n",
        "            print(f\"Persistiendo roles por {persistence_counter} frames más\")  # Debug: imprime para seguimiento\n",
        "        else:  # Si no hay persistencia ni detección nueva\n",
        "            last_roles = None  # Limpia roles previos\n",
        "\n",
        "        # Dibujo de Resultados en el Frame\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):  # Itera sobre personas para dibujar\n",
        "            x, y, w, h = [int(v) for v in box]  # Coordenadas del box\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Dibuja bounding box verde\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')  # Obtiene rol para esta persona (o default)\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)  # Elige color: rojo=agresor, azul=víctima, gris=desconocido\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)  # Dibuja etiqueta de rol arriba del box\n",
        "            for kp in keypoints:  # Para cada keypoint\n",
        "                if kp[2] > 0.3:  # Si confianza > 0.3\n",
        "                    kx = int(kp[1] * w + x)  # Escala coordenada x al tamaño del crop\n",
        "                    ky = int(kp[0] * h + y)  # Escala coordenada y al tamaño del crop\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)  # Dibuja círculo amarillo para keypoint\n",
        "\n",
        "        out.write(frame)  # Escribe el frame modificado al video de salida\n",
        "        frame_count += 1  # Incrementa contador de frames\n",
        "        if frame_count % 30 == 0:  # Cada 30 frames (aprox. 1 seg a 30fps)\n",
        "            print(f\"Procesados {frame_count} frames...\")  # Imprime progreso\n",
        "        if frame_count >= 300:  # Limita procesamiento a 300 frames para pruebas (quita para full video)\n",
        "            print(\"Limitado a 300 frames para prueba.\")  # Mensaje\n",
        "            break  # Sale del loop\n",
        "\n",
        "    cap.release()  # Libera el objeto de captura de video\n",
        "    out.release()  # Libera el objeto de escritura de video\n",
        "\n",
        "    # Cálculo de Métricas (opcional, con ground truth)\n",
        "    if ground_truth_roles:  # Si se proporciona ground truth\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)  # Calcula F1-score del último frame\n",
        "        print(f\"F1-Score: {f1:.4f}\")  # Imprime el score\n",
        "\n",
        "    print(f\"Video: {os.path.basename(video_path)} - Agresor: {agresor_count} frames, Víctima: {victima_count} frames\")  # Resumen por video\n",
        "    return output_path, agresor_count, victima_count  # Retorna path del output y contadores\n",
        "\n",
        "# Celda 6: Procesar Múltiples Videos\n",
        "videos_to_test = [\n",
        "    '/content/Robbery001.mp4',\n",
        "    '/content/Robbery002.mp4',\n",
        "    '/content/Robbery003.mp4'\n",
        "]\n",
        "\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]  # Ejemplo de ground truth (ajusta con anotaciones reales)\n",
        "\n",
        "report = []  # Lista para almacenar resultados\n",
        "for video in videos_to_test:  # Itera sobre videos\n",
        "    video_path = video  # Path absoluto ya definido\n",
        "    output_path = f'output_{os.path.basename(video_path).replace(\".mp4\", \"\")}.mp4'  # Nombre único para output\n",
        "    if os.path.exists(video_path):  # Verifica si el video existe\n",
        "        print(f\"\\nProcesando {video}...\")\n",
        "        result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)  # Llama procesamiento\n",
        "        if result:  # Si éxito\n",
        "            report.append((video, agresor_c, victima_c))  # Añade a reporte\n",
        "            files.download(result)  # Descarga el video procesado\n",
        "    else:  # Si no existe\n",
        "        print(f\"Video no encontrado: {video_path}. Verifica con !ls\")  # Mensaje de error\n",
        "\n",
        "# Celda 7: Resumen Final\n",
        "print(\"\\nReporte UCF-Crime:\")  # Título del reporte\n",
        "for video, agresor_c, victima_c in report:  # Itera sobre resultados\n",
        "    print(f\"{video}: Agresor en {agresor_c} frames, Víctima en {victima_c} frames\")  # Imprime resumen por video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT29O0_D6VqR",
        "outputId": "ab9bc1ce-ddcc-4b9c-8f9e-28b5248ee787"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando video de robo callejero (surveillance style)...\n",
            "\u001b[0;33mWARNING:\u001b[0m \"-f best\" selects the best pre-merged format which is often not the best option.\n",
            "         To let yt-dlp download and merge the best available formats, simply do not pass any format selection.\n",
            "         If you know what you are doing and want only the best pre-merged format, use \"-f b\" instead to suppress this warning\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=qowtDqraCmU\n",
            "[youtube] qowtDqraCmU: Downloading webpage\n",
            "[youtube] qowtDqraCmU: Downloading android sdkless player API JSON\n",
            "[youtube] qowtDqraCmU: Downloading tv client config\n",
            "[youtube] qowtDqraCmU: Downloading tv player API JSON\n",
            "[youtube] qowtDqraCmU: Downloading web safari player API JSON\n",
            "[youtube] qowtDqraCmU: Downloading player 7dc3db36-main\n",
            "\u001b[0;33mWARNING:\u001b[0m [youtube] Falling back to generic n function search\n",
            "         player = https://www.youtube.com/s/player/7dc3db36/player_ias.vflset/en_US/base.js\n",
            "\u001b[0;33mWARNING:\u001b[0m [youtube] qowtDqraCmU: nsig extraction failed: Some formats may be missing\n",
            "         n = Qf7Fph1hsnsuTnfPPb ; player = https://www.youtube.com/s/player/7dc3db36/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "\u001b[0;33mWARNING:\u001b[0m [youtube] qowtDqraCmU: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "\u001b[0;33mWARNING:\u001b[0m [youtube] qowtDqraCmU: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "[youtube] qowtDqraCmU: Downloading m3u8 information\n",
            "[info] qowtDqraCmU: Downloading 1 format(s): 95\n",
            "[download] Sleeping 6.00 seconds as required by the site...\n",
            "[hlsnative] Downloading m3u8 manifest\n",
            "[hlsnative] Total fragments: 10\n",
            "[download] Destination: /content/robbery.mp4\n",
            "\u001b[K[download] 100% of    6.70MiB in \u001b[1;37m00:00:03\u001b[0m at \u001b[0;32m2.06MiB/s\u001b[0m\n",
            "[FixupM3u8] Fixing MPEG-TS in MP4 container of \"/content/robbery.mp4\"\n",
            "Descargando video normal (control)...\n",
            "\u001b[0;33mWARNING:\u001b[0m \"-f best\" selects the best pre-merged format which is often not the best option.\n",
            "         To let yt-dlp download and merge the best available formats, simply do not pass any format selection.\n",
            "         If you know what you are doing and want only the best pre-merged format, use \"-f b\" instead to suppress this warning\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=3mJ3qJ7j8sY\n",
            "[youtube] 3mJ3qJ7j8sY: Downloading webpage\n",
            "[youtube] 3mJ3qJ7j8sY: Downloading android sdkless player API JSON\n",
            "[youtube] 3mJ3qJ7j8sY: Downloading tv client config\n",
            "[youtube] 3mJ3qJ7j8sY: Downloading tv player API JSON\n",
            "[youtube] 3mJ3qJ7j8sY: Downloading web safari player API JSON\n",
            "\u001b[0;31mERROR:\u001b[0m [youtube] 3mJ3qJ7j8sY: Video unavailable\n",
            "Videos descargados:\n",
            "-rw-r--r-- 1 root root  81M Nov  5 21:54 /content/normal.mp4\n",
            "-rw-r--r-- 1 root root 832K Nov  5 22:54 /content/output_fi1_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.2M Nov  5 22:54 /content/output_fi2_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.3M Nov  5 22:54 /content/output_fi3_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.3M Nov  5 22:54 /content/output_fi4_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.2M Nov  5 22:54 /content/output_fi5_xvid.mp4\n",
            "-rw-r--r-- 1 root root 837K Nov  5 22:54 /content/output_hockey_improved.mp4\n",
            "-rw-r--r-- 1 root root 837K Nov  5 22:53 /content/output_hockey.mp4\n",
            "-rw-r--r-- 1 root root 831K Nov  5 22:54 /content/output_hockey_persistent.mp4\n",
            "-rw-r--r-- 1 root root 824K Nov  5 22:55 /content/output_no1_xvid.mp4\n",
            "-rw-r--r-- 1 root root 4.2M Nov  5 22:04 /content/output_normal.mp4\n",
            "-rw-r--r-- 1 root root 3.9M Nov  5 22:31 /content/output_video.mp4\n",
            "-rw-r--r-- 1 root root 5.3M Nov  5 22:31 /content/people-detection.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery001.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery002.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery003.mp4\n",
            "-rw-r--r-- 1 root root 6.2M Nov  5 23:04 /content/robbery.mp4\n",
            "\n",
            "Procesando /content/Robbery001.mp4...\n",
            "Error: No se pudo abrir /content/Robbery001.mp4\n",
            "\n",
            "Procesando /content/Robbery002.mp4...\n",
            "Error: No se pudo abrir /content/Robbery002.mp4\n",
            "\n",
            "Procesando /content/Robbery003.mp4...\n",
            "Error: No se pudo abrir /content/Robbery003.mp4\n",
            "\n",
            "Reporte UCF-Crime:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias (ya hecho, salta si está instalado)\n",
        "# !pip install tensorflow tensorflow-hub opencv-python-headless ultralytics scikit-learn yt-dlp\n",
        "\n",
        "# Celda 2: Usar videos existentes (evitar redescarga)\n",
        "print(\"Usando videos existentes...\")\n",
        "print(\"Videos disponibles:\")\n",
        "!ls /content/*.mp4 -lh\n",
        "\n",
        "# Celda 3: Cargar Modelos\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Celda 4: Funciones Auxiliares\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]  # Mano izquierda, derecha\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:  # Umbral de confianza\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                for j, other_box in enumerate(person_boxes):\n",
        "                    if i != j:\n",
        "                        ox, oy, ow, oh = other_box\n",
        "                        center_other_x = ox + ow / 2\n",
        "                        center_other_y = oy + oh / 2\n",
        "                        dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                        dist_y = abs(hand_y * height - center_other_y)\n",
        "                        if dist_x < 150 and dist_y < 150:  # Umbral de distancia aumentado\n",
        "                            print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                            return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 2 and len(person_boxes) >= 2:  # Umbral de magnitud reducido\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No detectado (mag={mag_mean:.2f})\")\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 5: Procesamiento de Video\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir {video_path}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))\n",
        "\n",
        "    frame_count = 0\n",
        "    agresor_count = 0\n",
        "    victima_count = 0\n",
        "    prev_gray = None\n",
        "    last_roles = None\n",
        "    persistence_counter = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        r_height = 480\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        mag_mean = 0\n",
        "        if prev_gray is not None:\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "            mag_mean = np.mean(mag)\n",
        "\n",
        "        prev_gray = gray\n",
        "\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Solo personas\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, r_height, mag_mean)\n",
        "\n",
        "        if any(r != 'desconocido' for _, r in current_roles):\n",
        "            last_roles = current_roles\n",
        "            persistence_counter = persistence_frames\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "        elif persistence_counter > 0:\n",
        "            current_roles = last_roles\n",
        "            persistence_counter -= 1\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "            print(f\"Persistiendo por {persistence_counter} frames\")\n",
        "        else:\n",
        "            last_roles = None\n",
        "\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    print(f\"Video: {os.path.basename(video_path)} - Agresor: {agresor_count} frames, Víctima: {victima_count} frames\")\n",
        "    return output_path, agresor_count, victima_count  # Corrección del typo\n",
        "\n",
        "# Celda 6: Procesar Videos\n",
        "videos_to_test = [\n",
        "    '/content/robbery.mp4',\n",
        "    '/content/normal.mp4'\n",
        "]\n",
        "\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]\n",
        "\n",
        "report = []\n",
        "for video_path in videos_to_test:\n",
        "    output_path = f'output_{os.path.basename(video_path).replace(\".mp4\", \"\")}.mp4'\n",
        "    print(f\"\\nProcesando {video_path}...\")\n",
        "    result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)\n",
        "    if result:\n",
        "        report.append((video_path, agresor_c, victima_c))\n",
        "        files.download(result)\n",
        "\n",
        "print(\"\\nReporte UCF-Crime:\")\n",
        "for v, a, vi in report:\n",
        "    print(f\"{v}: Agresor en {a} frames, Víctima en {vi} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BF_27EWB6vKP",
        "outputId": "72c1f656-73ea-4772-fd68-2f9e8cbdebf6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando videos existentes...\n",
            "Videos disponibles:\n",
            "-rw-r--r-- 1 root root  81M Nov  5 21:54 /content/normal.mp4\n",
            "-rw-r--r-- 1 root root 832K Nov  5 22:54 /content/output_fi1_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.2M Nov  5 22:54 /content/output_fi2_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.3M Nov  5 22:54 /content/output_fi3_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.3M Nov  5 22:54 /content/output_fi4_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.2M Nov  5 22:54 /content/output_fi5_xvid.mp4\n",
            "-rw-r--r-- 1 root root 837K Nov  5 22:54 /content/output_hockey_improved.mp4\n",
            "-rw-r--r-- 1 root root 837K Nov  5 22:53 /content/output_hockey.mp4\n",
            "-rw-r--r-- 1 root root 831K Nov  5 22:54 /content/output_hockey_persistent.mp4\n",
            "-rw-r--r-- 1 root root 824K Nov  5 22:55 /content/output_no1_xvid.mp4\n",
            "-rw-r--r-- 1 root root 4.2M Nov  5 22:04 /content/output_normal.mp4\n",
            "-rw-r--r-- 1 root root 4.9M Nov  5 23:06 /content/output_robbery.mp4\n",
            "-rw-r--r-- 1 root root 3.9M Nov  5 22:31 /content/output_video.mp4\n",
            "-rw-r--r-- 1 root root 5.3M Nov  5 22:31 /content/people-detection.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery001.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery002.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery003.mp4\n",
            "-rw-r--r-- 1 root root 6.2M Nov  5 23:04 /content/robbery.mp4\n",
            "\n",
            "Procesando /content/robbery.mp4...\n",
            "\n",
            "0: 480x640 (no detections), 10.4ms\n",
            "Speed: 2.3ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.00)\n",
            "\n",
            "0: 480x640 (no detections), 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 (no detections), 9.1ms\n",
            "Speed: 2.2ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.14)\n",
            "\n",
            "0: 480x640 (no detections), 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.15)\n",
            "\n",
            "0: 480x640 (no detections), 7.0ms\n",
            "Speed: 2.2ms preprocess, 7.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.18)\n",
            "\n",
            "0: 480x640 (no detections), 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.06)\n",
            "\n",
            "0: 480x640 (no detections), 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.03)\n",
            "\n",
            "0: 480x640 (no detections), 9.1ms\n",
            "Speed: 1.9ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.07)\n",
            "\n",
            "0: 480x640 (no detections), 8.4ms\n",
            "Speed: 2.2ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.01)\n",
            "\n",
            "0: 480x640 (no detections), 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.07)\n",
            "\n",
            "0: 480x640 (no detections), 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.67)\n",
            "\n",
            "0: 480x640 (no detections), 7.2ms\n",
            "Speed: 2.2ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.77)\n",
            "\n",
            "0: 480x640 (no detections), 7.0ms\n",
            "Speed: 2.1ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.90)\n",
            "\n",
            "0: 480x640 (no detections), 7.1ms\n",
            "Speed: 2.2ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.45)\n",
            "\n",
            "0: 480x640 (no detections), 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.29)\n",
            "\n",
            "0: 480x640 (no detections), 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.56)\n",
            "\n",
            "0: 480x640 (no detections), 6.9ms\n",
            "Speed: 2.2ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.70)\n",
            "\n",
            "0: 480x640 (no detections), 14.5ms\n",
            "Speed: 2.8ms preprocess, 14.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.35)\n",
            "\n",
            "0: 480x640 (no detections), 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.66)\n",
            "\n",
            "0: 480x640 2 persons, 11.1ms\n",
            "Speed: 2.0ms preprocess, 11.1ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 cerca de víctima 1 (mag=0.90)\n",
            "\n",
            "0: 480x640 1 person, 10.7ms\n",
            "Speed: 1.9ms preprocess, 10.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.47)\n",
            "Persistiendo por 6 frames\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.97)\n",
            "Persistiendo por 5 frames\n",
            "\n",
            "0: 480x640 1 person, 13.2ms\n",
            "Speed: 2.0ms preprocess, 13.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.76)\n",
            "Persistiendo por 4 frames\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.0ms preprocess, 9.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.96)\n",
            "Persistiendo por 3 frames\n",
            "\n",
            "0: 480x640 1 person, 17.0ms\n",
            "Speed: 2.9ms preprocess, 17.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "Persistiendo por 2 frames\n",
            "\n",
            "0: 480x640 1 person, 14.0ms\n",
            "Speed: 2.8ms preprocess, 14.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "Persistiendo por 1 frames\n",
            "\n",
            "0: 480x640 1 person, 10.8ms\n",
            "Speed: 2.7ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "Persistiendo por 0 frames\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 1.9ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.3ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 12.4ms\n",
            "Speed: 1.9ms preprocess, 12.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.0ms\n",
            "Speed: 2.2ms preprocess, 7.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.29)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.29)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 1.9ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.25)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.0ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "Procesados 60 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 12.7ms\n",
            "Speed: 2.5ms preprocess, 12.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 1.9ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 1.9ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.0ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.05)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.04)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.0ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.2ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.05)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 1.9ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 90 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.1ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.3ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.3ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.28)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.35)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.42)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.2ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.49)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.47)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 1.9ms preprocess, 9.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.40)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 14.1ms\n",
            "Speed: 2.1ms preprocess, 14.1ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 11.4ms\n",
            "Speed: 1.9ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 12.8ms\n",
            "Speed: 2.3ms preprocess, 12.8ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 1.9ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.3ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 11.8ms\n",
            "Speed: 2.7ms preprocess, 11.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 10.9ms\n",
            "Speed: 1.9ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 17.9ms\n",
            "Speed: 3.1ms preprocess, 17.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 12.9ms\n",
            "Speed: 1.9ms preprocess, 12.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "Procesados 120 frames...\n",
            "\n",
            "0: 480x640 1 person, 13.4ms\n",
            "Speed: 2.2ms preprocess, 13.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 13.7ms\n",
            "Speed: 2.2ms preprocess, 13.7ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.1ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.8ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 1.9ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 1.6ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 8.7ms\n",
            "Speed: 2.1ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "Procesados 150 frames...\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.0ms\n",
            "Speed: 2.4ms preprocess, 7.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.0ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 1.7ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 11.1ms\n",
            "Speed: 2.2ms preprocess, 11.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.1ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 1.6ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 2.1ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.28)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.00)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.32)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.31)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 180 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 8.3ms\n",
            "Speed: 1.6ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.2ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.3ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.1ms\n",
            "Speed: 2.4ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.25)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 1.8ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 13.5ms\n",
            "Speed: 3.0ms preprocess, 13.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.28)\n",
            "\n",
            "0: 480x640 1 person, 14.4ms\n",
            "Speed: 2.8ms preprocess, 14.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 11.5ms\n",
            "Speed: 2.0ms preprocess, 11.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 1.9ms preprocess, 9.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "Procesados 210 frames...\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 2.1ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 11.5ms\n",
            "Speed: 2.7ms preprocess, 11.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 15.9ms\n",
            "Speed: 4.2ms preprocess, 15.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 15.4ms\n",
            "Speed: 2.2ms preprocess, 15.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 13.6ms\n",
            "Speed: 1.9ms preprocess, 13.6ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.1ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 10.3ms\n",
            "Speed: 2.1ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 11.7ms\n",
            "Speed: 1.5ms preprocess, 11.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.0ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 11.0ms\n",
            "Speed: 1.9ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.0ms preprocess, 8.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.1ms preprocess, 9.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.2ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 13.4ms\n",
            "Speed: 2.1ms preprocess, 13.4ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 240 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.3ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.0ms preprocess, 8.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.3ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 12.9ms\n",
            "Speed: 2.0ms preprocess, 12.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 1.9ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 3.0ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.2ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.2ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "Procesados 270 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 12.3ms\n",
            "Speed: 2.0ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.2ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.0ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.7ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 2.1ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.1ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 1.9ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.7ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.2ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.6ms preprocess, 10.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 13.2ms\n",
            "Speed: 2.9ms preprocess, 13.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 300 frames...\n",
            "F1-Score: 0.0000\n",
            "Video: robbery.mp4 - Agresor: 8 frames, Víctima: 8 frames\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_323aa73a-c06c-4a92-852e-278bb5cac55d\", \"output_robbery.mp4\", 5080066)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando /content/normal.mp4...\n",
            "\n",
            "0: 480x640 (no detections), 11.1ms\n",
            "Speed: 2.9ms preprocess, 11.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.00)\n",
            "\n",
            "0: 480x640 1 person, 10.5ms\n",
            "Speed: 3.6ms preprocess, 10.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.29)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 11.0ms\n",
            "Speed: 2.0ms preprocess, 11.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.66)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 11.4ms\n",
            "Speed: 2.0ms preprocess, 11.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.69)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 11.2ms\n",
            "Speed: 3.0ms preprocess, 11.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.19)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.3ms\n",
            "Speed: 2.0ms preprocess, 9.3ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.49)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.2ms\n",
            "Speed: 2.8ms preprocess, 10.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.67)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 15.7ms\n",
            "Speed: 1.9ms preprocess, 15.7ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.67)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.27)\n",
            "\n",
            "0: 480x640 1 person, 13.9ms\n",
            "Speed: 2.4ms preprocess, 13.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.92)\n",
            "\n",
            "0: 480x640 1 person, 14.3ms\n",
            "Speed: 3.4ms preprocess, 14.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.57)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 12.1ms\n",
            "Speed: 2.3ms preprocess, 12.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.36)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.31)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.26)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.3ms\n",
            "Speed: 2.1ms preprocess, 9.3ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.40)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.90)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.6ms\n",
            "Speed: 2.3ms preprocess, 8.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.92)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.24)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 8.0ms\n",
            "Speed: 2.2ms preprocess, 8.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.29)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.47)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 10.9ms\n",
            "Speed: 2.3ms preprocess, 10.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 cerca de víctima 1 (mag=2.34)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 cerca de víctima 1 (mag=2.45)\n",
            "\n",
            "0: 480x640 3 persons, 1 cell phone, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.07)\n",
            "\n",
            "0: 480x640 (no detections), 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.75)\n",
            "Persistiendo por 6 frames\n",
            "\n",
            "0: 480x640 (no detections), 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.87)\n",
            "Persistiendo por 5 frames\n",
            "\n",
            "0: 480x640 (no detections), 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.82)\n",
            "Persistiendo por 4 frames\n",
            "\n",
            "0: 480x640 (no detections), 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.37)\n",
            "Persistiendo por 3 frames\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.37)\n",
            "Persistiendo por 2 frames\n",
            "\n",
            "0: 480x640 1 vase, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.96)\n",
            "Persistiendo por 1 frames\n",
            "\n",
            "0: 480x640 1 vase, 10.7ms\n",
            "Speed: 1.5ms preprocess, 10.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "Persistiendo por 0 frames\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 vase, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.08)\n",
            "\n",
            "0: 480x640 1 vase, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.10)\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.21)\n",
            "\n",
            "0: 480x640 1 vase, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.51)\n",
            "\n",
            "0: 480x640 1 vase, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.91)\n",
            "\n",
            "0: 480x640 (no detections), 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.61)\n",
            "\n",
            "0: 480x640 1 vase, 9.7ms\n",
            "Speed: 2.0ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.81)\n",
            "\n",
            "0: 480x640 1 vase, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.39)\n",
            "\n",
            "0: 480x640 1 vase, 9.6ms\n",
            "Speed: 1.5ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.82)\n",
            "\n",
            "0: 480x640 1 vase, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.88)\n",
            "\n",
            "0: 480x640 1 vase, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.22)\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.47)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.24)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.3ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.29)\n",
            "\n",
            "0: 480x640 (no detections), 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.02)\n",
            "\n",
            "0: 480x640 1 vase, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.69)\n",
            "\n",
            "0: 480x640 1 vase, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.27)\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.13)\n",
            "\n",
            "0: 480x640 2 vases, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.01)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 13.5ms\n",
            "Speed: 1.9ms preprocess, 13.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=8.34)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.50)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.77)\n",
            "\n",
            "0: 480x640 (no detections), 10.8ms\n",
            "Speed: 2.1ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.92)\n",
            "\n",
            "0: 480x640 (no detections), 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.96)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 1.9ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.48)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.97)\n",
            "\n",
            "0: 480x640 2 persons, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.99)\n",
            "\n",
            "0: 480x640 2 persons, 16.1ms\n",
            "Speed: 1.8ms preprocess, 16.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.00)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=6.20)\n",
            "\n",
            "0: 480x640 (no detections), 10.7ms\n",
            "Speed: 2.1ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.67)\n",
            "Procesados 60 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.01)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.94)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.03)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.38)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.70)\n",
            "\n",
            "0: 480x640 1 person, 2 ties, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.09)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.4ms\n",
            "Speed: 2.3ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.19)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.0ms\n",
            "Speed: 2.1ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.65)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.52)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.2ms preprocess, 9.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.1ms\n",
            "Speed: 2.2ms preprocess, 8.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.81)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.1ms\n",
            "Speed: 2.1ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.89)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.35)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.2ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.66)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.28)\n",
            "\n",
            "0: 480x640 2 persons, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=3.06)\n",
            "\n",
            "0: 480x640 2 persons, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.40)\n",
            "Persistiendo por 6 frames\n",
            "\n",
            "0: 480x640 2 persons, 8.8ms\n",
            "Speed: 2.7ms preprocess, 8.8ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.23)\n",
            "Persistiendo por 5 frames\n",
            "\n",
            "0: 480x640 2 persons, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.98)\n",
            "Persistiendo por 4 frames\n",
            "\n",
            "0: 480x640 2 persons, 9.8ms\n",
            "Speed: 1.9ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.77)\n",
            "Persistiendo por 3 frames\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 1 cell phone, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.62)\n",
            "Persistiendo por 2 frames\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 11.0ms\n",
            "Speed: 2.7ms preprocess, 11.0ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.56)\n",
            "Persistiendo por 1 frames\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 8.3ms\n",
            "Speed: 2.0ms preprocess, 8.3ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.90)\n",
            "Persistiendo por 0 frames\n",
            "\n",
            "0: 480x640 3 persons, 1 tie, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 1 cell phone, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.41)\n",
            "\n",
            "0: 480x640 3 persons, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 2 cell phones, 8.0ms\n",
            "Speed: 2.2ms preprocess, 8.0ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.03)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.1ms\n",
            "Speed: 2.3ms preprocess, 9.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.46)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.7ms\n",
            "Speed: 2.8ms preprocess, 9.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.14)\n",
            "\n",
            "0: 480x640 2 persons, 12.9ms\n",
            "Speed: 4.0ms preprocess, 12.9ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.35)\n",
            "Procesados 90 frames...\n",
            "\n",
            "0: 480x640 1 person, 10.9ms\n",
            "Speed: 2.0ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.22)\n",
            "\n",
            "0: 480x640 1 person, 12.4ms\n",
            "Speed: 2.8ms preprocess, 12.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.26)\n",
            "\n",
            "0: 480x640 2 persons, 11.7ms\n",
            "Speed: 1.9ms preprocess, 11.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.89)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 13.0ms\n",
            "Speed: 2.7ms preprocess, 13.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.60)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.12)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.64)\n",
            "\n",
            "0: 480x640 3 persons, 1 tie, 10.1ms\n",
            "Speed: 2.0ms preprocess, 10.1ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.86)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 12.9ms\n",
            "Speed: 4.1ms preprocess, 12.9ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.00)\n",
            "\n",
            "0: 480x640 2 persons, 9.2ms\n",
            "Speed: 4.0ms preprocess, 9.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.25)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 15.6ms\n",
            "Speed: 2.1ms preprocess, 15.6ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.45)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 18.5ms\n",
            "Speed: 2.1ms preprocess, 18.5ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.31)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 10.3ms\n",
            "Speed: 1.9ms preprocess, 10.3ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.31)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=8.81)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.90)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.78)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.20)\n",
            "\n",
            "0: 480x640 2 persons, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.07)\n",
            "\n",
            "0: 480x640 2 persons, 10.5ms\n",
            "Speed: 2.7ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.25)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.56)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 10.3ms\n",
            "Speed: 2.1ms preprocess, 10.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.71)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 9.0ms\n",
            "Speed: 2.2ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.67)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.22)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.48)\n",
            "\n",
            "0: 480x640 1 person, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.79)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.9ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.87)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.60)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.18)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.1ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.33)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.1ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=6.82)\n",
            "Procesados 120 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.76)\n",
            "\n",
            "0: 480x640 1 person, 10.3ms\n",
            "Speed: 2.0ms preprocess, 10.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.30)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.87)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.83)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 1 cell phone, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.77)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 1 cell phone, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.39)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.53)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.74)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.69)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 17.5ms\n",
            "Speed: 2.7ms preprocess, 17.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.00)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 1 cell phone, 8.4ms\n",
            "Speed: 1.9ms preprocess, 8.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.48)\n",
            "\n",
            "0: 480x640 1 person, 2 ties, 9.6ms\n",
            "Speed: 1.9ms preprocess, 9.6ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=6.21)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.49)\n",
            "\n",
            "0: 480x640 1 person, 10.4ms\n",
            "Speed: 2.7ms preprocess, 10.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.74)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.88)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 1.9ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.89)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.96)\n",
            "\n",
            "0: 480x640 1 person, 13.7ms\n",
            "Speed: 3.4ms preprocess, 13.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.72)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.50)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.0ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.44)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.81)\n",
            "\n",
            "0: 480x640 1 person, 10.7ms\n",
            "Speed: 2.3ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.63)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.85)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.92)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.34)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.1ms preprocess, 9.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.43)\n",
            "\n",
            "0: 480x640 1 person, 13.4ms\n",
            "Speed: 1.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.43)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 10.5ms\n",
            "Speed: 2.1ms preprocess, 10.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.10)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.5ms\n",
            "Speed: 2.0ms preprocess, 8.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.63)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.82)\n",
            "Procesados 150 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.0ms\n",
            "Speed: 2.0ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.04)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 9.2ms\n",
            "Speed: 2.2ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.33)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.0ms\n",
            "Speed: 2.2ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.35)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.78)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.01)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 8.2ms\n",
            "Speed: 3.3ms preprocess, 8.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.41)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.80)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.4ms\n",
            "Speed: 4.6ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.78)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.39)\n",
            "\n",
            "0: 480x640 2 persons, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.89)\n",
            "\n",
            "0: 480x640 2 persons, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.42)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.37)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.53)\n",
            "\n",
            "0: 480x640 2 persons, 8.7ms\n",
            "Speed: 3.0ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.69)\n",
            "\n",
            "0: 480x640 2 persons, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.56)\n",
            "\n",
            "0: 480x640 2 persons, 9.0ms\n",
            "Speed: 2.0ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.20)\n",
            "\n",
            "0: 480x640 2 persons, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "\n",
            "0: 480x640 2 persons, 8.4ms\n",
            "Speed: 3.6ms preprocess, 8.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.97)\n",
            "\n",
            "0: 480x640 2 persons, 9.1ms\n",
            "Speed: 1.6ms preprocess, 9.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.74)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.91)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.23)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.7ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.24)\n",
            "\n",
            "0: 480x640 2 persons, 9.9ms\n",
            "Speed: 2.1ms preprocess, 9.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.20)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.07)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 9.2ms\n",
            "Speed: 1.9ms preprocess, 9.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.24)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 8.1ms\n",
            "Speed: 2.0ms preprocess, 8.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.36)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.68)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.9ms\n",
            "Speed: 4.9ms preprocess, 10.9ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.51)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 14.1ms\n",
            "Speed: 3.8ms preprocess, 14.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.13)\n",
            "Procesados 180 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.9ms\n",
            "Speed: 3.6ms preprocess, 9.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.00)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.0ms\n",
            "Speed: 2.2ms preprocess, 10.0ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.68)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.4ms\n",
            "Speed: 1.9ms preprocess, 10.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.97)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.33)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.70)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.1ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.23)\n",
            "\n",
            "0: 480x640 1 person, 15.3ms\n",
            "Speed: 2.8ms preprocess, 15.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.54)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 2.1ms preprocess, 10.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.59)\n",
            "\n",
            "0: 480x640 1 person, 14.5ms\n",
            "Speed: 2.0ms preprocess, 14.5ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.04)\n",
            "\n",
            "0: 480x640 1 person, 15.6ms\n",
            "Speed: 3.3ms preprocess, 15.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.34)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.2ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.66)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.95)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.22)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.27)\n",
            "\n",
            "0: 480x640 1 person, 10.3ms\n",
            "Speed: 2.3ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.46)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.07)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.70)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.56)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.66)\n",
            "\n",
            "0: 480x640 1 person, 10.4ms\n",
            "Speed: 2.3ms preprocess, 10.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.98)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.26)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.2ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.17)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.06)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.11)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.05)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.11)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.99)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.87)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.84)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.89)\n",
            "Procesados 210 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.77)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.3ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.53)\n",
            "\n",
            "0: 480x640 1 person, 1 handbag, 8.3ms\n",
            "Speed: 2.2ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.38)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.1ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.38)\n",
            "\n",
            "0: 480x640 1 person, 1 handbag, 8.0ms\n",
            "Speed: 2.3ms preprocess, 8.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.41)\n",
            "\n",
            "0: 480x640 1 person, 1 handbag, 9.9ms\n",
            "Speed: 2.8ms preprocess, 9.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.31)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.2ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.55)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.3ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.50)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=13.41)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.8ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.58)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.77)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.1ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.47)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.2ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.83)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.47)\n",
            "\n",
            "0: 480x640 2 persons, 8.7ms\n",
            "Speed: 2.3ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.75)\n",
            "\n",
            "0: 480x640 2 persons, 10.2ms\n",
            "Speed: 2.8ms preprocess, 10.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.92)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.02)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.1ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.06)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.22)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.1ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.90)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.53)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.40)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.76)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.68)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.3ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.75)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.58)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.23)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.42)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.52)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.89)\n",
            "Procesados 240 frames...\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.2ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.66)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.31)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 2.2ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.78)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.0ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.41)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.1ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.95)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.1ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.56)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.10)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.47)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.58)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.04)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.04)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.06)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.67)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.2ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.51)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 2.0ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.41)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.24)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.3ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.19)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.83)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.3ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.12)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.57)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.23)\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 13.2ms\n",
            "Speed: 2.3ms preprocess, 13.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.00)\n",
            "\n",
            "0: 480x640 2 persons, 1 bird, 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.25)\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 8.2ms\n",
            "Speed: 2.2ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.87)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.1ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.61)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.0ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.51)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.0ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.56)\n",
            "\n",
            "0: 480x640 1 person, 15.9ms\n",
            "Speed: 2.5ms preprocess, 15.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.57)\n",
            "\n",
            "0: 480x640 1 person, 13.4ms\n",
            "Speed: 2.2ms preprocess, 13.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.66)\n",
            "\n",
            "0: 480x640 1 person, 15.6ms\n",
            "Speed: 2.0ms preprocess, 15.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.75)\n",
            "Procesados 270 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 9.8ms\n",
            "Speed: 2.8ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.70)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.5ms\n",
            "Speed: 1.9ms preprocess, 10.5ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.52)\n",
            "\n",
            "0: 480x640 1 person, 14.6ms\n",
            "Speed: 3.4ms preprocess, 14.6ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.41)\n",
            "\n",
            "0: 480x640 1 person, 12.3ms\n",
            "Speed: 2.0ms preprocess, 12.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.51)\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 9.6ms\n",
            "Speed: 2.3ms preprocess, 9.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.63)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.0ms preprocess, 10.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.73)\n",
            "\n",
            "0: 480x640 1 person, 15.2ms\n",
            "Speed: 2.8ms preprocess, 15.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.64)\n",
            "\n",
            "0: 480x640 1 person, 13.7ms\n",
            "Speed: 3.8ms preprocess, 13.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.61)\n",
            "\n",
            "0: 480x640 1 person, 20.1ms\n",
            "Speed: 3.5ms preprocess, 20.1ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.54)\n",
            "\n",
            "0: 480x640 1 person, 15.3ms\n",
            "Speed: 4.3ms preprocess, 15.3ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.51)\n",
            "\n",
            "0: 480x640 1 person, 10.4ms\n",
            "Speed: 3.3ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.59)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=9.05)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.0ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.39)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.37)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.9ms\n",
            "Speed: 3.5ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.58)\n",
            "\n",
            "0: 480x640 1 person, 2 ties, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.59)\n",
            "\n",
            "0: 480x640 1 person, 10.6ms\n",
            "Speed: 2.1ms preprocess, 10.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.51)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.5ms\n",
            "Speed: 2.2ms preprocess, 8.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.85)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.3ms\n",
            "Speed: 1.9ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.01)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.2ms\n",
            "Speed: 2.0ms preprocess, 8.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.92)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.74)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 1.8ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.27)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.2ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.65)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.1ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.16)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 2.1ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.24)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.08)\n",
            "\n",
            "0: 480x640 1 person, 11.4ms\n",
            "Speed: 2.0ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.92)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.11)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.1ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.11)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.46)\n",
            "Procesados 300 frames...\n",
            "F1-Score: 0.0000\n",
            "Video: normal.mp4 - Agresor: 21 frames, Víctima: 21 frames\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1a0f48a2-6903-4c42-b4df-0da9da70c69d\", \"output_normal.mp4\", 4331356)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reporte UCF-Crime:\n",
            "/content/robbery.mp4: Agresor en 8 frames, Víctima en 8 frames\n",
            "/content/normal.mp4: Agresor en 21 frames, Víctima en 21 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalar dependencias (ya hecho)\n",
        "# (Salta si ya lo tienes)\n",
        "\n",
        "# Celda 2: Usar videos existentes (sin redescarga)\n",
        "print(\"Usando videos existentes...\")\n",
        "print(\"Videos disponibles:\")\n",
        "!ls /content/*.mp4 -lh\n",
        "\n",
        "# Celda 3: Cargar Modelos\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Celda 4: Funciones Auxiliares\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.int32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def detect_pose(frame):\n",
        "    input_image = preprocess_frame(frame)\n",
        "    outputs = movenet(input_image)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]  # Izq, der\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                for j, other_box in enumerate(person_boxes):\n",
        "                    if i != j:\n",
        "                        ox, oy, ow, oh = other_box\n",
        "                        center_other_x = ox + ow / 2\n",
        "                        center_other_y = oy + oh / 2\n",
        "                        dist_x = abs(hand_x * 192 - center_other_x)\n",
        "                        dist_y = abs(hand_y * height - center_other_y)\n",
        "                        if dist_x < 150 and dist_y < 150:\n",
        "                            print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                            return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "        dist_between = min(abs(person_boxes[0][0] - person_boxes[k][0]) for k in range(1, len(person_boxes)))\n",
        "        if dist_between < 150:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima más cercana (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "    print(f\"No detectado (mag={mag_mean:.2f})\")\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "def calculate_metrics(true_roles, pred_roles):\n",
        "    y_true = [r for _, r in true_roles]\n",
        "    y_pred = [r for _, r in pred_roles]\n",
        "    if y_true and y_pred and len(y_true) == len(y_pred):\n",
        "        return f1_score(y_true, y_pred, labels=['agresor', 'víctima', 'desconocido'], average='weighted')\n",
        "    return 0.0\n",
        "\n",
        "# Celda 5: Procesamiento de Video\n",
        "def process_video(video_path, output_path='output_ucf.mp4', ground_truth_roles=None, persistence_frames=7):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: No se pudo abrir {video_path}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))\n",
        "\n",
        "    frame_count = 0\n",
        "    agresor_count = 0\n",
        "    victima_count = 0\n",
        "    prev_gray = None\n",
        "    last_roles = None\n",
        "    persistence_counter = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        r_height = 480\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        mag_mean = 0\n",
        "        if prev_gray is not None:\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "            mag_mean = np.mean(mag)\n",
        "\n",
        "        prev_gray = gray\n",
        "\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Solo personas\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        keypoints_list = []\n",
        "        for box in person_boxes:\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            person_crop = frame[max(0, y):y+h, max(0, x):x+w]\n",
        "            if person_crop.size > 0 and w > 50 and h > 100:\n",
        "                keypoints = detect_pose(person_crop)\n",
        "                keypoints_list.append(keypoints)\n",
        "            else:\n",
        "                keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "        current_roles = analyze_roles(keypoints_list, person_boxes, r_height, mag_mean)\n",
        "\n",
        "        if any(r != 'desconocido' for _, r in current_roles):\n",
        "            last_roles = current_roles\n",
        "            persistence_counter = persistence_frames\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "        elif persistence_counter > 0:\n",
        "            current_roles = last_roles\n",
        "            persistence_counter -= 1\n",
        "            agresor_count += 1 if any(r == 'agresor' for _, r in current_roles) else 0\n",
        "            victima_count += 1 if any(r == 'víctima' for _, r in current_roles) else 0\n",
        "            print(f\"Persistiendo por {persistence_counter} frames\")\n",
        "        else:\n",
        "            last_roles = None\n",
        "\n",
        "        for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "            color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            for kp in keypoints:\n",
        "                if kp[2] > 0.3:\n",
        "                    kx = int(kp[1] * w + x)\n",
        "                    ky = int(kp[0] * h + y)\n",
        "                    cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "        if frame_count >= 300:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    if ground_truth_roles:\n",
        "        f1 = calculate_metrics(ground_truth_roles, current_roles)\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    print(f\"Video: {os.path.basename(video_path)} - Agresor: {agresor_count} frames, Víctima: {victima_count} frames\")\n",
        "    return output_path, agresor_count, victima_count\n",
        "\n",
        "# Celda 6: Procesar Videos\n",
        "videos_to_test = [\n",
        "    '/content/robbery.mp4',\n",
        "    '/content/normal.mp4'\n",
        "]\n",
        "\n",
        "ground_truth_roles = [(0, 'agresor'), (1, 'víctima')]\n",
        "\n",
        "report = []\n",
        "for video_path in videos_to_test:\n",
        "    output_path = f'output_{os.path.basename(video_path).replace(\".mp4\", \"\")}.mp4'\n",
        "    print(f\"\\nProcesando {video_path}...\")\n",
        "    result, agresor_c, victima_c = process_video(video_path, output_path, ground_truth_roles)\n",
        "    if result:\n",
        "        report.append((video_path, agresor_c, victima_c))\n",
        "        files.download(result)\n",
        "\n",
        "print(\"\\nReporte UCF-Crime:\")\n",
        "for v, a, vi in report:\n",
        "    print(f\"{v}: Agresor en {a} frames, Víctima en {vi} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8EY_E-cT7Rx3",
        "outputId": "e8ded6a1-f42a-41c9-f37e-09f692b3d9a6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando videos existentes...\n",
            "Videos disponibles:\n",
            "-rw-r--r-- 1 root root  81M Nov  5 21:54 /content/normal.mp4\n",
            "-rw-r--r-- 1 root root 832K Nov  5 22:54 /content/output_fi1_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.2M Nov  5 22:54 /content/output_fi2_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.3M Nov  5 22:54 /content/output_fi3_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.3M Nov  5 22:54 /content/output_fi4_xvid.mp4\n",
            "-rw-r--r-- 1 root root 1.2M Nov  5 22:54 /content/output_fi5_xvid.mp4\n",
            "-rw-r--r-- 1 root root 837K Nov  5 22:54 /content/output_hockey_improved.mp4\n",
            "-rw-r--r-- 1 root root 837K Nov  5 22:53 /content/output_hockey.mp4\n",
            "-rw-r--r-- 1 root root 831K Nov  5 22:54 /content/output_hockey_persistent.mp4\n",
            "-rw-r--r-- 1 root root 824K Nov  5 22:55 /content/output_no1_xvid.mp4\n",
            "-rw-r--r-- 1 root root 4.2M Nov  5 23:41 /content/output_normal.mp4\n",
            "-rw-r--r-- 1 root root 4.9M Nov  5 23:40 /content/output_robbery.mp4\n",
            "-rw-r--r-- 1 root root 3.9M Nov  5 22:31 /content/output_video.mp4\n",
            "-rw-r--r-- 1 root root 5.3M Nov  5 22:31 /content/people-detection.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery001.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery002.mp4\n",
            "-rw-r--r-- 1 root root    0 Nov  5 23:04 /content/Robbery003.mp4\n",
            "-rw-r--r-- 1 root root 6.2M Nov  5 23:04 /content/robbery.mp4\n",
            "\n",
            "Procesando /content/robbery.mp4...\n",
            "\n",
            "0: 480x640 (no detections), 9.9ms\n",
            "Speed: 2.2ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.00)\n",
            "\n",
            "0: 480x640 (no detections), 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 (no detections), 7.1ms\n",
            "Speed: 1.9ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.14)\n",
            "\n",
            "0: 480x640 (no detections), 7.0ms\n",
            "Speed: 2.3ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.15)\n",
            "\n",
            "0: 480x640 (no detections), 7.0ms\n",
            "Speed: 2.1ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.18)\n",
            "\n",
            "0: 480x640 (no detections), 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.06)\n",
            "\n",
            "0: 480x640 (no detections), 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.03)\n",
            "\n",
            "0: 480x640 (no detections), 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.07)\n",
            "\n",
            "0: 480x640 (no detections), 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.01)\n",
            "\n",
            "0: 480x640 (no detections), 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.07)\n",
            "\n",
            "0: 480x640 (no detections), 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.67)\n",
            "\n",
            "0: 480x640 (no detections), 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.77)\n",
            "\n",
            "0: 480x640 (no detections), 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.90)\n",
            "\n",
            "0: 480x640 (no detections), 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.45)\n",
            "\n",
            "0: 480x640 (no detections), 8.1ms\n",
            "Speed: 1.5ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.29)\n",
            "\n",
            "0: 480x640 (no detections), 7.3ms\n",
            "Speed: 2.0ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.56)\n",
            "\n",
            "0: 480x640 (no detections), 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.70)\n",
            "\n",
            "0: 480x640 (no detections), 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.35)\n",
            "\n",
            "0: 480x640 (no detections), 7.2ms\n",
            "Speed: 2.2ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.66)\n",
            "\n",
            "0: 480x640 2 persons, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 cerca de víctima 1 (mag=0.90)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 1.8ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.47)\n",
            "Persistiendo por 6 frames\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.97)\n",
            "Persistiendo por 5 frames\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.76)\n",
            "Persistiendo por 4 frames\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.96)\n",
            "Persistiendo por 3 frames\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.0ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "Persistiendo por 2 frames\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "Persistiendo por 1 frames\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.3ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "Persistiendo por 0 frames\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.0ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.0ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.1ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 14.5ms\n",
            "Speed: 2.1ms preprocess, 14.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.29)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.29)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.25)\n",
            "\n",
            "0: 480x640 1 person, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.0ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "Procesados 60 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 14.6ms\n",
            "Speed: 2.2ms preprocess, 14.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 10.5ms\n",
            "Speed: 1.9ms preprocess, 10.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 3.3ms preprocess, 10.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 11.7ms\n",
            "Speed: 2.0ms preprocess, 11.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 12.4ms\n",
            "Speed: 1.9ms preprocess, 12.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 10.4ms\n",
            "Speed: 1.8ms preprocess, 10.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 1.9ms preprocess, 8.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.05)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.0ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.04)\n",
            "\n",
            "0: 480x640 1 person, 14.6ms\n",
            "Speed: 2.7ms preprocess, 14.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 11.1ms\n",
            "Speed: 1.9ms preprocess, 11.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 10.8ms\n",
            "Speed: 2.0ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 12.9ms\n",
            "Speed: 2.9ms preprocess, 12.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.05)\n",
            "\n",
            "0: 480x640 1 person, 10.7ms\n",
            "Speed: 2.0ms preprocess, 10.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.8ms preprocess, 9.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.1ms\n",
            "Speed: 1.9ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 90 frames...\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.2ms preprocess, 8.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.1ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.8ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.8ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.28)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.35)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.42)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.49)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.0ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.47)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.40)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.2ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 14.2ms\n",
            "Speed: 2.0ms preprocess, 14.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 1.9ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "Procesados 120 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.2ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.0ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 3.0ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.2ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 3.0ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 10.0ms\n",
            "Speed: 2.0ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 9.4ms\n",
            "Speed: 2.0ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "Procesados 150 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 2.0ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.1ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 14.1ms\n",
            "Speed: 2.0ms preprocess, 14.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 12.2ms\n",
            "Speed: 2.1ms preprocess, 12.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 13.4ms\n",
            "Speed: 2.6ms preprocess, 13.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 12.8ms\n",
            "Speed: 1.9ms preprocess, 12.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 12.4ms\n",
            "Speed: 1.9ms preprocess, 12.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 12.1ms\n",
            "Speed: 2.2ms preprocess, 12.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.28)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 1.9ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 11.8ms\n",
            "Speed: 2.4ms preprocess, 11.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.00)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 1.8ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.32)\n",
            "\n",
            "0: 480x640 1 person, 13.8ms\n",
            "Speed: 1.8ms preprocess, 13.8ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.31)\n",
            "\n",
            "0: 480x640 1 person, 13.0ms\n",
            "Speed: 1.9ms preprocess, 13.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 13.8ms\n",
            "Speed: 3.0ms preprocess, 13.8ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 14.0ms\n",
            "Speed: 1.9ms preprocess, 14.0ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 15.6ms\n",
            "Speed: 3.3ms preprocess, 15.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 180 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 8.1ms\n",
            "Speed: 2.2ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.8ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 11.6ms\n",
            "Speed: 2.1ms preprocess, 11.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.2ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.07)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.0ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 1 car, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 1.8ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.25)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 1.9ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 2.1ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.28)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 3.1ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.30)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.3ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "Procesados 210 frames...\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.24)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.1ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.0ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.14)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.0ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 3.4ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.2ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.12)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.2ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.0ms preprocess, 10.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.7ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.0ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 240 frames...\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.1ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 1.9ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.1ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.7ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.0ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.2ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 1.9ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 10.7ms\n",
            "Speed: 2.1ms preprocess, 10.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.1ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 17.1ms\n",
            "Speed: 3.4ms preprocess, 17.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 17.2ms\n",
            "Speed: 2.8ms preprocess, 17.2ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 10.9ms\n",
            "Speed: 2.2ms preprocess, 10.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 3.7ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 12.0ms\n",
            "Speed: 2.2ms preprocess, 12.0ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 11.9ms\n",
            "Speed: 3.0ms preprocess, 11.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 1.9ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 12.7ms\n",
            "Speed: 2.0ms preprocess, 12.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.11)\n",
            "\n",
            "0: 480x640 1 person, 12.7ms\n",
            "Speed: 2.1ms preprocess, 12.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "Procesados 270 frames...\n",
            "\n",
            "0: 480x640 1 person, 15.2ms\n",
            "Speed: 3.6ms preprocess, 15.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 12.1ms\n",
            "Speed: 2.3ms preprocess, 12.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 13.5ms\n",
            "Speed: 3.0ms preprocess, 13.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 10.8ms\n",
            "Speed: 2.2ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 10.3ms\n",
            "Speed: 1.9ms preprocess, 10.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 1.9ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.10)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 14.4ms\n",
            "Speed: 2.0ms preprocess, 14.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.06)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.0ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.08)\n",
            "\n",
            "0: 480x640 1 person, 8.5ms\n",
            "Speed: 2.1ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.09)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.13)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.0ms preprocess, 8.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.20)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.27)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.26)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.2ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.23)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.17)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.15)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.16)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.18)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 1.9ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.19)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.21)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.22)\n",
            "Procesados 300 frames...\n",
            "F1-Score: 0.0000\n",
            "Video: robbery.mp4 - Agresor: 8 frames, Víctima: 8 frames\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6ddbd92f-2045-41fe-a19f-0f734f32fd52\", \"output_robbery.mp4\", 5080066)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando /content/normal.mp4...\n",
            "\n",
            "0: 480x640 (no detections), 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.00)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 3.1ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.29)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.66)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.69)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.19)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.49)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.67)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.5ms\n",
            "Speed: 1.9ms preprocess, 9.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.67)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.27)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.0ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.92)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.57)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.36)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.31)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.26)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.40)\n",
            "\n",
            "0: 480x640 1 person, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.90)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.92)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 9.0ms\n",
            "Speed: 2.1ms preprocess, 9.0ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.24)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 8.1ms\n",
            "Speed: 2.0ms preprocess, 8.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.29)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.47)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 cerca de víctima 1 (mag=2.34)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Detectado agresor 0 cerca de víctima 1 (mag=2.45)\n",
            "\n",
            "0: 480x640 3 persons, 1 cell phone, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=2.07)\n",
            "\n",
            "0: 480x640 (no detections), 9.7ms\n",
            "Speed: 2.2ms preprocess, 9.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.75)\n",
            "Persistiendo por 6 frames\n",
            "\n",
            "0: 480x640 (no detections), 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.87)\n",
            "Persistiendo por 5 frames\n",
            "\n",
            "0: 480x640 (no detections), 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.82)\n",
            "Persistiendo por 4 frames\n",
            "\n",
            "0: 480x640 (no detections), 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.37)\n",
            "Persistiendo por 3 frames\n",
            "\n",
            "0: 480x640 1 vase, 8.5ms\n",
            "Speed: 2.3ms preprocess, 8.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.37)\n",
            "Persistiendo por 2 frames\n",
            "\n",
            "0: 480x640 1 vase, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.96)\n",
            "Persistiendo por 1 frames\n",
            "\n",
            "0: 480x640 1 vase, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "Persistiendo por 0 frames\n",
            "Procesados 30 frames...\n",
            "\n",
            "0: 480x640 1 vase, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.08)\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.10)\n",
            "\n",
            "0: 480x640 1 vase, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.21)\n",
            "\n",
            "0: 480x640 1 vase, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.51)\n",
            "\n",
            "0: 480x640 1 vase, 7.5ms\n",
            "Speed: 2.0ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.91)\n",
            "\n",
            "0: 480x640 (no detections), 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.61)\n",
            "\n",
            "0: 480x640 1 vase, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.81)\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.39)\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.82)\n",
            "\n",
            "0: 480x640 1 vase, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.88)\n",
            "\n",
            "0: 480x640 1 vase, 7.7ms\n",
            "Speed: 2.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.22)\n",
            "\n",
            "0: 480x640 1 vase, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.47)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.0ms preprocess, 9.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.24)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.29)\n",
            "\n",
            "0: 480x640 (no detections), 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.02)\n",
            "\n",
            "0: 480x640 1 vase, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.69)\n",
            "\n",
            "0: 480x640 1 vase, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.27)\n",
            "\n",
            "0: 480x640 1 vase, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.13)\n",
            "\n",
            "0: 480x640 2 vases, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.01)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.2ms\n",
            "Speed: 2.1ms preprocess, 8.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=8.34)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.50)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.77)\n",
            "\n",
            "0: 480x640 (no detections), 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.92)\n",
            "\n",
            "0: 480x640 (no detections), 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.96)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.48)\n",
            "\n",
            "0: 480x640 1 person, 15.5ms\n",
            "Speed: 1.9ms preprocess, 15.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.97)\n",
            "\n",
            "0: 480x640 2 persons, 12.0ms\n",
            "Speed: 2.8ms preprocess, 12.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.99)\n",
            "\n",
            "0: 480x640 2 persons, 11.2ms\n",
            "Speed: 3.4ms preprocess, 11.2ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.00)\n",
            "\n",
            "0: 480x640 1 person, 10.4ms\n",
            "Speed: 1.9ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=6.20)\n",
            "\n",
            "0: 480x640 (no detections), 13.0ms\n",
            "Speed: 2.4ms preprocess, 13.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.67)\n",
            "Procesados 60 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.01)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.9ms\n",
            "Speed: 1.9ms preprocess, 10.9ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.94)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 11.2ms\n",
            "Speed: 2.1ms preprocess, 11.2ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.03)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 12.9ms\n",
            "Speed: 2.0ms preprocess, 12.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.38)\n",
            "\n",
            "0: 480x640 1 person, 14.7ms\n",
            "Speed: 2.0ms preprocess, 14.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.70)\n",
            "\n",
            "0: 480x640 1 person, 2 ties, 15.8ms\n",
            "Speed: 2.0ms preprocess, 15.8ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.09)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 13.3ms\n",
            "Speed: 3.3ms preprocess, 13.3ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.19)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.65)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.6ms\n",
            "Speed: 2.0ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.52)\n",
            "\n",
            "0: 480x640 1 person, 12.2ms\n",
            "Speed: 2.1ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.8ms\n",
            "Speed: 2.0ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.81)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.89)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.35)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.3ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.66)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.2ms preprocess, 8.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.28)\n",
            "\n",
            "0: 480x640 2 persons, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Movimiento alto: asignando agresor 0, víctima más cercana (mag=3.06)\n",
            "\n",
            "0: 480x640 2 persons, 9.0ms\n",
            "Speed: 2.2ms preprocess, 9.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.40)\n",
            "Persistiendo por 6 frames\n",
            "\n",
            "0: 480x640 2 persons, 13.7ms\n",
            "Speed: 2.1ms preprocess, 13.7ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.23)\n",
            "Persistiendo por 5 frames\n",
            "\n",
            "0: 480x640 2 persons, 9.5ms\n",
            "Speed: 2.1ms preprocess, 9.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.98)\n",
            "Persistiendo por 4 frames\n",
            "\n",
            "0: 480x640 2 persons, 8.7ms\n",
            "Speed: 3.1ms preprocess, 8.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.77)\n",
            "Persistiendo por 3 frames\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 1 cell phone, 10.0ms\n",
            "Speed: 2.2ms preprocess, 10.0ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.62)\n",
            "Persistiendo por 2 frames\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.56)\n",
            "Persistiendo por 1 frames\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 10.2ms\n",
            "Speed: 2.0ms preprocess, 10.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.90)\n",
            "Persistiendo por 0 frames\n",
            "\n",
            "0: 480x640 3 persons, 1 tie, 7.7ms\n",
            "Speed: 2.0ms preprocess, 7.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 4 persons, 1 tie, 1 cell phone, 10.1ms\n",
            "Speed: 1.5ms preprocess, 10.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.41)\n",
            "\n",
            "0: 480x640 3 persons, 8.0ms\n",
            "Speed: 2.3ms preprocess, 8.0ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 2 cell phones, 9.1ms\n",
            "Speed: 2.0ms preprocess, 9.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.03)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.46)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.14)\n",
            "\n",
            "0: 480x640 2 persons, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.35)\n",
            "Procesados 90 frames...\n",
            "\n",
            "0: 480x640 1 person, 10.4ms\n",
            "Speed: 2.3ms preprocess, 10.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.22)\n",
            "\n",
            "0: 480x640 1 person, 16.6ms\n",
            "Speed: 2.4ms preprocess, 16.6ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.26)\n",
            "\n",
            "0: 480x640 2 persons, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.89)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.60)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.1ms\n",
            "Speed: 2.0ms preprocess, 9.1ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.12)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.64)\n",
            "\n",
            "0: 480x640 3 persons, 1 tie, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.86)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.00)\n",
            "\n",
            "0: 480x640 2 persons, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.25)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.45)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.31)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.31)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.2ms\n",
            "Speed: 2.3ms preprocess, 10.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=8.81)\n",
            "\n",
            "0: 480x640 1 person, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.90)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.6ms\n",
            "Speed: 1.9ms preprocess, 9.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.78)\n",
            "\n",
            "0: 480x640 1 person, 8.8ms\n",
            "Speed: 2.2ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.20)\n",
            "\n",
            "0: 480x640 2 persons, 8.2ms\n",
            "Speed: 2.1ms preprocess, 8.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.07)\n",
            "\n",
            "0: 480x640 2 persons, 11.0ms\n",
            "Speed: 2.0ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.25)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.56)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.71)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 8.4ms\n",
            "Speed: 3.6ms preprocess, 8.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.67)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.22)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.48)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 3.4ms preprocess, 8.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.79)\n",
            "\n",
            "0: 480x640 1 person, 10.5ms\n",
            "Speed: 2.0ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.87)\n",
            "\n",
            "0: 480x640 1 person, 11.6ms\n",
            "Speed: 1.9ms preprocess, 11.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.60)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.18)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.1ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.33)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=6.82)\n",
            "Procesados 120 frames...\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.76)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.30)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.87)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 17.4ms\n",
            "Speed: 2.1ms preprocess, 17.4ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.83)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 1 cell phone, 9.3ms\n",
            "Speed: 2.1ms preprocess, 9.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.77)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 1 cell phone, 8.6ms\n",
            "Speed: 2.0ms preprocess, 8.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.39)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.53)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.74)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.69)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.00)\n",
            "\n",
            "0: 480x640 1 person, 1 remote, 1 cell phone, 9.0ms\n",
            "Speed: 2.0ms preprocess, 9.0ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.48)\n",
            "\n",
            "0: 480x640 1 person, 2 ties, 13.3ms\n",
            "Speed: 2.3ms preprocess, 13.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=6.21)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 3.0ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.49)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 2.1ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.74)\n",
            "\n",
            "0: 480x640 1 person, 11.1ms\n",
            "Speed: 2.4ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.88)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.89)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.2ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.96)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 1.9ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.72)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.50)\n",
            "\n",
            "0: 480x640 1 person, 12.1ms\n",
            "Speed: 2.1ms preprocess, 12.1ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.44)\n",
            "\n",
            "0: 480x640 1 person, 11.4ms\n",
            "Speed: 1.6ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.81)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.63)\n",
            "\n",
            "0: 480x640 1 person, 13.3ms\n",
            "Speed: 2.9ms preprocess, 13.3ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.85)\n",
            "\n",
            "0: 480x640 1 person, 10.8ms\n",
            "Speed: 1.9ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.92)\n",
            "\n",
            "0: 480x640 1 person, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.34)\n",
            "\n",
            "0: 480x640 1 person, 12.5ms\n",
            "Speed: 1.9ms preprocess, 12.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.43)\n",
            "\n",
            "0: 480x640 1 person, 12.2ms\n",
            "Speed: 2.4ms preprocess, 12.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.43)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 12.8ms\n",
            "Speed: 2.6ms preprocess, 12.8ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.10)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 11.9ms\n",
            "Speed: 2.6ms preprocess, 11.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.63)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.8ms\n",
            "Speed: 2.7ms preprocess, 10.8ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.82)\n",
            "Procesados 150 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 13.1ms\n",
            "Speed: 2.2ms preprocess, 13.1ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.04)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 15.5ms\n",
            "Speed: 1.9ms preprocess, 15.5ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.33)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 16.4ms\n",
            "Speed: 2.6ms preprocess, 16.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.35)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 21.9ms\n",
            "Speed: 4.3ms preprocess, 21.9ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.78)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.2ms\n",
            "Speed: 3.0ms preprocess, 10.2ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.01)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 11.5ms\n",
            "Speed: 2.2ms preprocess, 11.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.41)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.80)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.78)\n",
            "\n",
            "0: 480x640 2 persons, 1 cell phone, 9.6ms\n",
            "Speed: 2.0ms preprocess, 9.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.39)\n",
            "\n",
            "0: 480x640 2 persons, 8.5ms\n",
            "Speed: 2.0ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.89)\n",
            "\n",
            "0: 480x640 2 persons, 8.1ms\n",
            "Speed: 1.9ms preprocess, 8.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.42)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.37)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.53)\n",
            "\n",
            "0: 480x640 2 persons, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.69)\n",
            "\n",
            "0: 480x640 2 persons, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.56)\n",
            "\n",
            "0: 480x640 2 persons, 12.5ms\n",
            "Speed: 2.3ms preprocess, 12.5ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.20)\n",
            "\n",
            "0: 480x640 2 persons, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.19)\n",
            "\n",
            "0: 480x640 2 persons, 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.97)\n",
            "\n",
            "0: 480x640 2 persons, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.74)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.91)\n",
            "\n",
            "0: 480x640 2 persons, 1 tie, 12.9ms\n",
            "Speed: 2.3ms preprocess, 12.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.23)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.16)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.2ms preprocess, 9.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.24)\n",
            "\n",
            "0: 480x640 2 persons, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.20)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.07)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.24)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 1 cell phone, 10.0ms\n",
            "Speed: 2.2ms preprocess, 10.0ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.36)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.2ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.68)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 10.9ms\n",
            "Speed: 2.1ms preprocess, 10.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.51)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.13)\n",
            "Procesados 180 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.00)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.68)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.97)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.33)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 9.2ms\n",
            "Speed: 2.8ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.70)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.23)\n",
            "\n",
            "0: 480x640 1 person, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.54)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.59)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.3ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.04)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.2ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.34)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.66)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.7ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.95)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 1.9ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.22)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.27)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.46)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.07)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 1.9ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.70)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.56)\n",
            "\n",
            "0: 480x640 1 person, 10.8ms\n",
            "Speed: 2.2ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.66)\n",
            "\n",
            "0: 480x640 1 person, 8.6ms\n",
            "Speed: 2.0ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.98)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.2ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.26)\n",
            "\n",
            "0: 480x640 1 person, 7.3ms\n",
            "Speed: 3.4ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.17)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.06)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.11)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.9ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.05)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.11)\n",
            "\n",
            "0: 480x640 1 person, 8.9ms\n",
            "Speed: 2.2ms preprocess, 8.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.99)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.87)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 1.9ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.84)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.89)\n",
            "Procesados 210 frames...\n",
            "\n",
            "0: 480x640 1 person, 10.3ms\n",
            "Speed: 2.9ms preprocess, 10.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.77)\n",
            "\n",
            "0: 480x640 1 person, 8.4ms\n",
            "Speed: 2.3ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.53)\n",
            "\n",
            "0: 480x640 1 person, 1 handbag, 9.8ms\n",
            "Speed: 2.3ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.38)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.38)\n",
            "\n",
            "0: 480x640 1 person, 1 handbag, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.41)\n",
            "\n",
            "0: 480x640 1 person, 1 handbag, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.31)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.3ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.55)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.50)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=13.41)\n",
            "\n",
            "0: 480x640 1 person, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.58)\n",
            "\n",
            "0: 480x640 1 person, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.77)\n",
            "\n",
            "0: 480x640 1 person, 10.7ms\n",
            "Speed: 2.0ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.47)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.6ms preprocess, 10.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.83)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 1.9ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.47)\n",
            "\n",
            "0: 480x640 2 persons, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.75)\n",
            "\n",
            "0: 480x640 2 persons, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.92)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.02)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.2ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.06)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.22)\n",
            "\n",
            "0: 480x640 1 person, 12.6ms\n",
            "Speed: 3.2ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.90)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 1.9ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.53)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.0ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.40)\n",
            "\n",
            "0: 480x640 1 person, 14.2ms\n",
            "Speed: 1.9ms preprocess, 14.2ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.76)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.0ms preprocess, 10.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.68)\n",
            "\n",
            "0: 480x640 1 person, 13.3ms\n",
            "Speed: 2.8ms preprocess, 13.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.75)\n",
            "\n",
            "0: 480x640 1 person, 15.2ms\n",
            "Speed: 3.1ms preprocess, 15.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.58)\n",
            "\n",
            "0: 480x640 1 person, 10.9ms\n",
            "Speed: 2.3ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.23)\n",
            "\n",
            "0: 480x640 1 person, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.42)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 2.0ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.52)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.89)\n",
            "Procesados 240 frames...\n",
            "\n",
            "0: 480x640 1 person, 13.9ms\n",
            "Speed: 2.2ms preprocess, 13.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.66)\n",
            "\n",
            "0: 480x640 1 person, 13.8ms\n",
            "Speed: 2.0ms preprocess, 13.8ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.31)\n",
            "\n",
            "0: 480x640 1 person, 15.6ms\n",
            "Speed: 2.0ms preprocess, 15.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.78)\n",
            "\n",
            "0: 480x640 1 person, 17.2ms\n",
            "Speed: 2.4ms preprocess, 17.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.41)\n",
            "\n",
            "0: 480x640 1 person, 12.2ms\n",
            "Speed: 4.0ms preprocess, 12.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.95)\n",
            "\n",
            "0: 480x640 1 person, 10.3ms\n",
            "Speed: 2.7ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.56)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.10)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.47)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.1ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.58)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.1ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.04)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.9ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.04)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.06)\n",
            "\n",
            "0: 480x640 1 person, 10.6ms\n",
            "Speed: 2.1ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.67)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.2ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.51)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.1ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.41)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.24)\n",
            "\n",
            "0: 480x640 1 person, 8.7ms\n",
            "Speed: 2.8ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.19)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.1ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.83)\n",
            "\n",
            "0: 480x640 1 person, 9.0ms\n",
            "Speed: 2.3ms preprocess, 9.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.12)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 1.9ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.57)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=7.23)\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.00)\n",
            "\n",
            "0: 480x640 2 persons, 1 bird, 10.0ms\n",
            "Speed: 2.1ms preprocess, 10.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=1.25)\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 8.1ms\n",
            "Speed: 1.9ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.87)\n",
            "\n",
            "0: 480x640 1 person, 9.4ms\n",
            "Speed: 2.1ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.61)\n",
            "\n",
            "0: 480x640 1 person, 8.0ms\n",
            "Speed: 2.1ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.51)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.56)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.57)\n",
            "\n",
            "0: 480x640 1 person, 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.66)\n",
            "\n",
            "0: 480x640 1 person, 8.2ms\n",
            "Speed: 2.1ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.75)\n",
            "Procesados 270 frames...\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.70)\n",
            "\n",
            "0: 480x640 1 person, 1 cell phone, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.52)\n",
            "\n",
            "0: 480x640 1 person, 11.1ms\n",
            "Speed: 2.8ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.41)\n",
            "\n",
            "0: 480x640 1 person, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.51)\n",
            "\n",
            "0: 480x640 1 person, 1 bird, 9.8ms\n",
            "Speed: 2.2ms preprocess, 9.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.63)\n",
            "\n",
            "0: 480x640 1 person, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.73)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.1ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.64)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.61)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.54)\n",
            "\n",
            "0: 480x640 1 person, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.51)\n",
            "\n",
            "0: 480x640 1 person, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=0.59)\n",
            "\n",
            "0: 480x640 1 person, 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=9.05)\n",
            "\n",
            "0: 480x640 1 person, 11.2ms\n",
            "Speed: 2.0ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.39)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.37)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 12.0ms\n",
            "Speed: 2.1ms preprocess, 12.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.58)\n",
            "\n",
            "0: 480x640 1 person, 2 ties, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=2.59)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.51)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.2ms\n",
            "Speed: 2.1ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.85)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 12.7ms\n",
            "Speed: 2.4ms preprocess, 12.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.01)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.92)\n",
            "\n",
            "0: 480x640 1 person, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.74)\n",
            "\n",
            "0: 480x640 1 person, 8.3ms\n",
            "Speed: 2.2ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.27)\n",
            "\n",
            "0: 480x640 1 person, 10.1ms\n",
            "Speed: 2.0ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.65)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.0ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.16)\n",
            "\n",
            "0: 480x640 1 person, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.24)\n",
            "\n",
            "0: 480x640 1 person, 1 tie, 8.1ms\n",
            "Speed: 2.4ms preprocess, 8.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.08)\n",
            "\n",
            "0: 480x640 1 person, 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.92)\n",
            "\n",
            "0: 480x640 1 person, 9.3ms\n",
            "Speed: 2.2ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=5.11)\n",
            "\n",
            "0: 480x640 1 person, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=4.11)\n",
            "\n",
            "0: 480x640 1 person, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No detectado (mag=3.46)\n",
            "Procesados 300 frames...\n",
            "F1-Score: 0.0000\n",
            "Video: normal.mp4 - Agresor: 21 frames, Víctima: 21 frames\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_adaa650e-eccd-4268-bdcb-f1682bd64355\", \"output_normal.mp4\", 4331356)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reporte UCF-Crime:\n",
            "/content/robbery.mp4: Agresor en 8 frames, Víctima en 8 frames\n",
            "/content/normal.mp4: Agresor en 21 frames, Víctima en 21 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### App en Gradio para Identificación y Tracking de Víctima/Victimario en Videos de YouTube\n",
        "\n",
        "¡Claro! Te preparé una app completa en Gradio que hace exactamente lo que pides: subes una URL de YouTube, descarga el video, procesa con YOLO (detección de personas), MoveNet (poses), heurística para roles (agresor/víctima basada en movimiento y proximidad de manos), y luego trackea al agresor (victimario) usando SORT (un tracker simple). El video de salida muestra boxes, keypoints, etiquetas de roles y IDs de tracking (el agresor se resalta en rojo).\n",
        "\n",
        "**Requisitos**:\n",
        "- Ejecuta en Google Colab (instala dependencias con `!pip install gradio yt-dlp ultralytics tensorflow opencv-python sort-tracker`).\n",
        "- La app es local en Colab (usa `demo.launch(share=True)` para enlace público).\n",
        "- Heurística: Agresor = persona con mano alta y cerca de otra + movimiento alto; Víctima = persona cercana al agresor. Persistencia de 5 frames para estabilidad.\n",
        "- Tracking: Sigue al agresor con ID único, resaltado.\n",
        "\n",
        "**Código Completo** (copia y pega en una celda de Colab):\n",
        "\n",
        "```python\n",
        "# Instalar dependencias (ejecuta esta celda primero)\n",
        "!pip install -q gradio yt-dlp ultralytics tensorflow opencv-python sort-tracker numpy\n",
        "\n",
        "# Importar librerías\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from sort import Sort  # Para tracking simple\n",
        "import os\n",
        "import yt_dlp\n",
        "from pathlib import Path\n",
        "\n",
        "# Cargar modelos\n",
        "print(\"Cargando modelos...\")\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # YOLO para detección de personas\n",
        "tracker = Sort()  # Tracker SORT para seguir al agresor\n",
        "\n",
        "# Función para descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    ydl_opts = {'format': 'best', 'outtmpl': '/content/%(title)s.%(ext)s'}\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        video_path = ydl.prepare_filename(info)\n",
        "        return video_path  # Retorna path del video descargado\n",
        "\n",
        "# Función para preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.cast(img, tf.int32)\n",
        "    img = tf.expand_dims(img, 0)\n",
        "    return img\n",
        "\n",
        "# Función para detectar poses\n",
        "def detect_pose(frame):\n",
        "    input_img = preprocess_frame(frame)\n",
        "    outputs = movenet(input_img)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "# Función para heurística de roles (agresor/víctima)\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]  # Mano izquierda (9), derecha (10)\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:  # Confianza alta\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:  # Mano \"alta\" (para agresor)\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:  # Cerca\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 2 and len(person_boxes) >= 2:  # Movimiento alto → agresor 0, víctima 1\n",
        "        return [(0, 'agresor'), (1, 'víctima')]\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Función principal para procesar video\n",
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter('/content/processed_video.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "    frame_count = 0\n",
        "    prev_gray = None\n",
        "    last_roles = None\n",
        "    persistence_counter = 0\n",
        "    agresor_id = None  # ID del agresor para tracking\n",
        "\n",
        "    while cap.isOpened() and frame_count < 300:  # Limita a 300 frames para demo\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Optical Flow para movimiento\n",
        "        mag_mean = 0\n",
        "        if prev_gray is not None:\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "            mag_mean = np.mean(mag)\n",
        "\n",
        "        prev_gray = gray\n",
        "\n",
        "        # Detección de personas\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Persona\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "\n",
        "        # Tracking con SORT\n",
        "        tracks = tracker.update(np.array(person_boxes))\n",
        "        if len(tracks) > 0:\n",
        "            tracks = tracks.astype(int)\n",
        "            for track in tracks:\n",
        "                x1, y1, x2, y2, track_id = track\n",
        "                # Resalta tracking del agresor\n",
        "                color = (0, 0, 255) if track_id == agresor_id else (0, 255, 0)\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "                cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "        # Poses y roles (solo si hay >1 persona)\n",
        "        if len(person_boxes) > 1:\n",
        "            keypoints_list = []\n",
        "            for box in person_boxes:\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                crop = frame[y:y+h, x:x+w]\n",
        "                if crop.size > 0 and w > 50 and h > 100:\n",
        "                    keypoints = detect_pose(crop)\n",
        "                    keypoints_list.append(keypoints)\n",
        "                else:\n",
        "                    keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "            current_roles = analyze_roles(keypoints_list, person_boxes, frame.shape[0], mag_mean)\n",
        "\n",
        "            # Persistencia\n",
        "            if any(r != 'desconocido' for _, r in current_roles):\n",
        "                last_roles = current_roles\n",
        "                persistence_counter = persistence_frames\n",
        "                # Asigna ID del agresor\n",
        "                agresor_id = tracks[0][4]  # Asume el primero es agresor\n",
        "            elif persistence_counter > 0:\n",
        "                current_roles = last_roles\n",
        "                persistence_counter -= 1\n",
        "\n",
        "            # Dibujar roles\n",
        "            for i, (box, keypoints) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                color = (0, 0, 255) if role == 'agresor' else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                cv2.putText(frame, role.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                # Keypoints\n",
        "                for kp in keypoints:\n",
        "                    if kp[2] > 0.3:\n",
        "                        kx = int(kp[1] * w + x)\n",
        "                        ky = int(kp[0] * h + y)\n",
        "                        cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "        else:\n",
        "            current_roles = []\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    return '/content/processed_video.mp4'  # Retorna path del video procesado\n",
        "\n",
        "# Celda 7: App Gradio\n",
        "def gradio_app(youtube_url):\n",
        "    if not youtube_url:\n",
        "        return \"Ingresa una URL de YouTube.\"\n",
        "    \n",
        "    # Descargar video\n",
        "    video_path = download_youtube_video(youtube_url)\n",
        "    if not os.path.exists(video_path):\n",
        "        return \"Error al descargar el video.\"\n",
        "    \n",
        "    # Procesar\n",
        "    output_video = process_video(video_path)\n",
        "    if not os.path.exists(output_video):\n",
        "        return \"Error al procesar el video.\"\n",
        "    \n",
        "    # Limpiar video original\n",
        "    os.rm(video_path)\n",
        "    \n",
        "    return output_video\n",
        "\n",
        "# Interface Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_app,\n",
        "    inputs=gr.Textbox(label=\"URL de YouTube\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID\"),\n",
        "    outputs=gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "    title=\"Identificador de Víctima y Victimario en Videos de YouTube\",\n",
        "    description=\"Sube una URL de YouTube. La app descarga el video, identifica agresor/víctima y trackea al victimario.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Enlace público en Colab\n",
        "```\n",
        "\n",
        "### Cómo Usar\n",
        "1. **Ejecuta el Código**: En Colab, ejecuta todas las celdas. La última lanza la app Gradio con enlace público.\n",
        "2. **Prueba**:\n",
        "   - Ingresa una URL de YouTube con un video de robo CCTV (ej. \"https://www.youtube.com/watch?v=qowtDqraCmU\" del log anterior).\n",
        "   - La app descarga, procesa y muestra el video con anotaciones.\n",
        "3. **Salida**: El video procesado tiene:\n",
        "   - Boxes verdes para personas.\n",
        "   - Etiquetas \"AGRESOR\" (rojo), \"VÍCTIMA\" (azul).\n",
        "   - Keypoints amarillos.\n",
        "   - Tracking ID en el agresor (resaltado rojo).\n",
        "\n",
        "### Mejoras\n",
        "- **Persistencia**: Roles se mantienen 5 frames.\n",
        "- **Tracking**: SORT asigna IDs y sigue al agresor.\n",
        "- **Limpieza**: Elimina el video original para ahorrar espacio.\n",
        "\n",
        "Si necesitas ajustes (ej. más sensibilidad en heurística), avísame. ¡Prueba y cuéntame cómo va! 😊"
      ],
      "metadata": {
        "id": "4aunXzoKbjrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias\n",
        "!pip install -q gradio yt-dlp ultralytics tensorflow opencv-python norfair numpy\n",
        "\n",
        "# Importar librerías\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from ultralytics import YOLO\n",
        "from norfair import Tracker, Detection  # Usar norfair para tracking\n",
        "import os\n",
        "import yt_dlp\n",
        "from pathlib import Path\n",
        "\n",
        "# Cargar modelos\n",
        "print(\"Cargando modelos...\")\n",
        "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = movenet_model.signatures['serving_default']\n",
        "yolo = YOLO('yolov8n.pt')  # YOLO para detección de personas\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "tracker = Tracker(\n",
        "    distance_function=euclidean_distance,\n",
        "    distance_threshold=150,  # Umbral para asociar detecciones\n",
        ")\n",
        "\n",
        "# Función para descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    ydl_opts = {'format': 'best', 'outtmpl': '/content/%(title)s.%(ext)s'}\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            return video_path\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar el video: {str(e)}\"\n",
        "\n",
        "# Función para preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    img = cv2.resize(frame, (192, 192))\n",
        "    img = tf.cast(img, tf.int32)\n",
        "    img = tf.expand_dims(img, 0)\n",
        "    return img\n",
        "\n",
        "# Función para detectar poses\n",
        "def detect_pose(frame):\n",
        "    input_img = preprocess_frame(frame)\n",
        "    outputs = movenet(input_img)\n",
        "    keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "    return keypoints\n",
        "\n",
        "# Función para heurística de roles (agresor/víctima)\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    roles = []\n",
        "    for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "        hands = [keypoints[9], keypoints[10]]  # Mano izquierda (9), derecha (10)\n",
        "        for hand in hands:\n",
        "            if hand[2] > 0.3:  # Confianza alta\n",
        "                hand_y, hand_x = hand[0], hand[1]\n",
        "                if hand_y < 0.6:  # Mano \"alta\" (para agresor)\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:  # Cerca\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "    if mag_mean > 2 and len(person_boxes) >= 2:  # Movimiento alto\n",
        "        return [(0, 'agresor'), (1, 'víctima')]\n",
        "    return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Función principal para procesar video\n",
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter('/content/processed_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    prev_gray = None\n",
        "    last_roles = None\n",
        "    persistence_frames = 5\n",
        "    persistence_counter = 0\n",
        "    agresor_id = None  # ID del agresor para tracking\n",
        "\n",
        "    while cap.isOpened() and frame_count < 300:  # Limita a 300 frames\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Optical Flow para movimiento\n",
        "        mag_mean = 0\n",
        "        if prev_gray is not None:\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "            mag_mean = np.mean(mag)\n",
        "\n",
        "        prev_gray = gray\n",
        "\n",
        "        # Detección de personas con YOLO\n",
        "        results = yolo(frame)\n",
        "        person_boxes = []\n",
        "        detections = []\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Persona\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                    # Preparar detección para Norfair\n",
        "                    center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                    detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "        # Tracking con Norfair\n",
        "        tracked_objects = tracker.update(detections=detections)\n",
        "        track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "        # Poses y roles (solo si hay >1 persona)\n",
        "        if len(person_boxes) > 1:\n",
        "            keypoints_list = []\n",
        "            for box in person_boxes:\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                crop = frame[y:y+h, x:x+w]\n",
        "                if crop.size > 0 and w > 50 and h > 100:\n",
        "                    keypoints = detect_pose(crop)\n",
        "                    keypoints_list.append(keypoints)\n",
        "                else:\n",
        "                    keypoints_list.append(np.zeros((17, 3)))\n",
        "\n",
        "            current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "\n",
        "            # Persistencia\n",
        "            if any(r != 'desconocido' for _, r in current_roles):\n",
        "                last_roles = current_roles\n",
        "                persistence_counter = persistence_frames\n",
        "                # Asignar ID del agresor (primer objeto detectado como agresor)\n",
        "                agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                    agresor_id = tracked_objects[agresor_idx].id\n",
        "            elif persistence_counter > 0:\n",
        "                current_roles = last_roles\n",
        "                persistence_counter -= 1\n",
        "        else:\n",
        "            current_roles = []\n",
        "\n",
        "        # Dibujar boxes, roles y keypoints\n",
        "        for i, box in enumerate(person_boxes):\n",
        "            x, y, w, h = [int(v) for v in box]\n",
        "            role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "            track_id = track_ids[i] if i < len(track_ids) else None\n",
        "            # Color: rojo para agresor, azul para víctima, gris para desconocido\n",
        "            color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "            label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            # Keypoints\n",
        "            if i < len(keypoints_list):\n",
        "                for kp in keypoints_list[i]:\n",
        "                    if kp[2] > 0.3:\n",
        "                        kx = int(kp[1] * w + x)\n",
        "                        ky = int(kp[0] * h + y)\n",
        "                        cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    return '/content/processed_video.mp4'\n",
        "\n",
        "# Función para la app Gradio\n",
        "def gradio_app(youtube_url):\n",
        "    if not youtube_url:\n",
        "        return \"Ingresa una URL válida de YouTube.\"\n",
        "\n",
        "    # Descargar video\n",
        "    video_path = download_youtube_video(youtube_url)\n",
        "    if not os.path.exists(video_path):\n",
        "        return f\"Error al descargar el video: {video_path}\"\n",
        "\n",
        "    # Procesar\n",
        "    output_video = process_video(video_path)\n",
        "    if not os.path.exists(output_video):\n",
        "        return \"Error al procesar el video.\"\n",
        "\n",
        "    # Limpiar video original\n",
        "    os.remove(video_path)\n",
        "\n",
        "    return output_video\n",
        "\n",
        "# Interfaz Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_app,\n",
        "    inputs=gr.Textbox(label=\"URL de YouTube\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID\"),\n",
        "    outputs=gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "    title=\"Identificador de Víctima y Victimario con Tracking\",\n",
        "    description=\"Sube una URL de YouTube. La app identifica agresor/víctima y sigue al victimario.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Enlace público en Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "3qiGSXyeb_S3",
        "outputId": "95e8cac5-8d6c-4461-f398-43b40a635307"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelos...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8532fcbdc31f63a7d1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8532fcbdc31f63a7d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias con manejo de errores\n",
        "print(\"Instalando dependencias...\")\n",
        "try:\n",
        "    !pip install -q gradio==4.44.0 yt-dlp==2024.10.22 ultralytics==8.3.15 tensorflow==2.17.0 opencv-python==4.10.0.84 norfair==2.2.0 numpy==1.26.4\n",
        "    print(\"Dependencias instaladas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al instalar dependencias: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Importar librerías\n",
        "try:\n",
        "    import gradio as gr\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    from ultralytics import YOLO\n",
        "    from norfair import Tracker, Detection\n",
        "    import os\n",
        "    import yt_dlp\n",
        "    from pathlib import Path\n",
        "    print(\"Librerías importadas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al importar librerías: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Cargar modelos\n",
        "try:\n",
        "    print(\"Cargando modelos...\")\n",
        "    movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    movenet = movenet_model.signatures['serving_default']\n",
        "    yolo = YOLO('yolov8n.pt')  # Modelo ligero de YOLO\n",
        "    print(\"Modelos cargados correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar modelos: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "try:\n",
        "    tracker = Tracker(distance_function=euclidean_distance, distance_threshold=150)\n",
        "    print(\"Tracker Norfair configurado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar tracker: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Función para descargar video\n",
        "def download_youtube_video(url):\n",
        "    try:\n",
        "        ydl_opts = {'format': 'best[ext=mp4]', 'outtmpl': '/content/video.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            if os.path.exists(video_path):\n",
        "                print(f\"Video descargado: {video_path}\")\n",
        "                return video_path\n",
        "            else:\n",
        "                return \"Error: Video no encontrado tras la descarga.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar video: {str(e)}\"\n",
        "\n",
        "# Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    try:\n",
        "        img = cv2.resize(frame, (192, 192))\n",
        "        img = tf.cast(img, tf.int32)\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar frame: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Detectar poses\n",
        "def detect_pose(frame):\n",
        "    try:\n",
        "        input_img = preprocess_frame(frame)\n",
        "        if input_img is None:\n",
        "            return np.zeros((17, 3))\n",
        "        outputs = movenet(input_img)\n",
        "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "        return keypoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error al detectar poses: {str(e)}\")\n",
        "        return np.zeros((17, 3))\n",
        "\n",
        "# Heurística de roles\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    try:\n",
        "        roles = []\n",
        "        for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "            hands = [keypoints[9], keypoints[10]]  # Mano izquierda, derecha\n",
        "            for hand in hands:\n",
        "                if hand[2] > 0.3 and hand[0] < 0.6:  # Mano alta\n",
        "                    hand_y, hand_x = hand[0], hand[1]\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "        if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima 1 (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_roles: {str(e)}\")\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Procesar video\n",
        "def process_video(video_path):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        out_path = '/content/processed_video.mp4'\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "        frame_count = 0\n",
        "        prev_gray = None\n",
        "        last_roles = None\n",
        "        persistence_frames = 5\n",
        "        persistence_counter = 0\n",
        "        agresor_id = None\n",
        "\n",
        "        while cap.isOpened() and frame_count < 100:  # Reducido para pruebas\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            mag_mean = 0\n",
        "            if prev_gray is not None:\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "                mag_mean = np.mean(mag)\n",
        "            prev_gray = gray\n",
        "\n",
        "            # Detección con YOLO\n",
        "            results = yolo(frame, verbose=False)\n",
        "            person_boxes = []\n",
        "            detections = []\n",
        "            for result in results:\n",
        "                for box in result.boxes:\n",
        "                    if int(box.cls) == 0:  # Persona\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                        person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                        center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                        detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "            # Tracking con Norfair\n",
        "            tracked_objects = tracker.update(detections=detections)\n",
        "            track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "            # Poses y roles\n",
        "            keypoints_list = []\n",
        "            if len(person_boxes) > 1:\n",
        "                for box in person_boxes:\n",
        "                    x, y, w, h = [int(v) for v in box]\n",
        "                    crop = frame[y:y+h, x:x+w]\n",
        "                    keypoints = detect_pose(crop) if crop.size > 0 and w > 50 and h > 100 else np.zeros((17, 3))\n",
        "                    keypoints_list.append(keypoints)\n",
        "\n",
        "                current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "                if any(r != 'desconocido' for _, r in current_roles):\n",
        "                    last_roles = current_roles\n",
        "                    persistence_counter = persistence_frames\n",
        "                    agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                    if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                        agresor_id = tracked_objects[agresor_idx].id\n",
        "                elif persistence_counter > 0:\n",
        "                    current_roles = last_roles\n",
        "                    persistence_counter -= 1\n",
        "            else:\n",
        "                current_roles = []\n",
        "\n",
        "            # Dibujar\n",
        "            for i, box in enumerate(person_boxes):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                if i < len(keypoints_list):\n",
        "                    for kp in keypoints_list[i]:\n",
        "                        if kp[2] > 0.3:\n",
        "                            kx = int(kp[1] * w + x)\n",
        "                            ky = int(kp[0] * h + y)\n",
        "                            cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"Video procesado guardado en: {out_path}\")\n",
        "        return out_path if os.path.exists(out_path) else \"Error: No se generó el video de salida.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar video: {str(e)}\")\n",
        "        return f\"Error al procesar video: {str(e)}\"\n",
        "\n",
        "# Función para Gradio\n",
        "def gradio_app(youtube_url):\n",
        "    try:\n",
        "        if not youtube_url:\n",
        "            return \"Ingresa una URL válida de YouTube.\"\n",
        "\n",
        "        video_path = download_youtube_video(youtube_url)\n",
        "        if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
        "            return f\"Error al descargar: {video_path}\"\n",
        "\n",
        "        output_video = process_video(video_path)\n",
        "        if not isinstance(output_video, str) or not os.path.exists(output_video):\n",
        "            return f\"Error al procesar: {output_video}\"\n",
        "\n",
        "        os.remove(video_path)  # Limpiar\n",
        "        return output_video\n",
        "    except Exception as e:\n",
        "        return f\"Error en la app: {str(e)}\"\n",
        "\n",
        "# Interfaz Gradio\n",
        "try:\n",
        "    demo = gr.Interface(\n",
        "        fn=gradio_app,\n",
        "        inputs=gr.Textbox(label=\"URL de YouTube\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID\"),\n",
        "        outputs=gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "        title=\"Identificador de Víctima y Victimario\",\n",
        "        description=\"Sube una URL de YouTube para identificar agresor/víctima y seguir al victimario.\"\n",
        "    )\n",
        "    print(\"Interfaz Gradio creada. Lanzando...\")\n",
        "    demo.launch(share=True)  # Enlace público\n",
        "except Exception as e:\n",
        "    print(f\"Error al lanzar Gradio: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "qc4XnL5sfaLs",
        "outputId": "ca1e271e-394e-4085-948c-172d2f68887a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando dependencias...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.6/171.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Cannot install gradio and yt-dlp==2024.10.22 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencias instaladas correctamente.\n",
            "Librerías importadas correctamente.\n",
            "Cargando modelos...\n",
            "Modelos cargados correctamente.\n",
            "Tracker Norfair configurado.\n",
            "Interfaz Gradio creada. Lanzando...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e5b77865ca0dd64574.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e5b77865ca0dd64574.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias con versiones específicas\n",
        "print(\"Instalando dependencias...\")\n",
        "try:\n",
        "    !pip install -q gradio==4.44.0 yt-dlp==2024.10.22 ultralytics==8.3.15 tensorflow==2.17.0 opencv-python==4.10.0.84 norfair==2.2.0 numpy==1.26.4\n",
        "    print(\"Dependencias instaladas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al instalar dependencias: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Importar librerías\n",
        "try:\n",
        "    import gradio as gr\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    from ultralytics import YOLO\n",
        "    from norfair import Tracker, Detection\n",
        "    import os\n",
        "    import yt_dlp\n",
        "    from pathlib import Path\n",
        "    print(\"Librerías importadas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al importar librerías: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Cargar modelos\n",
        "try:\n",
        "    print(\"Cargando modelos...\")\n",
        "    movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    movenet = movenet_model.signatures['serving_default']\n",
        "    yolo = YOLO('yolov8n.pt')  # Modelo ligero de YOLO\n",
        "    print(\"Modelos cargados correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar modelos: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "try:\n",
        "    tracker = Tracker(distance_function=euclidean_distance, distance_threshold=150)\n",
        "    print(\"Tracker Norfair configurado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar tracker: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Casos de prueba predefinidos (URLs de YouTube o placeholders)\n",
        "test_cases = {\n",
        "    \"Caso 1: Robo CCTV (Placeholder)\": \"https://www.youtube.com/watch?v=your_robbery_video_id\",\n",
        "    \"Caso 2: CCTV Normal (Placeholder)\": \"https://www.youtube.com/watch?v=your_normal_video_id\",\n",
        "    \"Caso 3: Video Local (robbery.mp4)\": \"/content/robbery.mp4\"\n",
        "}\n",
        "# Nota: Reemplaza las URLs de YouTube con videos reales de CCTV (busca \"CCTV robbery footage\" o \"CCTV normal activity\").\n",
        "# Si tienes videos locales, súbelos a /content/ con files.upload().\n",
        "\n",
        "# Descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    try:\n",
        "        ydl_opts = {'format': 'best[ext=mp4]', 'outtmpl': '/content/video_%(id)s.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            if os.path.exists(video_path):\n",
        "                print(f\"Video descargado: {video_path}\")\n",
        "                return video_path\n",
        "            else:\n",
        "                return \"Error: Video no encontrado tras la descarga.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar video: {str(e)}\"\n",
        "\n",
        "# Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    try:\n",
        "        img = cv2.resize(frame, (192, 192))\n",
        "        img = tf.cast(img, tf.int32)\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar frame: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Detectar poses\n",
        "def detect_pose(frame):\n",
        "    try:\n",
        "        input_img = preprocess_frame(frame)\n",
        "        if input_img is None:\n",
        "            return np.zeros((17, 3))\n",
        "        outputs = movenet(input_img)\n",
        "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "        return keypoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error al detectar poses: {str(e)}\")\n",
        "        return np.zeros((17, 3))\n",
        "\n",
        "# Heurística de roles\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    try:\n",
        "        roles = []\n",
        "        for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "            hands = [keypoints[9], keypoints[10]]  # Mano izquierda, derecha\n",
        "            for hand in hands:\n",
        "                if hand[2] > 0.3 and hand[0] < 0.6:  # Mano alta\n",
        "                    hand_y, hand_x = hand[0], hand[1]\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "        if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima 1 (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_roles: {str(e)}\")\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Procesar video\n",
        "def process_video(video_path):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        out_path = '/content/processed_video.mp4'\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "        frame_count = 0\n",
        "        prev_gray = None\n",
        "        last_roles = None\n",
        "        persistence_frames = 5\n",
        "        persistence_counter = 0\n",
        "        agresor_id = None\n",
        "\n",
        "        while cap.isOpened() and frame_count < 100:  # Limite para pruebas\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            mag_mean = 0\n",
        "            if prev_gray is not None:\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "                mag_mean = np.mean(mag)\n",
        "            prev_gray = gray\n",
        "\n",
        "            # Detección con YOLO\n",
        "            results = yolo(frame, verbose=False)\n",
        "            person_boxes = []\n",
        "            detections = []\n",
        "            for result in results:\n",
        "                for box in result.boxes:\n",
        "                    if int(box.cls) == 0:  # Persona\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                        person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                        center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                        detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "            # Tracking con Norfair\n",
        "            tracked_objects = tracker.update(detections=detections)\n",
        "            track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "            # Poses y roles\n",
        "            keypoints_list = []\n",
        "            if len(person_boxes) > 1:\n",
        "                for box in person_boxes:\n",
        "                    x, y, w, h = [int(v) for v in box]\n",
        "                    crop = frame[y:y+h, x:x+w]\n",
        "                    keypoints = detect_pose(crop) if crop.size > 0 and w > 50 and h > 100 else np.zeros((17, 3))\n",
        "                    keypoints_list.append(keypoints)\n",
        "\n",
        "                current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "                if any(r != 'desconocido' for _, r in current_roles):\n",
        "                    last_roles = current_roles\n",
        "                    persistence_counter = persistence_frames\n",
        "                    agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                    if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                        agresor_id = tracked_objects[agresor_idx].id\n",
        "                elif persistence_counter > 0:\n",
        "                    current_roles = last_roles\n",
        "                    persistence_counter -= 1\n",
        "            else:\n",
        "                current_roles = []\n",
        "\n",
        "            # Dibujar\n",
        "            for i, box in enumerate(person_boxes):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                if i < len(keypoints_list):\n",
        "                    for kp in keypoints_list[i]:\n",
        "                        if kp[2] > 0.3:\n",
        "                            kx = int(kp[1] * w + x)\n",
        "                            ky = int(kp[0] * h + y)\n",
        "                            cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"Video procesado guardado en: {out_path}\")\n",
        "        return out_path if os.path.exists(out_path) else \"Error: No se generó el video de salida.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar video: {str(e)}\")\n",
        "        return f\"Error al procesar video: {str(e)}\"\n",
        "\n",
        "# Función para Gradio\n",
        "def gradio_app(test_case, youtube_url):\n",
        "    try:\n",
        "        video_path = None\n",
        "        if test_case != \"Subir URL personalizada\":\n",
        "            # Usar caso de prueba\n",
        "            video_path = test_cases.get(test_case)\n",
        "            if not video_path:\n",
        "                return \"Error: Caso de prueba no válido.\"\n",
        "            if video_path.startswith(\"/content/\") and not os.path.exists(video_path):\n",
        "                return f\"Error: Video local {video_path} no encontrado. Sube el video a /content/.\"\n",
        "            elif video_path.startswith(\"https://\"):\n",
        "                video_path = download_youtube_video(video_path)\n",
        "        else:\n",
        "            # Usar URL personalizada\n",
        "            if not youtube_url:\n",
        "                return \"Ingresa una URL válida de YouTube.\"\n",
        "            video_path = download_youtube_video(youtube_url)\n",
        "\n",
        "        if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
        "            return f\"Error al descargar: {video_path}\"\n",
        "\n",
        "        output_video = process_video(video_path)\n",
        "        if not isinstance(output_video, str) or not os.path.exists(output_video):\n",
        "            return f\"Error al procesar: {output_video}\"\n",
        "\n",
        "        if os.path.exists(video_path):\n",
        "            os.remove(video_path)  # Limpiar\n",
        "        return output_video\n",
        "    except Exception as e:\n",
        "        return f\"Error en la app: {str(e)}\"\n",
        "\n",
        "# Interfaz Gradio\n",
        "try:\n",
        "    test_case_options = list(test_cases.keys()) + [\"Subir URL personalizada\"]\n",
        "    demo = gr.Interface(\n",
        "        fn=gradio_app,\n",
        "        inputs=[\n",
        "            gr.Dropdown(label=\"Seleccionar Caso de Prueba\", choices=test_case_options, value=\"Subir URL personalizada\"),\n",
        "            gr.Textbox(label=\"URL de YouTube (si seleccionas 'Subir URL personalizada')\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID\")\n",
        "        ],\n",
        "        outputs=gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "        title=\"Identificador de Víctima y Victimario\",\n",
        "        description=\"Elige un caso de prueba o sube una URL de YouTube para identificar agresor/víctima y seguir al victimario.\"\n",
        "    )\n",
        "    print(\"Interfaz Gradio creada. Lanzando...\")\n",
        "    demo.launch(share=True)  # Enlace público\n",
        "except Exception as e:\n",
        "    print(f\"Error al lanzar Gradio: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "qK31UBrhgIqy",
        "outputId": "8f42a372-20b7-4fbe-88ba-93e248fe3e9f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando dependencias...\n",
            "\u001b[31mERROR: Cannot install gradio and yt-dlp==2024.10.22 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencias instaladas correctamente.\n",
            "Librerías importadas correctamente.\n",
            "Cargando modelos...\n",
            "Modelos cargados correctamente.\n",
            "Tracker Norfair configurado.\n",
            "Interfaz Gradio creada. Lanzando...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b0547c5a9136b04781.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b0547c5a9136b04781.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias\n",
        "print(\"Instalando dependencias...\")\n",
        "try:\n",
        "    !pip install -q gradio==4.44.0 yt-dlp==2024.10.22 ultralytics==8.3.15 tensorflow==2.17.0 opencv-python==4.10.0.84 norfair==2.2.0 numpy==1.26.4\n",
        "    print(\"Dependencias instaladas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al instalar dependencias: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Importar librerías\n",
        "try:\n",
        "    import gradio as gr\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    from ultralytics import YOLO\n",
        "    from norfair import Tracker, Detection\n",
        "    import os\n",
        "    import yt_dlp\n",
        "    from pathlib import Path\n",
        "    print(\"Librerías importadas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al importar librerías: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Cargar modelos\n",
        "try:\n",
        "    print(\"Cargando modelos...\")\n",
        "    movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    movenet = movenet_model.signatures['serving_default']\n",
        "    yolo = YOLO('yolov8n.pt')\n",
        "    print(\"Modelos cargados correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar modelos: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "try:\n",
        "    tracker = Tracker(distance_function=euclidean_distance, distance_threshold=150)\n",
        "    print(\"Tracker Norfair configurado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar tracker: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Casos de prueba predefinidos\n",
        "test_cases = {\n",
        "    \"Caso 1: Robo CCTV (Placeholder)\": \"https://www.youtube.com/watch?v=your_robbery_video_id\",\n",
        "    \"Caso 2: CCTV Normal (Placeholder)\": \"https://www.youtube.com/watch?v=your_normal_video_id\",\n",
        "    \"Caso 3: Video Local (robbery.mp4)\": \"/content/robbery.mp4\"\n",
        "}\n",
        "# Reemplaza URLs con videos reales de CCTV o súbelos a /content/\n",
        "\n",
        "# Descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    try:\n",
        "        ydl_opts = {'format': 'best[ext=mp4]', 'outtmpl': '/content/video_%(id)s.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            if os.path.exists(video_path):\n",
        "                print(f\"Video descargado: {video_path}\")\n",
        "                return video_path\n",
        "            return \"Error: Video no encontrado tras la descarga.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar video: {str(e)}\"\n",
        "\n",
        "# Dividir video en segmentos\n",
        "def slice_video(video_path, segment_duration=5):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return None, \"Error: No se pudo abrir el video para segmentar.\"\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        segment_frames = segment_duration * fps\n",
        "        segments = []\n",
        "        segment_paths = []\n",
        "\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frame_count = 0\n",
        "        segment_idx = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            out_path = f\"/content/segment_{segment_idx}.mp4\"\n",
        "            out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "            segment_start = frame_count / fps\n",
        "            for _ in range(segment_frames):\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                out.write(frame)\n",
        "                frame_count += 1\n",
        "            out.release()\n",
        "            if os.path.exists(out_path) and frame_count > segment_idx * segment_frames:\n",
        "                segment_label = f\"Segmento {segment_idx + 1}: {segment_start:.1f}-{(segment_start + segment_duration):.1f}s\"\n",
        "                segments.append(segment_label)\n",
        "                segment_paths.append(out_path)\n",
        "            segment_idx += 1\n",
        "            if frame_count >= total_frames:\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "        return segments, segment_paths\n",
        "    except Exception as e:\n",
        "        return None, f\"Error al segmentar video: {str(e)}\"\n",
        "\n",
        "# Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    try:\n",
        "        img = cv2.resize(frame, (192, 192))\n",
        "        img = tf.cast(img, tf.int32)\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar frame: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Detectar poses\n",
        "def detect_pose(frame):\n",
        "    try:\n",
        "        input_img = preprocess_frame(frame)\n",
        "        if input_img is None:\n",
        "            return np.zeros((17, 3))\n",
        "        outputs = movenet(input_img)\n",
        "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "        return keypoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error al detectar poses: {str(e)}\")\n",
        "        return np.zeros((17, 3))\n",
        "\n",
        "# Heurística de roles\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    try:\n",
        "        roles = []\n",
        "        for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "            hands = [keypoints[9], keypoints[10]]\n",
        "            for hand in hands:\n",
        "                if hand[2] > 0.3 and hand[0] < 0.6:\n",
        "                    hand_y, hand_x = hand[0], hand[1]\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "        if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima 1 (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_roles: {str(e)}\")\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Procesar video\n",
        "def process_video(video_path):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        out_path = '/content/processed_segment.mp4'\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "        frame_count = 0\n",
        "        prev_gray = None\n",
        "        last_roles = None\n",
        "        persistence_frames = 5\n",
        "        persistence_counter = 0\n",
        "        agresor_id = None\n",
        "        tracker.reset()  # Resetear tracker para cada segmento\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            mag_mean = 0\n",
        "            if prev_gray is not None:\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "                mag_mean = np.mean(mag)\n",
        "            prev_gray = gray\n",
        "\n",
        "            results = yolo(frame, verbose=False)\n",
        "            person_boxes = []\n",
        "            detections = []\n",
        "            for result in results:\n",
        "                for box in result.boxes:\n",
        "                    if int(box.cls) == 0:\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                        person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                        center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                        detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "            tracked_objects = tracker.update(detections=detections)\n",
        "            track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "            keypoints_list = []\n",
        "            if len(person_boxes) > 1:\n",
        "                for box in person_boxes:\n",
        "                    x, y, w, h = [int(v) for v in box]\n",
        "                    crop = frame[y:y+h, x:x+w]\n",
        "                    keypoints = detect_pose(crop) if crop.size > 0 and w > 50 and h > 100 else np.zeros((17, 3))\n",
        "                    keypoints_list.append(keypoints)\n",
        "\n",
        "                current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "                if any(r != 'desconocido' for _, r in current_roles):\n",
        "                    last_roles = current_roles\n",
        "                    persistence_counter = persistence_frames\n",
        "                    agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                    if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                        agresor_id = tracked_objects[agresor_idx].id\n",
        "                elif persistence_counter > 0:\n",
        "                    current_roles = last_roles\n",
        "                    persistence_counter -= 1\n",
        "            else:\n",
        "                current_roles = []\n",
        "\n",
        "            for i, box in enumerate(person_boxes):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                if i < len(keypoints_list):\n",
        "                    for kp in keypoints_list[i]:\n",
        "                        if kp[2] > 0.3:\n",
        "                            kx = int(kp[1] * w + x)\n",
        "                            ky = int(kp[0] * h + y)\n",
        "                            cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"Segmento procesado guardado en: {out_path}\")\n",
        "        return out_path if os.path.exists(out_path) else \"Error: No se generó el video de salida.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar video: {str(e)}\")\n",
        "        return f\"Error al procesar video: {str(e)}\"\n",
        "\n",
        "# Función para Gradio\n",
        "def gradio_app(test_case, youtube_url, segment_choice):\n",
        "    try:\n",
        "        # Limpiar segmentos anteriores\n",
        "        for f in Path(\"/content\").glob(\"segment_*.mp4\"):\n",
        "            f.unlink()\n",
        "\n",
        "        video_path = None\n",
        "        if test_case != \"Subir URL personalizada\":\n",
        "            video_path = test_cases.get(test_case)\n",
        "            if not video_path:\n",
        "                return \"Error: Caso de prueba no válido.\", None\n",
        "            if video_path.startswith(\"/content/\") and not os.path.exists(video_path):\n",
        "                return f\"Error: Video local {video_path} no encontrado. Sube el video a /content/.\", None\n",
        "            elif video_path.startswith(\"https://\"):\n",
        "                video_path = download_youtube_video(video_path)\n",
        "        else:\n",
        "            if not youtube_url:\n",
        "                return \"Ingresa una URL válida de YouTube.\", None\n",
        "            video_path = download_youtube_video(youtube_url)\n",
        "\n",
        "        if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
        "            return f\"Error al descargar: {video_path}\", None\n",
        "\n",
        "        # Dividir video en segmentos\n",
        "        segments, segment_paths = slice_video(video_path, segment_duration=5)\n",
        "        if not segments:\n",
        "            return segment_paths, None  # segment_paths contiene el mensaje de error\n",
        "\n",
        "        # Actualizar opciones de segmentos\n",
        "        segment_options = segments\n",
        "        if not segment_options:\n",
        "            return \"Error: No se generaron segmentos.\", None\n",
        "\n",
        "        # Procesar el segmento seleccionado\n",
        "        if segment_choice not in segments:\n",
        "            return \"Selecciona un segmento válido.\", segment_options\n",
        "        segment_idx = segments.index(segment_choice)\n",
        "        output_video = process_video(segment_paths[segment_idx])\n",
        "\n",
        "        # Limpiar\n",
        "        if os.path.exists(video_path):\n",
        "            os.remove(video_path)\n",
        "        for sp in segment_paths:\n",
        "            if os.path.exists(sp):\n",
        "                os.remove(sp)\n",
        "\n",
        "        return output_video, segment_options\n",
        "    except Exception as e:\n",
        "        return f\"Error en la app: {str(e)}\", None\n",
        "\n",
        "# Interfaz Gradio\n",
        "try:\n",
        "    test_case_options = list(test_cases.keys()) + [\"Subir URL personalizada\"]\n",
        "    demo = gr.Interface(\n",
        "        fn=gradio_app,\n",
        "        inputs=[\n",
        "            gr.Dropdown(label=\"Seleccionar Caso de Prueba\", choices=test_case_options, value=\"Subir URL personalizada\"),\n",
        "            gr.Textbox(label=\"URL de YouTube (si seleccionas 'Subir URL personalizada')\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID\"),\n",
        "            gr.Dropdown(label=\"Seleccionar Segmento\", choices=[\"Sube un video primero\"], value=None, interactive=True)\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "            gr.Dropdown(label=\"Segmentos Disponibles\", choices=[\"Sube un video primero\"], interactive=False, visible=False)\n",
        "        ],\n",
        "        title=\"Identificador de Víctima y Victimario con Slicing\",\n",
        "        description=\"Elige un caso de prueba o sube una URL de YouTube. Selecciona un segmento (5s) para procesar.\"\n",
        "    )\n",
        "    print(\"Interfaz Gradio creada. Lanzando...\")\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error al lanzar Gradio: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "CkdSkmMKhduh",
        "outputId": "8eb68f9c-c8ca-4522-865b-558e2e6e8854"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando dependencias...\n",
            "\u001b[31mERROR: Cannot install gradio and yt-dlp==2024.10.22 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencias instaladas correctamente.\n",
            "Librerías importadas correctamente.\n",
            "Cargando modelos...\n",
            "Modelos cargados correctamente.\n",
            "Tracker Norfair configurado.\n",
            "Interfaz Gradio creada. Lanzando...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://34c6613d18f48e702d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://34c6613d18f48e702d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias\n",
        "print(\"Instalando dependencias...\")\n",
        "try:\n",
        "    !pip install -q gradio==4.44.0 yt-dlp==2024.10.22 ultralytics==8.3.15 tensorflow==2.17.0 opencv-python==4.10.0.84 norfair==2.2.0 numpy==1.26.4\n",
        "    print(\"Dependencias instaladas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al instalar dependencias: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Importar librerías\n",
        "try:\n",
        "    import gradio as gr\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    from ultralytics import YOLO\n",
        "    from norfair import Tracker, Detection\n",
        "    import os\n",
        "    import yt_dlp\n",
        "    from pathlib import Path\n",
        "    print(\"Librerías importadas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al importar librerías: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Cargar modelos\n",
        "try:\n",
        "    print(\"Cargando modelos...\")\n",
        "    movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    movenet = movenet_model.signatures['serving_default']\n",
        "    yolo = YOLO('yolov8n.pt')\n",
        "    print(\"Modelos cargados correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar modelos: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "try:\n",
        "    tracker = Tracker(distance_function=euclidean_distance, distance_threshold=150)\n",
        "    print(\"Tracker Norfair configurado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar tracker: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Casos de prueba predefinidos\n",
        "test_cases = {\n",
        "    \"Caso 1: Robo CCTV (Placeholder)\": \"https://www.youtube.com/watch?v=your_robbery_video_id\",\n",
        "    \"Caso 2: CCTV Normal (Placeholder)\": \"https://www.youtube.com/watch?v=your_normal_video_id\",\n",
        "    \"Caso 3: Video Local (robbery.mp4)\": \"/content/robbery.mp4\"\n",
        "}\n",
        "# Reemplaza URLs con videos reales o súbelos a /content/\n",
        "\n",
        "# Descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    try:\n",
        "        ydl_opts = {'format': 'best[ext=mp4]', 'outtmpl': '/content/video_%(id)s.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            if os.path.exists(video_path):\n",
        "                print(f\"Video descargado: {video_path}\")\n",
        "                return video_path\n",
        "            return \"Error: Video no encontrado tras la descarga.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar video: {str(e)}\"\n",
        "\n",
        "# Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    try:\n",
        "        img = cv2.resize(frame, (192, 192))\n",
        "        img = tf.cast(img, tf.int32)\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar frame: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Detectar poses\n",
        "def detect_pose(frame):\n",
        "    try:\n",
        "        input_img = preprocess_frame(frame)\n",
        "        if input_img is None:\n",
        "            return np.zeros((17, 3))\n",
        "        outputs = movenet(input_img)\n",
        "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "        return keypoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error al detectar poses: {str(e)}\")\n",
        "        return np.zeros((17, 3))\n",
        "\n",
        "# Heurística de roles\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    try:\n",
        "        roles = []\n",
        "        for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "            hands = [keypoints[9], keypoints[10]]\n",
        "            for hand in hands:\n",
        "                if hand[2] > 0.3 and hand[0] < 0.6:\n",
        "                    hand_y, hand_x = hand[0], hand[1]\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "        if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima 1 (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_roles: {str(e)}\")\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Procesar segmento de video\n",
        "def process_video(video_path, start_time=0, segment_duration=5):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        out_path = '/content/processed_segment.mp4'\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "        # Ir al frame inicial\n",
        "        start_frame = int(start_time * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "        segment_frames = int(segment_duration * fps)\n",
        "\n",
        "        frame_count = 0\n",
        "        prev_gray = None\n",
        "        last_roles = None\n",
        "        persistence_frames = 5\n",
        "        persistence_counter = 0\n",
        "        agresor_id = None\n",
        "        tracker.reset()\n",
        "\n",
        "        while cap.isOpened() and frame_count < segment_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            mag_mean = 0\n",
        "            if prev_gray is not None:\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "                mag_mean = np.mean(mag)\n",
        "            prev_gray = gray\n",
        "\n",
        "            results = yolo(frame, verbose=False)\n",
        "            person_boxes = []\n",
        "            detections = []\n",
        "            for result in results:\n",
        "                for box in result.boxes:\n",
        "                    if int(box.cls) == 0:\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                        person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                        center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                        detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "            tracked_objects = tracker.update(detections=detections)\n",
        "            track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "            keypoints_list = []\n",
        "            if len(person_boxes) > 1:\n",
        "                for box in person_boxes:\n",
        "                    x, y, w, h = [int(v) for v in box]\n",
        "                    crop = frame[y:y+h, x:x+w]\n",
        "                    keypoints = detect_pose(crop) if crop.size > 0 and w > 50 and h > 100 else np.zeros((17, 3))\n",
        "                    keypoints_list.append(keypoints)\n",
        "\n",
        "                current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "                if any(r != 'desconocido' for _, r in current_roles):\n",
        "                    last_roles = current_roles\n",
        "                    persistence_counter = persistence_frames\n",
        "                    agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                    if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                        agresor_id = tracked_objects[agresor_idx].id\n",
        "                elif persistence_counter > 0:\n",
        "                    current_roles = last_roles\n",
        "                    persistence_counter -= 1\n",
        "            else:\n",
        "                current_roles = []\n",
        "\n",
        "            for i, box in enumerate(person_boxes):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                if i < len(keypoints_list):\n",
        "                    for kp in keypoints_list[i]:\n",
        "                        if kp[2] > 0.3:\n",
        "                            kx = int(kp[1] * w + x)\n",
        "                            ky = int(kp[0] * h + y)\n",
        "                            cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"Segmento procesado guardado en: {out_path}\")\n",
        "        return out_path if os.path.exists(out_path) else \"Error: No se generó el video de salida.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar video: {str(e)}\")\n",
        "        return f\"Error al procesar video: {str(e)}\"\n",
        "\n",
        "# Obtener duración del video para generar opciones de segmentos\n",
        "def get_video_duration(video_path):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return 30.0  # Duración por defecto si falla\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        cap.release()\n",
        "        return total_frames / fps if fps > 0 else 30.0\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener duración: {str(e)}\")\n",
        "        return 30.0\n",
        "\n",
        "# Función para Gradio\n",
        "def gradio_app(test_case, youtube_url, start_time):\n",
        "    try:\n",
        "        # Limpiar archivos anteriores\n",
        "        for f in Path(\"/content\").glob(\"video_*.mp4\"):\n",
        "            f.unlink()\n",
        "        for f in Path(\"/content\").glob(\"processed_segment.mp4\"):\n",
        "            f.unlink()\n",
        "\n",
        "        video_path = None\n",
        "        if test_case != \"Subir URL personalizada\":\n",
        "            video_path = test_cases.get(test_case)\n",
        "            if not video_path:\n",
        "                return \"Error: Caso de prueba no válido.\", None\n",
        "            if video_path.startswith(\"/content/\") and not os.path.exists(video_path):\n",
        "                return f\"Error: Video local {video_path} no encontrado. Sube el video a /content/.\", None\n",
        "            elif video_path.startswith(\"https://\"):\n",
        "                video_path = download_youtube_video(video_path)\n",
        "        else:\n",
        "            if not youtube_url:\n",
        "                return \"Ingresa una URL válida de YouTube.\", None\n",
        "            video_path = download_youtube_video(youtube_url)\n",
        "\n",
        "        if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
        "            return f\"Error al descargar: {video_path}\", None\n",
        "\n",
        "        # Obtener duración y generar opciones de segmentos\n",
        "        duration = get_video_duration(video_path)\n",
        "        segment_duration = 5\n",
        "        segment_starts = [f\"Segmento {i+1}: {t:.1f}s\" for i, t in enumerate(range(0, int(duration), segment_duration))]\n",
        "        if not segment_starts:\n",
        "            segment_starts = [\"Segmento 1: 0.0s\"]\n",
        "\n",
        "        if start_time not in segment_starts:\n",
        "            return \"Selecciona un segmento válido.\", segment_starts\n",
        "\n",
        "        start_time_sec = float(start_time.split(\": \")[1].replace(\"s\", \"\"))\n",
        "        output_video = process_video(video_path, start_time=start_time_sec, segment_duration=5)\n",
        "\n",
        "        if os.path.exists(video_path):\n",
        "            os.remove(video_path)\n",
        "\n",
        "        return output_video, segment_starts\n",
        "    except Exception as e:\n",
        "        return f\"Error en la app: {str(e)}\", None\n",
        "\n",
        "# Interfaz Gradio\n",
        "try:\n",
        "    test_case_options = list(test_cases.keys()) + [\"Subir URL personalizada\"]\n",
        "    demo = gr.Interface(\n",
        "        fn=gradio_app,\n",
        "        inputs=[\n",
        "            gr.Dropdown(label=\"Seleccionar Caso de Prueba\", choices=test_case_options, value=\"Subir URL personalizada\"),\n",
        "            gr.Textbox(label=\"URL de YouTube (si seleccionas 'Subir URL personalizada')\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID\"),\n",
        "            gr.Dropdown(label=\"Seleccionar Segmento\", choices=[\"Sube un video primero\"], value=None, interactive=True)\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "            gr.Dropdown(label=\"Segmentos Disponibles\", choices=[\"Sube un video primero\"], interactive=False, visible=False)\n",
        "        ],\n",
        "        title=\"Identificador de Víctima y Victimario\",\n",
        "        description=\"Elige un caso de prueba o sube una URL de YouTube. Selecciona un segmento para procesar.\"\n",
        "    )\n",
        "    print(\"Interfaz Gradio creada. Lanzando...\")\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error al lanzar Gradio: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "x_yXwCJyid19",
        "outputId": "bf094360-40ca-41e9-839c-2e3ed81d55bf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando dependencias...\n",
            "\u001b[31mERROR: Cannot install gradio and yt-dlp==2024.10.22 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencias instaladas correctamente.\n",
            "Librerías importadas correctamente.\n",
            "Cargando modelos...\n",
            "Modelos cargados correctamente.\n",
            "Tracker Norfair configurado.\n",
            "Interfaz Gradio creada. Lanzando...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://22b5d95d48c5aca1ce.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://22b5d95d48c5aca1ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar dependencias sin reinstalar\n",
        "import importlib.util\n",
        "import sys\n",
        "import os\n",
        "\n",
        "required_packages = ['gradio', 'yt_dlp', 'ultralytics', 'tensorflow', 'cv2', 'norfair', 'numpy']\n",
        "all_installed = True\n",
        "for pkg in required_packages:\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        all_installed = False\n",
        "        print(f\"Paquete {pkg} no encontrado. Instalando dependencias...\")\n",
        "        break\n",
        "\n",
        "if not all_installed:\n",
        "    print(\"Instalando dependencias...\")\n",
        "    try:\n",
        "        !pip install -q gradio==4.44.0 yt-dlp==2024.10.22 ultralytics==8.3.15 tensorflow==2.17.0 opencv-python==4.10.0.84 norfair==2.2.0 numpy==1.26.4\n",
        "        print(\"Dependencias instaladas correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al instalar dependencias: {str(e)}\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"Dependencias ya instaladas. Saltando instalación.\")\n",
        "\n",
        "# Importar librerías\n",
        "try:\n",
        "    import gradio as gr\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    from ultralytics import YOLO\n",
        "    from norfair import Tracker, Detection\n",
        "    from pathlib import Path\n",
        "    print(\"Librerías importadas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al importar librerías: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Verificar y cargar modelos\n",
        "movenet = None\n",
        "yolo = None\n",
        "if 'movenet' not in globals() or 'yolo' not in globals():\n",
        "    try:\n",
        "        print(\"Cargando modelos...\")\n",
        "        movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "        movenet = movenet_model.signatures['serving_default']\n",
        "        yolo = YOLO('yolov8n.pt')\n",
        "        print(\"Modelos cargados correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar modelos: {str(e)}\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"Modelos ya cargados. Saltando carga.\")\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "try:\n",
        "    tracker = Tracker(distance_function=euclidean_distance, distance_threshold=150)\n",
        "    print(\"Tracker Norfair configurado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar tracker: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Casos de prueba predefinidos\n",
        "test_cases = {\n",
        "    \"Caso 1: Robo CCTV (Placeholder)\": \"https://www.youtube.com/watch?v=your_robbery_video_id\",\n",
        "    \"Caso 2: CCTV Normal (Placeholder)\": \"https://www.youtube.com/watch?v=your_normal_video_id\",\n",
        "    \"Caso 3: Video Local (robbery.mp4)\": \"/content/robbery.mp4\"\n",
        "}\n",
        "# Reemplaza URLs con videos reales o súbelos a /content/\n",
        "\n",
        "# Descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    try:\n",
        "        ydl_opts = {'format': 'best[ext=mp4]', 'outtmpl': '/content/video_%(id)s.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            if os.path.exists(video_path):\n",
        "                print(f\"Video descargado: {video_path}\")\n",
        "                return video_path\n",
        "            return \"Error: Video no encontrado tras la descarga.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar video: {str(e)}\"\n",
        "\n",
        "# Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    try:\n",
        "        img = cv2.resize(frame, (192, 192))\n",
        "        img = tf.cast(img, tf.int32)\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar frame: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Detectar poses\n",
        "def detect_pose(frame):\n",
        "    try:\n",
        "        input_img = preprocess_frame(frame)\n",
        "        if input_img is None:\n",
        "            return np.zeros((17, 3))\n",
        "        outputs = movenet(input_img)\n",
        "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "        return keypoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error al detectar poses: {str(e)}\")\n",
        "        return np.zeros((17, 3))\n",
        "\n",
        "# Heurística de roles\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    try:\n",
        "        roles = []\n",
        "        for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "            hands = [keypoints[9], keypoints[10]]\n",
        "            for hand in hands:\n",
        "                if hand[2] > 0.3 and hand[0] < 0.6:\n",
        "                    hand_y, hand_x = hand[0], hand[1]\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "        if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima 1 (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_roles: {str(e)}\")\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Procesar segmento de video\n",
        "def process_video(video_path, start_time=0, segment_duration=5):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        out_path = '/content/processed_segment.mp4'\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "        start_frame = int(start_time * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "        segment_frames = int(segment_duration * fps)\n",
        "\n",
        "        frame_count = 0\n",
        "        prev_gray = None\n",
        "        last_roles = None\n",
        "        persistence_frames = 5\n",
        "        persistence_counter = 0\n",
        "        agresor_id = None\n",
        "        tracker.reset()\n",
        "\n",
        "        while cap.isOpened() and frame_count < segment_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            mag_mean = 0\n",
        "            if prev_gray is not None:\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "                mag_mean = np.mean(mag)\n",
        "            prev_gray = gray\n",
        "\n",
        "            results = yolo(frame, verbose=False)\n",
        "            person_boxes = []\n",
        "            detections = []\n",
        "            for result in results:\n",
        "                for box in result.boxes:\n",
        "                    if int(box.cls) == 0:\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                        person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                        center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                        detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "            tracked_objects = tracker.update(detections=detections)\n",
        "            track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "            keypoints_list = []\n",
        "            if len(person_boxes) > 1:\n",
        "                for box in person_boxes:\n",
        "                    x, y, w, h = [int(v) for v in box]\n",
        "                    crop = frame[y:y+h, x:x+w]\n",
        "                    keypoints = detect_pose(crop) if crop.size > 0 and w > 50 and h > 100 else np.zeros((17, 3))\n",
        "                    keypoints_list.append(keypoints)\n",
        "\n",
        "                current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "                if any(r != 'desconocido' for _, r in current_roles):\n",
        "                    last_roles = current_roles\n",
        "                    persistence_counter = persistence_frames\n",
        "                    agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                    if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                        agresor_id = tracked_objects[agresor_idx].id\n",
        "                elif persistence_counter > 0:\n",
        "                    current_roles = last_roles\n",
        "                    persistence_counter -= 1\n",
        "            else:\n",
        "                current_roles = []\n",
        "\n",
        "            for i, box in enumerate(person_boxes):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                if i < len(keypoints_list):\n",
        "                    for kp in keypoints_list[i]:\n",
        "                        if kp[2] > 0.3:\n",
        "                            kx = int(kp[1] * w + x)\n",
        "                            ky = int(kp[0] * h + y)\n",
        "                            cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        if os.path.exists(out_path):\n",
        "            print(f\"Segmento procesado guardado en: {out_path}\")\n",
        "            return out_path\n",
        "        return \"Error: No se generó el video de salida.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar video: {str(e)}\")\n",
        "        return f\"Error al procesar video: {str(e)}\"\n",
        "\n",
        "# Obtener duración del video\n",
        "def get_video_duration(video_path):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return 30.0\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        cap.release()\n",
        "        return total_frames / fps if fps > 0 else 30.0\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener duración: {str(e)}\")\n",
        "        return 30.0\n",
        "\n",
        "# Función para Gradio\n",
        "def gradio_app(test_case, youtube_url, start_time):\n",
        "    try:\n",
        "        # Limpiar archivos anteriores\n",
        "        for f in Path(\"/content\").glob(\"video_*.mp4\"):\n",
        "            f.unlink()\n",
        "        for f in Path(\"/content\").glob(\"processed_segment.mp4\"):\n",
        "            f.unlink()\n",
        "\n",
        "        video_path = None\n",
        "        if test_case != \"Subir URL personalizada\":\n",
        "            video_path = test_cases.get(test_case)\n",
        "            if not video_path:\n",
        "                return \"Error: Caso de prueba no válido.\", None, None\n",
        "            if video_path.startswith(\"/content/\") and not os.path.exists(video_path):\n",
        "                return f\"Error: Video local {video_path} no encontrado. Sube el video a /content/.\", None, None\n",
        "            elif video_path.startswith(\"https://\"):\n",
        "                video_path = download_youtube_video(video_path)\n",
        "        else:\n",
        "            if not youtube_url:\n",
        "                return \"Ingresa una URL válida de YouTube.\", None, None\n",
        "            video_path = download_youtube_video(youtube_url)\n",
        "\n",
        "        if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
        "            return f\"Error al descargar: {video_path}\", None, None\n",
        "\n",
        "        # Generar opciones de segmentos\n",
        "        duration = get_video_duration(video_path)\n",
        "        segment_duration = 5\n",
        "        segment_starts = [f\"Segmento {i+1}: {t:.1f}s\" for i, t in enumerate(range(0, int(duration), segment_duration))]\n",
        "        if not segment_starts:\n",
        "            segment_starts = [\"Segmento 1: 0.0s\"]\n",
        "\n",
        "        if start_time not in segment_starts:\n",
        "            return \"Selecciona un segmento válido.\", None, segment_starts\n",
        "\n",
        "        start_time_sec = float(start_time.split(\": \")[1].replace(\"s\", \"\"))\n",
        "        output_video = process_video(video_path, start_time=start_time_sec, segment_duration=5)\n",
        "\n",
        "        if os.path.exists(video_path):\n",
        "            os.remove(video_path)\n",
        "\n",
        "        if isinstance(output_video, str) and os.path.exists(output_video):\n",
        "            return output_video, gr.File(label=\"Descargar Video Procesado\", value=output_video), segment_starts\n",
        "        return f\"Error al procesar: {output_video}\", None, segment_starts\n",
        "    except Exception as e:\n",
        "        return f\"Error en la app: {str(e)}\", None, None\n",
        "\n",
        "# Interfaz Gradio\n",
        "try:\n",
        "    test_case_options = list(test_cases.keys()) + [\"Subir URL personalizada\"]\n",
        "    demo = gr.Interface(\n",
        "        fn=gradio_app,\n",
        "        inputs=[\n",
        "            gr.Dropdown(label=\"Seleccionar Caso de Prueba\", choices=test_case_options, value=\"Subir URL personalizada\"),\n",
        "            gr.Textbox(label=\"URL de YouTube (si seleccionas 'Subir URL personalizada')\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID\"),\n",
        "            gr.Dropdown(label=\"Seleccionar Segmento\", choices=[\"Sube un video primero\"], value=None, interactive=True)\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "            gr.File(label=\"Descargar Video Procesado\"),\n",
        "            gr.Dropdown(label=\"Segmentos Disponibles\", choices=[\"Sube un video primero\"], interactive=False, visible=False)\n",
        "        ],\n",
        "        title=\"Identificador de Víctima y Victimario\",\n",
        "        description=\"Elige un caso de prueba o sube una URL de YouTube. Selecciona un segmento para procesar. Descarga el video resultante.\"\n",
        "    )\n",
        "    print(\"Interfaz Gradio creada. Lanzando...\")\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error al lanzar Gradio: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "JdFnkJLCjltd",
        "outputId": "a95ae14c-72ac-4173-a3ab-91aa48eedb83"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencias ya instaladas. Saltando instalación.\n",
            "Librerías importadas correctamente.\n",
            "Modelos ya cargados. Saltando carga.\n",
            "Tracker Norfair configurado.\n",
            "Interfaz Gradio creada. Lanzando...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3a1d1681be603df2e2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3a1d1681be603df2e2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar dependencias sin reinstalar\n",
        "import importlib.util\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "required_packages = ['gradio', 'yt_dlp', 'ultralytics', 'tensorflow', 'cv2', 'norfair', 'numpy']\n",
        "all_installed = True\n",
        "for pkg in required_packages:\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        all_installed = False\n",
        "        print(f\"Paquete {pkg} no encontrado. Instalando dependencias...\")\n",
        "        break\n",
        "\n",
        "if not all_installed:\n",
        "    print(\"Instalando dependencias...\")\n",
        "    try:\n",
        "        !pip install -q gradio==4.44.0 yt-dlp==2024.10.22 ultralytics==8.3.15 tensorflow==2.17.0 opencv-python==4.10.0.84 norfair==2.2.0 numpy==1.26.4\n",
        "        print(\"Dependencias instaladas correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al instalar dependencias: {str(e)}\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"Dependencias ya instaladas. Saltando instalación.\")\n",
        "\n",
        "# Importar librerías\n",
        "try:\n",
        "    import gradio as gr\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    from ultralytics import YOLO\n",
        "    from norfair import Tracker, Detection\n",
        "    from pathlib import Path\n",
        "    print(\"Librerías importadas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al importar librerías: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Verificar y cargar modelos\n",
        "movenet = None\n",
        "yolo = None\n",
        "if 'movenet' not in globals() or 'yolo' not in globals():\n",
        "    try:\n",
        "        print(\"Cargando modelos...\")\n",
        "        movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "        movenet = movenet_model.signatures['serving_default']\n",
        "        yolo = YOLO('yolov8n.pt')\n",
        "        print(\"Modelos cargados correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar modelos: {str(e)}\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"Modelos ya cargados. Saltando carga.\")\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "try:\n",
        "    tracker = Tracker(distance_function=euclidean_distance, distance_threshold=150)\n",
        "    print(\"Tracker Norfair configurado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar tracker: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Casos de prueba predefinidos\n",
        "test_cases = {\n",
        "    \"Caso 1: Robo CCTV (Placeholder)\": \"https://www.youtube.com/watch?v=your_robbery_video_id\",\n",
        "    \"Caso 2: CCTV Normal (Placeholder)\": \"https://www.youtube.com/watch?v=your_normal_video_id\",\n",
        "    \"Caso 3: Video Local (robbery.mp4)\": \"/content/robbery.mp4\"\n",
        "}\n",
        "# Reemplaza URLs con videos reales o súbelos a /content/\n",
        "\n",
        "# Extraer start_time de URL de YouTube\n",
        "def extract_start_time(url):\n",
        "    try:\n",
        "        match = re.search(r\"[?&]t=(\\d+)s?\", url)\n",
        "        return int(match.group(1)) if match else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error al extraer start_time: {str(e)}\")\n",
        "        return 0\n",
        "\n",
        "# Descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    try:\n",
        "        ydl_opts = {'format': 'best[ext=mp4]', 'outtmpl': '/content/video_%(id)s.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            if os.path.exists(video_path):\n",
        "                print(f\"Video descargado: {video_path}\")\n",
        "                return video_path\n",
        "            return \"Error: Video no encontrado tras la descarga.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar video: {str(e)}\"\n",
        "\n",
        "# Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    try:\n",
        "        img = cv2.resize(frame, (192, 192))\n",
        "        img = tf.cast(img, tf.int32)\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar frame: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Detectar poses\n",
        "def detect_pose(frame):\n",
        "    try:\n",
        "        input_img = preprocess_frame(frame)\n",
        "        if input_img is None:\n",
        "            return np.zeros((17, 3))\n",
        "        outputs = movenet(input_img)\n",
        "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "        return keypoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error al detectar poses: {str(e)}\")\n",
        "        return np.zeros((17, 3))\n",
        "\n",
        "# Heurística de roles\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    try:\n",
        "        roles = []\n",
        "        for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "            hands = [keypoints[9], keypoints[10]]\n",
        "            for hand in hands:\n",
        "                if hand[2] > 0.3 and hand[0] < 0.6:\n",
        "                    hand_y, hand_x = hand[0], hand[1]\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "        if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima 1 (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_roles: {str(e)}\")\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Procesar segmento de video\n",
        "def process_video(video_path, start_time=0, segment_duration=5):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        out_path = '/content/processed_segment.mp4'\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "        start_frame = int(start_time * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "        segment_frames = int(segment_duration * fps)\n",
        "\n",
        "        frame_count = 0\n",
        "        prev_gray = None\n",
        "        last_roles = None\n",
        "        persistence_frames = 5\n",
        "        persistence_counter = 0\n",
        "        agresor_id = None\n",
        "        tracker.reset()\n",
        "\n",
        "        while cap.isOpened() and frame_count < segment_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            mag_mean = 0\n",
        "            if prev_gray is not None:\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "                mag_mean = np.mean(mag)\n",
        "            prev_gray = gray\n",
        "\n",
        "            results = yolo(frame, verbose=False)\n",
        "            person_boxes = []\n",
        "            detections = []\n",
        "            for result in results:\n",
        "                for box in result.boxes:\n",
        "                    if int(box.cls) == 0:\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                        person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                        center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                        detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "            tracked_objects = tracker.update(detections=detections)\n",
        "            track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "            keypoints_list = []\n",
        "            if len(person_boxes) > 1:\n",
        "                for box in person_boxes:\n",
        "                    x, y, w, h = [int(v) for v in box]\n",
        "                    crop = frame[y:y+h, x:x+w]\n",
        "                    keypoints = detect_pose(crop) if crop.size > 0 and w > 50 and h > 100 else np.zeros((17, 3))\n",
        "                    keypoints_list.append(keypoints)\n",
        "\n",
        "                current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "                if any(r != 'desconocido' for _, r in current_roles):\n",
        "                    last_roles = current_roles\n",
        "                    persistence_counter = persistence_frames\n",
        "                    agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                    if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                        agresor_id = tracked_objects[agresor_idx].id\n",
        "                elif persistence_counter > 0:\n",
        "                    current_roles = last_roles\n",
        "                    persistence_counter -= 1\n",
        "            else:\n",
        "                current_roles = []\n",
        "\n",
        "            for i, box in enumerate(person_boxes):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                if i < len(keypoints_list):\n",
        "                    for kp in keypoints_list[i]:\n",
        "                        if kp[2] > 0.3:\n",
        "                            kx = int(kp[1] * w + x)\n",
        "                            ky = int(kp[0] * h + y)\n",
        "                            cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        if os.path.exists(out_path):\n",
        "            print(f\"Segmento procesado guardado en: {out_path}\")\n",
        "            return out_path\n",
        "        return \"Error: No se generó el video de salida.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar video: {str(e)}\")\n",
        "        return f\"Error al procesar video: {str(e)}\"\n",
        "\n",
        "# Función para Gradio\n",
        "def gradio_app(test_case, youtube_url):\n",
        "    try:\n",
        "        # Limpiar archivos anteriores\n",
        "        for f in Path(\"/content\").glob(\"video_*.mp4\"):\n",
        "            f.unlink()\n",
        "        for f in Path(\"/content\").glob(\"processed_segment.mp4\"):\n",
        "            f.unlink()\n",
        "\n",
        "        video_path = None\n",
        "        start_time = 0\n",
        "        if test_case != \"Subir URL personalizada\":\n",
        "            video_path = test_cases.get(test_case)\n",
        "            if not video_path:\n",
        "                return \"Error: Caso de prueba no válido.\", None\n",
        "            if video_path.startswith(\"/content/\") and not os.path.exists(video_path):\n",
        "                return f\"Error: Video local {video_path} no encontrado. Sube el video a /content/.\", None\n",
        "            elif video_path.startswith(\"https://\"):\n",
        "                start_time = extract_start_time(video_path)\n",
        "                video_path = download_youtube_video(video_path)\n",
        "        else:\n",
        "            if not youtube_url:\n",
        "                return \"Ingresa una URL válida de YouTube.\", None\n",
        "            start_time = extract_start_time(youtube_url)\n",
        "            video_path = download_youtube_video(youtube_url)\n",
        "\n",
        "        if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
        "            return f\"Error al descargar: {video_path}\", None\n",
        "\n",
        "        output_video = process_video(video_path, start_time=start_time, segment_duration=5)\n",
        "\n",
        "        if os.path.exists(video_path):\n",
        "            os.remove(video_path)\n",
        "\n",
        "        if isinstance(output_video, str) and os.path.exists(output_video):\n",
        "            return output_video, gr.File(label=\"Descargar Video Procesado\", value=output_video)\n",
        "        return f\"Error al procesar: {output_video}\", None\n",
        "    except Exception as e:\n",
        "        return f\"Error en la app: {str(e)}\", None\n",
        "\n",
        "# Interfaz Gradio\n",
        "try:\n",
        "    test_case_options = list(test_cases.keys()) + [\"Subir URL personalizada\"]\n",
        "    demo = gr.Interface(\n",
        "        fn=gradio_app,\n",
        "        inputs=[\n",
        "            gr.Dropdown(label=\"Seleccionar Caso de Prueba\", choices=test_case_options, value=\"Subir URL personalizada\"),\n",
        "            gr.Textbox(label=\"URL de YouTube (puedes incluir &t=XXs para empezar en un segundo específico)\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID&t=5s\")\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "            gr.File(label=\"Descargar Video Procesado\")\n",
        "        ],\n",
        "        title=\"Identificador de Víctima y Victimario\",\n",
        "        description=\"Elige un caso de prueba o sube una URL de YouTube. Usa &t=XXs en la URL para empezar en el segundo XX. Se procesan 5 segundos.\"\n",
        "    )\n",
        "    print(\"Interfaz Gradio creada. Lanzando...\")\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error al lanzar Gradio: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "Ws93T1XdkTzb",
        "outputId": "5f04771d-59b3-4962-9e8b-91c33fc95154"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencias ya instaladas. Saltando instalación.\n",
            "Librerías importadas correctamente.\n",
            "Modelos ya cargados. Saltando carga.\n",
            "Tracker Norfair configurado.\n",
            "Interfaz Gradio creada. Lanzando...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://699aedd0af7b323fc5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://699aedd0af7b323fc5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar dependencias sin reinstalar\n",
        "import importlib.util\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "required_packages = ['gradio', 'yt_dlp', 'ultralytics', 'tensorflow', 'cv2', 'norfair', 'numpy']\n",
        "all_installed = True\n",
        "for pkg in required_packages:\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        all_installed = False\n",
        "        print(f\"Paquete {pkg} no encontrado. Instalando dependencias...\")\n",
        "        break\n",
        "\n",
        "if not all_installed:\n",
        "    print(\"Instalando dependencias...\")\n",
        "    try:\n",
        "        !pip install -q gradio==4.44.0 yt-dlp==2024.10.22 ultralytics==8.3.15 tensorflow==2.17.0 opencv-python==4.10.0.84 norfair==2.2.0 numpy==1.26.4\n",
        "        print(\"Dependencias instaladas correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al instalar dependencias: {str(e)}\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"Dependencias ya instaladas. Saltando instalación.\")\n",
        "\n",
        "# Importar librerías\n",
        "try:\n",
        "    import gradio as gr\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    from ultralytics import YOLO\n",
        "    from norfair import Tracker, Detection\n",
        "    from pathlib import Path\n",
        "    print(\"Librerías importadas correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al importar librerías: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Verificar y cargar modelos\n",
        "movenet = None\n",
        "yolo = None\n",
        "if 'movenet' not in globals() or 'yolo' not in globals():\n",
        "    try:\n",
        "        print(\"Cargando modelos...\")\n",
        "        movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "        movenet = movenet_model.signatures['serving_default']\n",
        "        yolo = YOLO('yolov8n.pt')\n",
        "        print(\"Modelos cargados correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar modelos: {str(e)}\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"Modelos ya cargados. Saltando carga.\")\n",
        "\n",
        "# Configurar Norfair Tracker\n",
        "def euclidean_distance(detection, tracked_object):\n",
        "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
        "\n",
        "try:\n",
        "    tracker = Tracker(distance_function=euclidean_distance, distance_threshold=150)\n",
        "    print(\"Tracker Norfair configurado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar tracker: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Casos de prueba predefinidos\n",
        "test_cases = {\n",
        "    \"Caso 1: Robo CCTV (Placeholder)\": \"https://www.youtube.com/watch?v=your_robbery_video_id&t=0s\",\n",
        "    \"Caso 2: CCTV Normal (Placeholder)\": \"https://www.youtube.com/watch?v=your_normal_video_id&t=0s\",\n",
        "    \"Caso 3: Video Local (robbery.mp4)\": \"/content/robbery.mp4\"\n",
        "}\n",
        "# Reemplaza URLs con videos reales de CCTV (busca \"CCTV robbery footage\") y añade &t=XXs si quieres.\n",
        "\n",
        "# Extraer start_time de URL de YouTube\n",
        "def extract_start_time(url):\n",
        "    try:\n",
        "        match = re.search(r\"[?&]t=(\\d+)s?\", url)\n",
        "        return int(match.group(1)) if match else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error al extraer start_time: {str(e)}\")\n",
        "        return 0\n",
        "\n",
        "# Descargar video de YouTube\n",
        "def download_youtube_video(url):\n",
        "    try:\n",
        "        ydl_opts = {'format': 'best[ext=mp4]', 'outtmpl': '/content/video_%(id)s.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "            if os.path.exists(video_path):\n",
        "                print(f\"Video descargado: {video_path}\")\n",
        "                return video_path\n",
        "            return \"Error: Video no encontrado tras la descarga.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error al descargar video: {str(e)}\"\n",
        "\n",
        "# Preprocesar frame para MoveNet\n",
        "def preprocess_frame(frame):\n",
        "    try:\n",
        "        img = cv2.resize(frame, (192, 192))\n",
        "        img = tf.cast(img, tf.int32)\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar frame: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Detectar poses\n",
        "def detect_pose(frame):\n",
        "    try:\n",
        "        input_img = preprocess_frame(frame)\n",
        "        if input_img is None:\n",
        "            return np.zeros((17, 3))\n",
        "        outputs = movenet(input_img)\n",
        "        keypoints = outputs['output_0'].numpy()[0, 0]\n",
        "        return keypoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error al detectar poses: {str(e)}\")\n",
        "        return np.zeros((17, 3))\n",
        "\n",
        "# Heurística de roles\n",
        "def analyze_roles(keypoints_list, person_boxes, height, mag_mean):\n",
        "    try:\n",
        "        roles = []\n",
        "        for i, (keypoints, box) in enumerate(zip(keypoints_list, person_boxes)):\n",
        "            hands = [keypoints[9], keypoints[10]]\n",
        "            for hand in hands:\n",
        "                if hand[2] > 0.3 and hand[0] < 0.6:\n",
        "                    hand_y, hand_x = hand[0], hand[1]\n",
        "                    for j, other_box in enumerate(person_boxes):\n",
        "                        if i != j:\n",
        "                            ox, oy, ow, oh = other_box\n",
        "                            center_x = ox + ow / 2\n",
        "                            center_y = oy + oh / 2\n",
        "                            dist_x = abs(hand_x * 192 - center_x)\n",
        "                            dist_y = abs(hand_y * height - center_y)\n",
        "                            if dist_x < 120 and dist_y < 120:\n",
        "                                print(f\"Detectado agresor {i} cerca de víctima {j} (mag={mag_mean:.2f})\")\n",
        "                                return [(i, 'agresor'), (j, 'víctima')]\n",
        "        if mag_mean > 2 and len(person_boxes) >= 2:\n",
        "            print(f\"Movimiento alto: asignando agresor 0, víctima 1 (mag={mag_mean:.2f})\")\n",
        "            return [(0, 'agresor'), (1, 'víctima')]\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_roles: {str(e)}\")\n",
        "        return [(i, 'desconocido') for i in range(len(person_boxes))]\n",
        "\n",
        "# Procesar video\n",
        "def process_video(video_path, start_time=0, segment_duration=5):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return \"Error: No se pudo abrir el video.\"\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        out_path = '/content/processed_video.mp4'\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "        start_frame = int(start_time * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "        segment_frames = int(segment_duration * fps)\n",
        "\n",
        "        frame_count = 0\n",
        "        prev_gray = None\n",
        "        last_roles = None\n",
        "        persistence_frames = 5\n",
        "        persistence_counter = 0\n",
        "        agresor_id = None\n",
        "        tracker.reset()\n",
        "\n",
        "        while cap.isOpened() and frame_count < segment_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            mag_mean = 0\n",
        "            if prev_gray is not None:\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "                mag_mean = np.mean(mag)\n",
        "            prev_gray = gray\n",
        "\n",
        "            results = yolo(frame, verbose=False)\n",
        "            person_boxes = []\n",
        "            detections = []\n",
        "            for result in results:\n",
        "                for box in result.boxes:\n",
        "                    if int(box.cls) == 0:\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                        person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                        center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "                        detections.append(Detection(points=center, scores=np.array([box.conf.item()])))\n",
        "\n",
        "            tracked_objects = tracker.update(detections=detections)\n",
        "            track_ids = [obj.id for obj in tracked_objects]\n",
        "\n",
        "            keypoints_list = []\n",
        "            if len(person_boxes) > 1:\n",
        "                for box in person_boxes:\n",
        "                    x, y, w, h = [int(v) for v in box]\n",
        "                    crop = frame[y:y+h, x:x+w]\n",
        "                    keypoints = detect_pose(crop) if crop.size > 0 and w > 50 and h > 100 else np.zeros((17, 3))\n",
        "                    keypoints_list.append(keypoints)\n",
        "\n",
        "                current_roles = analyze_roles(keypoints_list, person_boxes, height, mag_mean)\n",
        "                if any(r != 'desconocido' for _, r in current_roles):\n",
        "                    last_roles = current_roles\n",
        "                    persistence_counter = persistence_frames\n",
        "                    agresor_idx = next((i for i, r in current_roles if r == 'agresor'), None)\n",
        "                    if agresor_idx is not None and len(tracked_objects) > agresor_idx:\n",
        "                        agresor_id = tracked_objects[agresor_idx].id\n",
        "                elif persistence_counter > 0:\n",
        "                    current_roles = last_roles\n",
        "                    persistence_counter -= 1\n",
        "            else:\n",
        "                current_roles = []\n",
        "\n",
        "            for i, box in enumerate(person_boxes):\n",
        "                x, y, w, h = [int(v) for v in box]\n",
        "                role = next((r for idx, r in current_roles if idx == i), 'desconocido')\n",
        "                track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                color = (0, 0, 255) if role == 'agresor' or track_id == agresor_id else (255, 0, 0) if role == 'víctima' else (128, 128, 128)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                label = f\"{role.upper()} ID:{track_id}\" if track_id else role.upper()\n",
        "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                if i < len(keypoints_list):\n",
        "                    for kp in keypoints_list[i]:\n",
        "                        if kp[2] > 0.3:\n",
        "                            kx = int(kp[1] * w + x)\n",
        "                            ky = int(kp[0] * h + y)\n",
        "                            cv2.circle(frame, (kx, ky), 3, (255, 255, 0), -1)\n",
        "\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "            if frame_count % 10 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        if os.path.exists(out_path):\n",
        "            print(f\"Video procesado guardado en: {out_path}\")\n",
        "            return out_path\n",
        "        return \"Error: No se generó el video de salida.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar video: {str(e)}\")\n",
        "        return f\"Error al procesar video: {str(e)}\"\n",
        "\n",
        "# Función para Gradio\n",
        "def gradio_app(test_case, youtube_url):\n",
        "    try:\n",
        "        # Limpiar archivos anteriores\n",
        "        for f in Path(\"/content\").glob(\"video_*.mp4\"):\n",
        "            f.unlink()\n",
        "        for f in Path(\"/content\").glob(\"processed_video.mp4\"):\n",
        "            f.unlink()\n",
        "\n",
        "        video_path = None\n",
        "        start_time = 0\n",
        "        if test_case != \"Subir URL personalizada\":\n",
        "            video_path = test_cases.get(test_case)\n",
        "            if not video_path:\n",
        "                return \"Error: Caso de prueba no válido.\", None\n",
        "            if video_path.startswith(\"/content/\") and not os.path.exists(video_path):\n",
        "                return f\"Error: Video local {video_path} no encontrado. Sube el video a /content/.\", None\n",
        "            elif video_path.startswith(\"https://\"):\n",
        "                start_time = extract_start_time(video_path)\n",
        "                video_path = download_youtube_video(video_path)\n",
        "        else:\n",
        "            if not youtube_url:\n",
        "                return \"Ingresa una URL válida de YouTube.\", None\n",
        "            start_time = extract_start_time(youtube_url)\n",
        "            video_path = download_youtube_video(youtube_url)\n",
        "\n",
        "        if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
        "            return f\"Error al descargar: {video_path}\", None\n",
        "\n",
        "        output_video = process_video(video_path, start_time=start_time)\n",
        "        if isinstance(output_video, str) and os.path.exists(output_video):\n",
        "            return output_video, gr.File(label=\"Descargar Video Procesado\", value=output_video)\n",
        "        return f\"Error al procesar: {output_video}\", None\n",
        "    except Exception as e:\n",
        "        return f\"Error en la app: {str(e)}\", None\n",
        "\n",
        "# Interfaz Gradio\n",
        "try:\n",
        "    test_case_options = list(test_cases.keys()) + [\"Subir URL personalizada\"]\n",
        "    demo = gr.Interface(\n",
        "        fn=gradio_app,\n",
        "        inputs=[\n",
        "            gr.Dropdown(label=\"Seleccionar Caso de Prueba\", choices=test_case_options, value=\"Subir URL personalizada\"),\n",
        "            gr.Textbox(label=\"URL de YouTube (puedes incluir &t=XXs para empezar en un segundo específico)\", placeholder=\"Ej: https://www.youtube.com/watch?v=VIDEO_ID&t=5s\")\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Video(label=\"Video Procesado con Tracking de Victimario\"),\n",
        "            gr.File(label=\"Descargar Video Procesado\")\n",
        "        ],\n",
        "        title=\"Identificador de Víctima y Victimario\",\n",
        "        description=\"Elige un caso de prueba o sube una URL de YouTube con &t=XXs para empezar en el segundo XX. Se procesan 5 segundos.\"\n",
        "    )\n",
        "    print(\"Interfaz Gradio creada. Lanzando...\")\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error al lanzar Gradio: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "gvCQeXk-lWyg",
        "outputId": "78e6bd20-b873-49c8-9e24-aceb63476d91"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencias ya instaladas. Saltando instalación.\n",
            "Librerías importadas correctamente.\n",
            "Modelos ya cargados. Saltando carga.\n",
            "Tracker Norfair configurado.\n",
            "Interfaz Gradio creada. Lanzando...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0a4c065bbb15fc1c3f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0a4c065bbb15fc1c3f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://grok.com/share/bGVnYWN5_07e99e43-7b9c-4dc2-814a-cf1386366a84"
      ],
      "metadata": {
        "id": "PEVkhTP8l-0w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-eqpq3-l9y5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}