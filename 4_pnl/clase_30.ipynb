{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCHojGd8C7a2gE3XadUjk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_sept/blob/main/4_pnl/clase_30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYK-DRZRArks",
        "outputId": "2a9b1bc7-a2a9-4ed5-a991-c0f8832309ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame original:\n",
            "  Categoria\n",
            "0      Rojo\n",
            "1      Azul\n",
            "2     Verde\n",
            "3      Rojo\n",
            "4      Azul\n",
            "------------------------------\n",
            "Usando pandas get_dummies:\n",
            "   Categoria_Azul  Categoria_Rojo  Categoria_Verde\n",
            "0           False            True            False\n",
            "1            True           False            False\n",
            "2           False           False             True\n",
            "3           False            True            False\n",
            "4            True           False            False\n",
            "------------------------------\n",
            "Usando Scikit-learn OneHotEncoder:\n",
            "[[0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "------------------------------\n",
            "Usando Keras to_categorical:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n",
            "------------------------------\n",
            "Diferencias clave:\n",
            "- `pd.get_dummies`: Funciona directamente con DataFrames, crea nuevas columnas con los nombres de las categorías.\n",
            "- `sklearn.preprocessing.OneHotEncoder`: Trabaja con arrays NumPy (requiere reshape si es necesario), devuelve un array NumPy. Requiere `fit` y `transform`.\n",
            "- `tf.keras.utils.to_categorical`: Trabaja con etiquetas numéricas (enteros), devuelve un array NumPy. Útil para la salida de modelos de clasificación en Keras.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Ejemplo de datos categóricos\n",
        "data = {'Categoria': ['Rojo', 'Azul', 'Verde', 'Rojo', 'Azul']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"DataFrame original:\")\n",
        "print(df)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Usando pandas get_dummies\n",
        "df_dummies = pd.get_dummies(df, columns=['Categoria'])\n",
        "print(\"Usando pandas get_dummies:\")\n",
        "print(df_dummies)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Usando Scikit-learn OneHotEncoder\n",
        "# Necesita un array numpy 2D\n",
        "data_np = df[['Categoria']].values\n",
        "encoder_sk = OneHotEncoder(sparse_output=False)\n",
        "data_encoded_sk = encoder_sk.fit_transform(data_np)\n",
        "print(\"Usando Scikit-learn OneHotEncoder:\")\n",
        "print(data_encoded_sk)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Usando Keras to_categorical\n",
        "# Necesita etiquetas numéricas. Primero convertimos las categorías a números.\n",
        "# Usamos factorize para esto\n",
        "codes, uniques = pd.factorize(df['Categoria'])\n",
        "data_encoded_keras = to_categorical(codes)\n",
        "print(\"Usando Keras to_categorical:\")\n",
        "print(data_encoded_keras)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Diferencias clave:\")\n",
        "print(\"- `pd.get_dummies`: Funciona directamente con DataFrames, crea nuevas columnas con los nombres de las categorías.\")\n",
        "print(\"- `sklearn.preprocessing.OneHotEncoder`: Trabaja con arrays NumPy (requiere reshape si es necesario), devuelve un array NumPy. Requiere `fit` y `transform`.\")\n",
        "print(\"- `tf.keras.utils.to_categorical`: Trabaja con etiquetas numéricas (enteros), devuelve un array NumPy. Útil para la salida de modelos de clasificación en Keras.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Ejemplo de texto\n",
        "sentences = [\n",
        "    'El perro corre en el parque',\n",
        "    'El gato duerme en la casa',\n",
        "    'El perro juega con la pelota',\n",
        "    'El gato caza un ratón'\n",
        "]\n",
        "\n",
        "# Tokenización\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(\"Índice de palabras:\")\n",
        "print(word_index)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Secuencias de texto:\")\n",
        "print(sequences)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Padding (para que todas las secuencias tengan la misma longitud)\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"Secuencias con padding:\")\n",
        "print(padded_sequences)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Crear un modelo simple con una capa de Embedding\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word_index) + 1, output_dim=8, input_length=max_length),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid') # Capa de ejemplo, puede ser cualquier otra\n",
        "])\n",
        "\n",
        "# Build the model to initialize weights\n",
        "model.build(input_shape=(None, max_length))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Para ver los embeddings, puedes acceder a los pesos de la capa de Embedding\n",
        "# Ten en cuenta que estos embeddings son aleatorios al principio y se aprenderían durante el entrenamiento\n",
        "embedding_layer = model.layers[0]\n",
        "embeddings = embedding_layer.get_weights()[0]\n",
        "\n",
        "print(\"Forma de la matriz de embeddings (vocabulario_size, embedding_dim):\")\n",
        "print(embeddings.shape)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Puedes ver el vector de embedding para una palabra específica (ej. 'perro')\n",
        "word = 'perro'\n",
        "if word in word_index:\n",
        "    index = word_index[word]\n",
        "    word_embedding = embeddings[index]\n",
        "    print(f\"Vector de embedding para '{word}':\")\n",
        "    print(word_embedding)\n",
        "else:\n",
        "    print(f\"La palabra '{word}' no está en el vocabulario.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Explicación:\")\n",
        "print(\"- **Tokenización:** Convierte el texto en secuencias de números (IDs de palabras).\")\n",
        "print(\"- **Padding:** Asegura que todas las secuencias tengan la misma longitud rellenando con ceros.\")\n",
        "print(\"- **Capa de Embedding:** Representa cada palabra con un vector de números de menor dimensión.\")\n",
        "print(\"- **output_dim:** Es la dimensión del vector de embedding para cada palabra.\")\n",
        "print(\"- **input_dim:** Es el tamaño del vocabulario + 1 (para el padding).\")\n",
        "print(\"- **input_length:** Es la longitud máxima de las secuencias.\")\n",
        "print(\"Estos embeddings son inicialmente aleatorios y se ajustarían durante el entrenamiento de un modelo en una tarea específica (como clasificación de texto).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "WMu9ZuXMU5tR",
        "outputId": "fcddcd9e-98e7-407f-8adc-e2d7c60a2b61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice de palabras:\n",
            "{'<OOV>': 1, 'el': 2, 'perro': 3, 'en': 4, 'gato': 5, 'la': 6, 'corre': 7, 'parque': 8, 'duerme': 9, 'casa': 10, 'juega': 11, 'con': 12, 'pelota': 13, 'caza': 14, 'un': 15, 'ratón': 16}\n",
            "------------------------------\n",
            "Secuencias de texto:\n",
            "[[2, 3, 7, 4, 2, 8], [2, 5, 9, 4, 6, 10], [2, 3, 11, 12, 6, 13], [2, 5, 14, 15, 16]]\n",
            "------------------------------\n",
            "Secuencias con padding:\n",
            "[[ 2  3  7  4  2  8]\n",
            " [ 2  5  9  4  6 10]\n",
            " [ 2  3 11 12  6 13]\n",
            " [ 2  5 14 15 16  0]]\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │           \u001b[38;5;34m136\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m49\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m185\u001b[0m (740.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185</span> (740.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m185\u001b[0m (740.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185</span> (740.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de la matriz de embeddings (vocabulario_size, embedding_dim):\n",
            "(17, 8)\n",
            "------------------------------\n",
            "Vector de embedding para 'perro':\n",
            "[-0.00524797 -0.0449581   0.03110847 -0.02950633  0.00090873  0.00061829\n",
            " -0.0278508   0.01563616]\n",
            "------------------------------\n",
            "Explicación:\n",
            "- **Tokenización:** Convierte el texto en secuencias de números (IDs de palabras).\n",
            "- **Padding:** Asegura que todas las secuencias tengan la misma longitud rellenando con ceros.\n",
            "- **Capa de Embedding:** Representa cada palabra con un vector de números de menor dimensión.\n",
            "- **output_dim:** Es la dimensión del vector de embedding para cada palabra.\n",
            "- **input_dim:** Es el tamaño del vocabulario + 1 (para el padding).\n",
            "- **input_length:** Es la longitud máxima de las secuencias.\n",
            "Estos embeddings son inicialmente aleatorios y se ajustarían durante el entrenamiento de un modelo en una tarea específica (como clasificación de texto).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "sentences = [\n",
        "    'El perro corre en el parque',\n",
        "    'El gato duerme en la casa',\n",
        "    'El perro juega con la pelota',\n",
        "    'El gato caza un ratón'\n",
        "]\n",
        "\n",
        "# Tokenización para One-Hot Encoding a nivel de palabra\n",
        "words = [word for sentence in sentences for word in sentence.lower().split()]\n",
        "unique_words = sorted(list(set(words)))\n",
        "vocab_size = len(unique_words)\n",
        "\n",
        "print(\"Vocabulario para One-Hot Encoding:\")\n",
        "print(unique_words)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Crear representaciones One-Hot para cada oración\n",
        "one_hot_encoded_sentences = []\n",
        "for sentence in sentences:\n",
        "    sentence_words = sentence.lower().split()\n",
        "    one_hot_vector = np.zeros(vocab_size)\n",
        "    for word in sentence_words:\n",
        "        if word in unique_words:\n",
        "            index = unique_words.index(word)\n",
        "            one_hot_vector[index] = 1  # O podrías contar la frecuencia: one_hot_vector[index] += 1\n",
        "    one_hot_encoded_sentences.append(one_hot_vector)\n",
        "\n",
        "print(\"Representación One-Hot de las oraciones:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Oración {i+1}: {sentence}\")\n",
        "    print(one_hot_encoded_sentences[i])\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "print(\"\\nComparación con Word Embeddings (basado en la celda anterior):\")\n",
        "print(\"- **One-Hot Encoding:** Crea un vector binario largo donde cada dimensión representa una palabra única del vocabulario. La dimensión del vector es igual al tamaño del vocabulario. No captura relaciones semánticas entre palabras.\")\n",
        "print(\"- **Word Embeddings:** Representa cada palabra como un vector denso de menor dimensión. La dimensión del vector es un parámetro (output_dim en Keras). Captura relaciones semánticas (palabras similares tienen vectores similares).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0KlYqNHVR_6",
        "outputId": "c1d3ea03-f093-4ce8-b813-6b57dfe3d307"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario para One-Hot Encoding:\n",
            "['casa', 'caza', 'con', 'corre', 'duerme', 'el', 'en', 'gato', 'juega', 'la', 'parque', 'pelota', 'perro', 'ratón', 'un']\n",
            "------------------------------\n",
            "Representación One-Hot de las oraciones:\n",
            "Oración 1: El perro corre en el parque\n",
            "[0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "----------\n",
            "Oración 2: El gato duerme en la casa\n",
            "[1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "----------\n",
            "Oración 3: El perro juega con la pelota\n",
            "[0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
            "----------\n",
            "Oración 4: El gato caza un ratón\n",
            "[0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
            "----------\n",
            "\n",
            "Comparación con Word Embeddings (basado en la celda anterior):\n",
            "- **One-Hot Encoding:** Crea un vector binario largo donde cada dimensión representa una palabra única del vocabulario. La dimensión del vector es igual al tamaño del vocabulario. No captura relaciones semánticas entre palabras.\n",
            "- **Word Embeddings:** Representa cada palabra como un vector denso de menor dimensión. La dimensión del vector es un parámetro (output_dim en Keras). Captura relaciones semánticas (palabras similares tienen vectores similares).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Acceder al word_index y a los embeddings de la celda WMu9ZuXMU5tR\n",
        "# Es importante que la celda WMu9ZuXMU5tR se haya ejecutado antes para que estas variables existan.\n",
        "\n",
        "words_to_show = ['pelota', 'parque', 'perro', 'gato']\n",
        "\n",
        "print(\"Vectores de embedding para palabras específicas:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for word in words_to_show:\n",
        "    if word in word_index:\n",
        "        index = word_index[word]\n",
        "        word_embedding = embeddings[index]\n",
        "        print(f\"Vector de embedding para '{word}':\")\n",
        "        print(word_embedding)\n",
        "        print(\"-\" * 20)\n",
        "    else:\n",
        "        print(f\"La palabra '{word}' no está en el vocabulario.\")\n",
        "        print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgohDu3wW9VD",
        "outputId": "8a1e65ce-6064-48d0-bfca-52c30c82a6cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectores de embedding para palabras específicas:\n",
            "----------------------------------------\n",
            "Vector de embedding para 'pelota':\n",
            "[ 0.02356908 -0.02547683  0.03363779  0.02462841  0.02558625 -0.03036195\n",
            "  0.03894265  0.04249628]\n",
            "--------------------\n",
            "Vector de embedding para 'parque':\n",
            "[ 0.01349691 -0.00854899 -0.03189993 -0.00283885  0.04491376  0.00339184\n",
            "  0.03399635  0.03922579]\n",
            "--------------------\n",
            "Vector de embedding para 'perro':\n",
            "[-0.00524797 -0.0449581   0.03110847 -0.02950633  0.00090873  0.00061829\n",
            " -0.0278508   0.01563616]\n",
            "--------------------\n",
            "Vector de embedding para 'gato':\n",
            "[ 0.03620568 -0.02313721  0.00301746 -0.01685438 -0.03915597  0.03584406\n",
            " -0.00405413  0.02971119]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85a6639e",
        "outputId": "b8e94ae7-c926-4846-a537-dc8151931ad3"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "words_to_compare = ['pelota', 'parque', 'perro', 'gato']\n",
        "word_vectors = {}\n",
        "\n",
        "# Obtener los vectores de embedding para las palabras de interés\n",
        "for word in words_to_compare:\n",
        "    if word in word_index:\n",
        "        index = word_index[word]\n",
        "        word_vectors[word] = embeddings[index].reshape(1, -1) # Reshape para que sea 2D\n",
        "    else:\n",
        "        print(f\"Advertencia: La palabra '{word}' no está en el vocabulario.\")\n",
        "\n",
        "print(\"Medidas de similitud del coseno entre conceptos:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Calcular y mostrar la similitud del coseno entre pares de palabras\n",
        "for i in range(len(words_to_compare)):\n",
        "    for j in range(i + 1, len(words_to_compare)):\n",
        "        word1 = words_to_compare[i]\n",
        "        word2 = words_to_compare[j]\n",
        "\n",
        "        if word1 in word_vectors and word2 in word_vectors:\n",
        "            vector1 = word_vectors[word1]\n",
        "            vector2 = word_vectors[word2]\n",
        "            similarity = cosine_similarity(vector1, vector2)[0][0]\n",
        "            print(f\"Similitud '{word1}' y '{word2}': {similarity:.4f}\")\n",
        "        else:\n",
        "            print(f\"No se pudo calcular la similitud entre '{word1}' y '{word2}' (una o ambas palabras no encontradas).\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Nota: Estos valores de similitud se basan en los embeddings aleatorios iniciales. Después del entrenamiento, los valores reflejarían mejor las relaciones semánticas aprendidas.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medidas de similitud del coseno entre conceptos:\n",
            "--------------------------------------------------\n",
            "Similitud 'pelota' y 'parque': 0.4999\n",
            "Similitud 'pelota' y 'perro': 0.1492\n",
            "Similitud 'pelota' y 'gato': 0.0212\n",
            "Similitud 'parque' y 'perro': -0.1631\n",
            "Similitud 'parque' y 'gato': 0.0048\n",
            "Similitud 'perro' y 'gato': 0.3740\n",
            "--------------------------------------------------\n",
            "Nota: Estos valores de similitud se basan en los embeddings aleatorios iniciales. Después del entrenamiento, los valores reflejarían mejor las relaciones semánticas aprendidas.\n"
          ]
        }
      ]
    }
  ]
}